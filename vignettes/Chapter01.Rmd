---
title: "Chapter 1"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{code-examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
knitr::opts_knit$set(global.par = TRUE)
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2, 2, 3, .5), lwd = 1)
```

# Section 1.3
## Example 1.3 and 1.4: Classifying categorical observations into two states

We have two firms, a reliable firm $A$ with $P(A) = 0.7$ and a less reliable firm $A^C$ with
$P(A^C) = 1 - 0.7 = 0.3$.

```{r}
PrA <- 0.7
PrAC <- 1 - PrA
```

The failure rate, i.e. the probability of a faulty item, is 0.01 for the reliable
firm $A$ and 0.05 for the less reliable firm $A^C$.

```{r}
PrFA <- 0.01
PrFAC <- 0.05
```

Assume that we observe any number of failures $y$ between 0 and 6 (from $N = 100$ observations total),
but we don't know whether the items are from company $A$ or $A^C$.

```{r}
N <- 100
y <- 0:6
```

We can now compute the probability of dealing with items from company $A$ by using
Bayes' theorem.

```{r}
PosteriorTheta1Unormalized <- PrFA ^ y * (1 - PrFA) ^ (N - y) * PrA
PosteriorTheta0Unormalized <- PrFAC ^ y * (1 - PrFAC) ^ (N - y) * PrAC
NormalizingConstant <- PosteriorTheta0Unormalized + PosteriorTheta1Unormalized

round(PosteriorTheta1Unormalized / NormalizingConstant, 3)
```

Of course, the probabilities of dealing with items from company $A^C$ are
complementary.

```{r}
round(PosteriorTheta0Unormalized / NormalizingConstant, 3)
```

Assuming we don't have any prior information about the probability of occurence
of company $A$, we could simply set $P(A) = P(A^C) = 0.5$ and recompute the
posterior probabilities.

```{r}
PrA <- 0.5
PrAC <- 1 - PrA
PosteriorTheta1Unormalized <- PrFA ^ y * (1 - PrFA) ^ (N - y) * PrA
PosteriorTheta0Unormalized <- PrFAC ^ y * (1 - PrFAC) ^ (N - y) * PrAC
NormalizingConstant <- PosteriorTheta0Unormalized + PosteriorTheta1Unormalized

round(PosteriorTheta1Unormalized / NormalizingConstant, 3)
```

Again, the probabilities of dealing with items from company $A^C$ are
complementary.

```{r}
round(PosteriorTheta0Unormalized / NormalizingConstant, 3)
```


## Example 1.5: Classifying continuous observations into two states

We have a Bernoulli random variable $\vartheta$ with prior probability
$$P(\vartheta) = 0.55,$$
and we know for the females that
$$y|\vartheta_1 \sim Exp(\lambda),$$
and for the males that $$y|\vartheta_0 \sim G(\alpha, \beta),$$
where $\lambda = 1/2.2$, $\alpha = 5$, and $\beta = 5/5.5$.

```{r}
Prtheta = 0.55
lambda = 1/2.2
alpha = 5
beta = 5/5.5
```

First, we visualize the conditional densitites.
Then, we compute the non-normalized posteriors for $y \in \{1, 2, 3, 6, 8\}$,
mark those in the plot, and add lines to visualize the non-normalized
posteriors.

```{r}
x <- seq(0, 10, 0.01)
plot(x, dexp(x, lambda), type = "l", xlab = "", ylab = "")
title("Conditional densities and non-normalized posteriors of distance")
lines(x, dgamma(x, alpha, beta), col = "red")

y <- c(1, 2, 3, 6, 8)
theta1_y <- dexp(y, lambda) * Prtheta
theta0_y <- dgamma(y, alpha, beta) * (1 - Prtheta)

points(y, theta1_y)
points(y, theta0_y, col = "red")

lines(x,  dexp(x, lambda) * Prtheta, lty = 3)
lines(x,  dgamma(x, alpha, beta) * (1 - Prtheta), lty = 3, col = "red")
```

Finally, here are the posterior probabilities. Note that each ratio of the pairs
of corresponding posterior probabilities is the same as each ratio of the
heights of the vertically corresponding points in the graph.

```{r}
female <- theta1_y / (theta0_y + theta1_y)
male <- theta0_y / (theta0_y + theta1_y)
res <- rbind(female, male)
colnames(res) <- y
rownames(res) <- c("female", "male")
round(res, 3)
```

