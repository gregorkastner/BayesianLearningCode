---
title: "Chapter 5: Learning More About Bayesian Learning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Chapter 5: Learning More About Bayesian Learning}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
knitr::opts_knit$set(global.par = TRUE)
pdfplots <- FALSE # default: FALSE; set this to TRUE only if you like pdf figures
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2.6, 2.6, 2.6, .4), lwd = 1)
```

# Section 5.2: Conjugate or non-conjugate?
## Example 5.3 / Figure 5.1: The adaptive nature of Bayesian learning

We illustrate the adaptive nature of Bayesian learning (also referred to
as *sequential updating* or *on-line learning*) via the beta-binomial model
from earlier.

```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("5-5_1.pdf", width = 8, height = 5)
  par(mar = c(2.2, 1.6, 1.6, .2), mgp = c(1.2, .5, 0))
}
par(mfrow = c(1, 3))
set.seed(42)

a0 <- 1
b0 <- 1
N <- 100 
thetatrue <- c(0, .1, .5)
theta <- seq(0, 1, length.out = 201)

for (i in seq_along(thetatrue)) {
  plot(theta, dbeta(theta, a0, b0), type = 'l', ylim = c(0, 11),
       col = rgb(0, 0, 0, .2), xlab = expression(vartheta), ylab = '',
       main = bquote(vartheta[true] == .(thetatrue[i])))
  
  succ <- fail <- 0L
  for (j in seq_len(N)) {
    if (rbinom(1, 1, thetatrue[i])) succ <- succ + 1L else fail <- fail + 1L
    lines(theta, dbeta(theta, a0 + succ, b0 + fail), col = rgb(0, 0, 0, .2 + .4*j/N))
  }
  
  legend('topright', paste("N =", c(0, 20, 40, 60, 80, 100)), lty = 1,
         col = rgb(0, 0, 0, .2 + .4*c(0, 20, 40, 60, 80, 100)/N))
}
```

# Section 5.3.2: Uncertainty quantification
## Example 5.5: Quantifying and visualizing posterior uncertainty for the mean and the variance of a normal distribution

We re-use the posterior established in Chapter 4, where we modeled the CHF/USD
exchange rate as a normal distribution with unknown mean and variance.

```{r}
library(BayesianLearningCode)
posterior <- function(mu, sigma2, ybar, s2, N) {
  dnorm(mu, ybar, sqrt(sigma2 / N)) *
    dinvgamma(sigma2, (N - 1) / 2, N * s2 / 2)
}
```

First, we read in the data once more.

```{r}
data(exrates, package = "stochvol")
y <- 100 * diff(log(exrates$USD / exrates$CHF))
N <- length(y)
ybar <- mean(y)
s2 <- var(y) * (N - 1) / N
```

Now, we simulate from the posterior by Monte Carlo,
i.e., we draw many times from the marginal posterior $\sigma^2|\mathbf{y}$,
and then, conditioning on these draws, from $\mu|\sigma^2,\mathbf{y}$.

```{r}
ndraws <- 100000
sigma2draws <- rinvgamma(ndraws, (N - 1) / 2, N * s2 / 2)
mudraws <- rnorm(ndraws, ybar, sqrt(sigma2draws / N))
draws <- data.frame(mu = mudraws, sigma2 = sigma2draws)
```

Next, we need to fix $\alpha$.

```{r}
alpha <- c(.01, .05, .1)
```

To approximate the bivariate HPD regions for our posterior, we obtain the
functional values and keep the top `r 100 * (1 - alpha)` percent,
respectively.

```{r}
dens <- posterior(draws$mu, draws$sigma2, ybar, s2, N)
HPDdraws <- vector("list", length(alpha))
for (i in seq_along(alpha)) {
 HPDdraws[[i]] <- draws[rank(dens) > floor(ndraws * alpha[i]),]
}
```

To visualize, we compute the convex hull of the remaining draws. Note that we
randomly exclude some draws for visualization only in order to avoid plotting
too many overlapping points. We also draw the projections on the two axes,
yielding the corresponding univariate regions (intervals).

```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("5-3_1.pdf", width = 8, height = 6)
  par(mar = c(2.3, 2.3, .2, .6), mgp = c(1.2, .5, 0))
}
par(mfrow = c(1, 1))
toplot <- sample.int(nrow(draws), 1000)
plot(draws[toplot,], pch = 16, col = rgb(0, 0, 0, .3), cex = 2,
     xlab = expression(mu), ylab = expression(sigma^2))
legend("topright", paste0(100 * (1 - alpha), "%"),
       fill = rgb(1, 0, 0, c(.2, .4, .6)), border = NA)
#rug(draws[toplot, "mu"])
#rug(draws[toplot, "sigma2"], side = 2)
for (i in seq_along(alpha)) {
 hullind <- chull(HPDdraws[[i]])
 polygon(HPDdraws[[i]][hullind,], col = rgb(1, 0, 0, .2), border = NA)
 rect(min(HPDdraws[[i]]$mu), par("usr")[3],
      max(HPDdraws[[i]]$mu), par("usr")[3] + .03 * diff(par("usr")[3:4]),
      col = rgb(1, 0, 0, .2), border = NA)
 rect(par("usr")[1], min(HPDdraws[[i]]$sigma2), 
      par("usr")[1] + .03 * 6 / 8 * diff(par("usr")[1:2]), max(HPDdraws[[i]]$sigma2),
      col = rgb(1, 0, 0, .2), border = NA)
}
```



# Section 5.4
## Figure 5.2: Bayesian asymptotics 1

To reproduce this figure, we again re-use the theory from Chapter 3 (the
beta-binomial model).

```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("5-5_2.pdf", width = 8, height = 5)
  par(mgp = c(1, .5, 0), mar = c(2.2, 1.5, 2, .2), lwd = 2)
}
par(mfrow = c(3, 2))
set.seed(2)
thetatrue <- c(0.02, 0.25)
N <- c(25, 100, 400)
settings <- expand.grid(N = N, thetatrue = thetatrue)
rbinom(6, settings$N, settings$thetatrue)
theta <- seq(0, 1, .0005)

for (n in N) {
  for (p in thetatrue) {
    aN <- n*p + 1
    bN <- n - n*p + 1
    plot(theta, dbeta(theta, aN, bN), type = "l", xlab = expression(vartheta),
         ylab = "", main = bquote(vartheta[true] == .(p) ~ "and" ~ N == .(n)),
         xlim = c(0, 1.1*sqrt(p)), ylim = c(0, 9.5/sqrt(p + 0.008)))
    
    aN <- n*p + 2
    bN <- n - n*p + 4
    lines(theta, dbeta(theta, aN, bN), lty = 2, col = 2)
    
    legend("topright", c("Beta(1,1)", "Beta(2,4)"), lty = c(1, 2),
           col = 1:2)
  }
}
```

## Figure 5.3: Bayesian asymptotics 2
As above, just with higher sample size.

```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("5-5_3.pdf", width = 12, height = 5)
  par(mgp = c(1.4, .5, 0), mar = c(2.3, 1.5, 2, .2), lwd = 2)
}
par(mfrow = c(1, 2))
n <- 1000000

for (p in thetatrue) {
  aN <- n*p + 1
  bN <- n - n*p + 1
  asySD <- sqrt(p * (1 - p) / n)
  theta <- seq(p - 25*asySD, p + 25*asySD, length.out = 333)
  plot(theta, dbeta(theta, aN, bN), type = "l", xlab = expression(vartheta),
       ylab = "", main = bquote(vartheta[true] == .(p)))
    
  aN <- n*p + 2
  bN <- n - n*p + 4
  lines(theta, dbeta(theta, aN, bN), lty = 2, col = 2)
    
  legend("topright", c("Beta(1,1)", "Beta(2,4)"), lty = c(1, 2),
         col = 1:2)
}
```


## Figure 5.4: Bayesian asymptotics 3
We now want to approximate the log posteriors via quadratic polynomials and
visualize these.

```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("5-5_4.pdf", width = 8, height = 5)
  par(mgp = c(1.35, .5, 0), mar = c(2.2, 1.5, 2, .2), lwd = 2)
}
par(mfrow = c(3, 2))
for (n in N) {
  for (p in thetatrue) {
    aN <- n*p + 1
    bN <- n - n*p + 1
    asySD <- sqrt(p * (1 - p) / n)
    theta <- seq(max(0.001, p - 2*asySD), p + 2*asySD, length.out = 222)
    plot(theta, dbeta(theta, aN, bN), type = "l", xlab = expression(vartheta),
         ylab = "", main = bquote(vartheta[true] == .(p) ~ "and" ~ N == .(n)),
         log = "y")
    
    approx <- function(theta, thetastar, n, log = FALSE) {
      res <- dbeta(thetastar, aN, bN, log = TRUE) -
        0.5 * n * 1 / (thetastar * (1 - thetastar)) * (theta - thetastar)^2
      if (log) res else exp(res)
    }
    
    lines(theta, approx(theta, p, n), col = 2, lty = 2)
    
    legend("bottom", c("Posterior", "Approximation"),
           lty = c(1, 2), col = 1:2)
  }
}
```

## Table 5.1: Point estimates
We now compute various point estimates under several settings.

```{r}
a0 <- c(0.5, 2)
set <- expand.grid(N = N, thetatrue = thetatrue, a0 = a0)
set$b0 <- c(rep(0.5, 6), rep(4, 6))
aN <- set$N * set$thetatrue + set$a0
bN <- set$N * (1 - set$thetatrue) + set$b0
posteriormean <- aN / (aN + bN)
posteriormode <- (aN - 1) / (aN + bN - 2)
res <- cbind(mean = posteriormean, mode = posteriormode)
rownames(res) <- set$N
knitr::kable(t(round(res, 4)))
```
