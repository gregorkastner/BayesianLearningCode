---
title: "Chapter 7: Introduction to Bayesian Time Series Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Chapter 7: Introduction to Bayesian Time Series Analysis}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
knitr::opts_knit$set(global.par = TRUE)
pdfplots <- FALSE # default: FALSE; set this to TRUE only if you like pdf figures
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2.6, 2.6, 2.6, .4), lwd = 1)
```

# Section 7.2
## Figure 7.1
First, we load the data. Because of the extreme outliers during the COVID
pandemic, we restrict our analysis to the time before its outbreak.

```{r}
data("gdp", package = "BayesianLearningCode")
dat <- gdp[1:which(names(gdp) == '2019-10-01')]
```

Next, we compute the log returns.

```{r}
logret <- log(dat[-1]) - log(dat[-length(dat)])
```

Now we can plot the data and its empirical autocorrelation function.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("7-2_1.pdf", width = 8, height = 5)
  par(mar = c(2.5, 1.5, .1, .1), mgp = c(1.5, .5, 0))
}
par(mfrow = c(1,2))
ts.plot(logret, main = "U.S. GDP log returns")
acf(logret, lag = 6, main = "")
title("Empirical autocorrelation function")
```

Before we move on, we define a function yielding posterior draws under a
standard regression model, using the tools developed in Chapter 6.

```{r}
library(BayesianLearningCode)
library(mvtnorm)
regression <- function(y, X, prior = "improper", b0 = 0, B0 = 1, c0 = 0.01,
                       C0 = 0.01, burnin = 1000L, M = 5000L) {
  
  N <- nrow(X)
  d <- ncol(X)
  
  if (length(b0) == 1L) b0 <- rep(b0, d)
  
  if (!is.matrix(B0)) {
    if (length(B0) == 1L) {
      B0 <- diag(rep(B0, d))
    } else {
      B0 <- diag(B0)
    }
  }
  
  if (prior == "improper") {

    fit <- lm.fit(X, y)
    betahat <- fit$coefficients
    SSR <- sum(fit$residuals^2)
    cN <- (N - d) / 2
    CN <- SSR / 2
    bN <- betahat
    BN <- solve(crossprod(X))
    
    sigma2s <- rinvgamma(M, cN, CN)
    betas <- matrix(NA_real_, M, d)
    for (i in seq_len(M)) betas[i,] <- rmvnorm(1, bN, sigma2s[i] * BN)
  
  } else if (prior == "conjugate") {
    
    B0inv <- solve(B0)
    BNinv <- B0inv + crossprod(X)
    BN <- solve(BNinv)
    bN <- BN %*% (B0inv %*% b0 + crossprod(X, y))
    Seps0 <- crossprod(y) + crossprod(b0, B0inv) %*% b0 -
      crossprod(bN, BNinv) %*% bN
    cN <- c0 + N / 2
    CN <- C0 + Seps0 / 2
    
    sigma2s <- rinvgamma(M, cN, CN)
    betas <- matrix(NA_real_, M, d)
    for (i in seq_len(M)) betas[i,] <- rmvnorm(1, bN, sigma2s[i] * BN)
  
  } else if (prior == "semi-conjugate") {
    
    # Precompute some values
    B0inv <- solve(B0)
    B0invb0 <- B0inv %*% b0
    cN <- c0 + N / 2
    XX <- crossprod(X)
    Xy <- crossprod(X, y)
    
    # Prepare memory to store the draws
    betas <- matrix(NA_real_, nrow = M, ncol = d)
    sigma2s <- rep(NA_real_, M)
    colnames(betas) <- colnames(X)
    
    # Set the tarting value for sigma2
    sigma2 <- var(y) / 2
    
    # Run the Gibbs sampler
    for (m in seq_len(burnin + M)) {
      # Sample beta from its full conditional
      BN <- solve(B0inv + XX / sigma2) 
      bN <- BN %*% (B0invb0 + Xy / sigma2)
      beta <- rmvnorm(1, mean = bN, sigma = BN)
      
      # Sample sigma^2 from its full conditional
      eps <- y - tcrossprod(X, beta)
      CN <- C0 + crossprod(eps) / 2
      sigma2 <- rinvgamma(1, cN, CN)
      
      # Store the results
      if (m > burnin) {
        betas[m - burnin,] <- beta
        sigma2s[m - burnin] <- sigma2
      }
    }
  }
  list(betas = betas, sigma2s = sigma2s)
}
```

Now we are ready to reproduce the results in the book.

## Example 7.1

We begin by writing a function that sets up the design matrix for an AR($p$)
model.

```{r}
ARdesignmatrix <- function(dat, p = 1) {
  d <- p + 1
  N <- length(dat) - p

  Xy <- matrix(NA_real_, N, d)
  Xy[, 1] <- 1
  for (i in seq_len(p)) {
    Xy[, i + 1] <- logret[(p + 1 - i) : (length(logret) - i)]
  }
  Xy
}
```

We obtain draws for four AR models under the improper prior.

```{r}
res <- vector("list", 4)
for (p in 1:4) {
  y <- tail(logret, -p)
  Xy <- ARdesignmatrix(logret, p)
  res[[p]] <- regression(y, Xy, prior = "improper")
}
```

## Figure 7.2
Now we plot the draws for the leading coefficient, i.e., the coefficient
corresponding to the highest lag in each of the models.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("7-2_2.pdf", width = 8, height = 5)
}
par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.2, .1), mgp = c(1.5, .5, 0))
for (p in 1:4) {
  hist(res[[p]]$betas[, p + 1], freq = FALSE, main = bquote(AR(.(p))),
       xlab = bquote(phi[.(p)]), ylab = "", breaks = seq(-.7, .7, .02))
}
```
