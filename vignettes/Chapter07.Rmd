---
title: "Chapter 7: Introduction to Bayesian Time Series Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Chapter 7: Introduction to Bayesian Time Series Analysis}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
knitr::opts_knit$set(global.par = TRUE)
pdfplots <- FALSE # default: FALSE; set this to TRUE only if you like pdf figures
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2.6, 2.6, 2.6, .4), lwd = 1.5)
```

# Section 7.2
## Figure 7.1
First, we load the data. Because of the extreme outliers during the COVID
pandemic, we restrict our analysis to the time before its outbreak.

```{r}
data("gdp", package = "BayesianLearningCode")
dat <- gdp[1:which(names(gdp) == '2019-10-01')]
```

Next, we compute the log returns.

```{r}
logret <- log(dat[-1]) - log(dat[-length(dat)])
```

Now we can plot the data and its empirical autocorrelation function.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("7-2_1.pdf", width = 12, height = 5)
  par(mar = c(2.6, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
}
par(mfrow = c(1, 2))
ts.plot(logret, main = "U.S. GDP log returns")
acf(logret, lag = 8, main = "")
title("Empirical autocorrelation function")
```

Before we move on, we define a function yielding posterior draws under a
standard regression model, using the tools developed in Chapter 6.

```{r}
library(BayesianLearningCode)
library(mvtnorm)
regression <- function(y, X, prior = "improper", b0 = 0, B0 = 1, c0 = 0.01,
                       C0 = 0.01, burnin = 1000L, M = 5000L) {
  
  N <- nrow(X)
  d <- ncol(X)
  
  if (length(b0) == 1L) b0 <- rep(b0, d)
  
  if (!is.matrix(B0)) {
    if (length(B0) == 1L) {
      B0 <- diag(rep(B0, d))
    } else {
      B0 <- diag(B0)
    }
  }
  
  if (prior == "improper") {

    fit <- lm.fit(X, y)
    betahat <- fit$coefficients
    SSR <- sum(fit$residuals^2)
    cN <- (N - d) / 2
    CN <- SSR / 2
    bN <- betahat
    BN <- solve(crossprod(X))
    
    sigma2s <- rinvgamma(M, cN, CN)
    betas <- matrix(NA_real_, M, d)
    for (i in seq_len(M)) betas[i,] <- rmvnorm(1, bN, sigma2s[i] * BN)
  
  } else if (prior == "conjugate") {
    
    B0inv <- solve(B0)
    BNinv <- B0inv + crossprod(X)
    BN <- solve(BNinv)
    bN <- BN %*% (B0inv %*% b0 + crossprod(X, y))
    Seps0 <- crossprod(y) + crossprod(b0, B0inv) %*% b0 -
      crossprod(bN, BNinv) %*% bN
    cN <- c0 + N / 2
    CN <- C0 + Seps0 / 2
    
    sigma2s <- rinvgamma(M, cN, CN)
    betas <- matrix(NA_real_, M, d)
    for (i in seq_len(M)) betas[i,] <- rmvnorm(1, bN, sigma2s[i] * BN)
  
  } else if (prior == "semi-conjugate") {
    
    # Precompute some values
    B0inv <- solve(B0)
    B0invb0 <- B0inv %*% b0
    cN <- c0 + N / 2
    XX <- crossprod(X)
    Xy <- crossprod(X, y)
    
    # Prepare memory to store the draws
    betas <- matrix(NA_real_, nrow = M, ncol = d)
    sigma2s <- rep(NA_real_, M)
    colnames(betas) <- colnames(X)
    
    # Set the tarting value for sigma2
    sigma2 <- var(y) / 2
    
    # Run the Gibbs sampler
    for (m in seq_len(burnin + M)) {
      # Sample beta from its full conditional
      BN <- solve(B0inv + XX / sigma2) 
      bN <- BN %*% (B0invb0 + Xy / sigma2)
      beta <- rmvnorm(1, mean = bN, sigma = BN)
      
      # Sample sigma^2 from its full conditional
      eps <- y - tcrossprod(X, beta)
      CN <- C0 + crossprod(eps) / 2
      sigma2 <- rinvgamma(1, cN, CN)
      
      # Store the results
      if (m > burnin) {
        betas[m - burnin,] <- beta
        sigma2s[m - burnin] <- sigma2
      }
    }
  }
  list(betas = betas, sigma2s = sigma2s)
}
```

Now we are ready to reproduce the results in the book.

## Example 7.1

We begin by writing a function that sets up the design matrix for an AR($p$)
model.

```{r}
ARdesignmatrix <- function(dat, p = 1) {
  d <- p + 1
  N <- length(dat) - p

  Xy <- matrix(NA_real_, N, d)
  Xy[, 1] <- 1
  for (i in seq_len(p)) {
    Xy[, i + 1] <- dat[(p + 1 - i) : (length(dat) - i)]
  }
  Xy
}
```

We obtain draws for four AR models under the improper prior.

```{r}
res <- vector("list", 4)
for (p in 1:4) {
  y <- tail(logret, -p)
  Xy <- ARdesignmatrix(logret, p)
  res[[p]] <- regression(y, Xy, prior = "improper")
}
```

## Figure 7.2
Now we plot the draws for the leading coefficient, i.e., the coefficient
corresponding to the highest lag in each of the models.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("7-2_2.pdf", width = 8, height = 5)
}
par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
for (p in 1:4) {
  hist(res[[p]]$betas[, p + 1], freq = FALSE, main = bquote(AR(.(p))),
       xlab = bquote(phi[.(p)]), ylab = "", breaks = seq(-.75, .75, .02))
}
```

Next, we fit an AR(3) model with different priors.

```{r}
y <- tail(logret, -3)
Xy <- ARdesignmatrix(logret, 3)

res_improper <- res[[3]]

res_conj_1 <- regression(y, Xy, prior = "conjugate",
                              b0 = rep(0, 4), B0 = diag(rep(1, 4)),
                              c0 = 2, C0 = 0.001)

res_semi_1 <- regression(y, Xy, prior = "semi-conjugate",
                              b0 = rep(0, 4), B0 = diag(rep(1, 4)),
                              c0 = 2, C0 = 0.001)

res_conj_2 <- regression(y, Xy, prior = "conjugate",
                              b0 = rep(0, 4), B0 = diag(rep(100, 4)),
                              c0 = 2, C0 = 0.001)

res_semi_2 <- regression(y, Xy, prior = "semi-conjugate",
                              b0 = rep(0, 4), B0 = diag(rep(100, 4)),
                              c0 = 2, C0 = 0.001)

```

## Figure 7.3
We now visualize the posterior of the intercept under all those priors.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("7-2_3.pdf", width = 12, height = 5, lwd = 1.5)
}
par(mfrow = c(1, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0))
dens_improper <- density(res_improper$betas[, 1], bw = "SJ", adj = 2)
dens_semi_1 <- density(res_semi_1$betas[, 1], bw = "SJ", adj = 2)
dens_semi_2 <- density(res_semi_2$betas[, 1], bw = "SJ", adj = 2)
  
plot(dens_improper, xlim = range(dens_improper$x, dens_semi_1$x, dens_semi_2$x),
     ylim = range(dens_improper$y, dens_semi_1$y, dens_semi_2$y),
     main = "Semi-conjugate priors", xlab = bquote(zeta), ylab = "")
lines(dens_semi_1, col = 2, lty = 2)
lines(dens_semi_2, col = 3, lty = 3)
legend("topright", c("Improper", "Tight", "Loose"), col = 1:3, lty = 1:3)

dens_improper <- density(res_improper$betas[, 1], bw = "SJ", adj = 2)
dens_conj_1 <- density(res_conj_1$betas[, 1], bw = "SJ", adj = 2)
dens_conj_2 <- density(res_conj_2$betas[, 1], bw = "SJ", adj = 2)
  
plot(dens_improper, xlim = range(dens_improper$x, dens_conj_1$x, dens_conj_2$x),
     ylim = range(dens_improper$y, dens_conj_1$y, dens_conj_2$y),
     main = "Conjugate priors", xlab = bquote(zeta), ylab = "")
lines(dens_conj_1, col = 2, lty = 2)
lines(dens_conj_2, col = 3, lty = 3)
legend("topleft", c("Improper", "Tight", "Loose"), col = 1:3, lty = 1:3)
```

## Figure 7.4
And the marginal posteriors for the autoregressive coefficients and the error
variance.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("7-2_4.pdf", width = 9, height = 5)
}
par(mfrow = c(4, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
for (p in 1:3) {
  dens_improper <- density(res_improper$betas[, p + 1], bw = "SJ", adj = 2)
  dens_semi_1 <- density(res_semi_1$betas[, p + 1], bw = "SJ", adj = 2)
  dens_semi_2 <- density(res_semi_2$betas[, p + 1], bw = "SJ", adj = 2)
  
  plot(dens_improper, xlim = range(dens_improper$x, dens_semi_1$x, dens_semi_2$x),
       ylim = range(dens_improper$y, dens_semi_1$y, dens_semi_2$y),
       main = "", xlab = bquote(phi[.(p)]), ylab = "")
  if (p == 1) title("Semi-conjugate priors")
  lines(dens_semi_1, col = 2, lty = 2)
  lines(dens_semi_2, col = 3, lty = 3)
  legend("topright", c("Improper", "Tight", "Loose"), col = 1:3, lty = 1:3)
  
  dens_improper <- density(res_improper$betas[, p + 1], bw = "SJ", adj = 2)
  dens_conj_1 <- density(res_conj_1$betas[, p + 1], bw = "SJ", adj = 2)
  dens_conj_2 <- density(res_conj_2$betas[, p + 1], bw = "SJ", adj = 2)
  
  plot(dens_improper, xlim = range(dens_improper$x, dens_conj_1$x, dens_conj_2$x),
       ylim = range(dens_improper$y, dens_conj_1$y, dens_conj_2$y),
       main = "", xlab = bquote(phi[.(p)]), ylab = "")
  if (p == 1) title("Conjugate priors")
  lines(dens_conj_1, col = 2, lty = 2)
  lines(dens_conj_2, col = 3, lty = 3)
  legend("topright", c("Improper", "Tight", "Loose"), col = 1:3, lty = 1:3)
}

dens_improper <- density(res_improper$sigma2, bw = "SJ", adj = 2)
dens_semi_1 <- density(res_semi_1$sigma2, bw = "SJ", adj = 2)
dens_semi_2 <- density(res_semi_2$sigma2, bw = "SJ", adj = 2)
  
plot(dens_improper, xlim = range(dens_improper$x, dens_semi_1$x, dens_semi_2$x),
     ylim = range(dens_improper$y, dens_semi_1$y, dens_semi_2$y),
     main = "", xlab = bquote(sigma^2), ylab = "")
lines(dens_semi_1, col = 2, lty = 2)
lines(dens_semi_2, col = 3, lty = 3)
legend("topright", c("Improper", "Tight", "Loose"), col = 1:3, lty = 1:3)
  
dens_improper <- density(res_improper$sigma2, bw = "SJ", adj = 2)
dens_conj_1 <- density(res_conj_1$sigma2, bw = "SJ", adj = 2)
dens_conj_2 <- density(res_conj_2$sigma2, bw = "SJ", adj = 2)
  
plot(dens_improper, xlim = range(dens_improper$x, dens_conj_1$x, dens_conj_2$x),
     ylim = range(dens_improper$y, dens_conj_1$y, dens_conj_2$y),
     main = "", xlab = bquote(sigma^2), ylab = "")
lines(dens_conj_1, col = 2, lty = 2)
lines(dens_conj_2, col = 3, lty = 3)
legend("topright", c("Improper", "Tight", "Loose"), col = 1:3, lty = 1:3)
```

## Section 7.2.2: Exploring stationarity during post-processing

We reuse the AR($p$) models under the improper prior from above to explore
stationarity for $p = 1, 2, 3$.

## Figure 7.5

```{r, echo = -c(1:2), fig.width = 9, fig.height = 3}
if (pdfplots) {
  pdf("7-2_5.pdf", width = 9, height = 3)
}
par(mfrow = c(1, 3), mar = c(2.5, 2.5, 1.2, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
hist(res[[1]]$betas[,2], breaks = 20, freq = FALSE, main = "AR(1)",
     xlab = bquote(phi), ylab = "")

plot(res[[2]]$betas[,2:3], main = "AR(2)", xlab = bquote(phi[1]),
     ylab = bquote(phi[2]), xlim = c(-2, 2), ylim = c(-1, 1),
     col = rgb(0, 0, 0, .05), pch = 16)
polygon(c(-2, 0, 2, -2), c(-1, 1, -1, -1), border = 2)

draws <- res[[3]]$betas[,2:4]
eigenvalues <- matrix(NA_complex_, nrow(draws), ncol(draws))
for (m in seq_len(nrow(draws))) {
  Phi <- matrix(c(draws[m,], c(1, 0, 0), c(0, 1, 0)), byrow = TRUE, nrow = 3)
  eigenvalues[m,] <- eigen(Phi, only.values = TRUE)$values
}
plot(eigenvalues, xlim = c(-1, 1), ylim = c(-1, 1), asp = 1,
     main = "AR(3)", col = rgb(0, 0, 0, .05), pch = 16)
symbols(0, 0, 1, add = TRUE, fg = 2, inches = FALSE)
```

We now move towards analyzing EU inflation data.

```{r}
data("inflation", package = "BayesianLearningCode")
```

First, we plot the data and its empirical autocorrelation function.

## Figure 7.6

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("7-2_6.pdf", width = 12, height = 5)
  par(mar = c(2.6, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
}
par(mfrow = c(1,2))
ts.plot(inflation, main = "EU Inflation")
acf(inflation, main = "")
title("Empirical autocorrelation function")
```

We fit AR($p$) models under the improper prior to explore stationarity for
$p = 1, 2, 3$.

```{r}
res <- vector("list", 4)
for (p in 1:3) {
  y <- tail(inflation, -p)
  Xy <- ARdesignmatrix(inflation, p)
  res[[p]] <- regression(y, Xy, prior = "improper")
}
```

## Figure 7.7

```{r, echo = -c(1:2), fig.width = 9, fig.height = 3}
if (pdfplots) {
  pdf("7-2_7.pdf", width = 9, height = 3)
}
par(mfrow = c(1, 3), mar = c(2.5, 2.5, 1.2, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
hist(res[[1]]$betas[,2], breaks = 20, freq = FALSE, main = "AR(1)",
     xlab = bquote(phi), ylab = "")
abline(v = 1, col = 2)

plot(res[[2]]$betas[,2:3], main = "AR(2)", xlab = bquote(phi[1]),
     ylab = bquote(phi[2]), xlim = c(-2, 2), ylim = c(-1, 1),
     col = rgb(0, 0, 0, .05), pch = 16)
polygon(c(-2, 0, 2, -2), c(-1, 1, -1, -1), border = 2)

draws <- res[[3]]$betas[,2:4]
eigenvalues <- matrix(NA_complex_, nrow(draws), ncol(draws))
for (m in seq_len(nrow(draws))) {
  Phi <- matrix(c(draws[m,], c(1, 0, 0), c(0, 1, 0)), byrow = TRUE, nrow = 3)
  eigenvalues[m,] <- eigen(Phi, only.values = TRUE)$values
}
plot(eigenvalues, xlim = c(-1, 1), ylim = c(-1, 1), asp = 1,
     main = "AR(3)", col = rgb(0, 0, 0, .05), pch = 16)
symbols(0, 0, 1, add = TRUE, fg = 2, inches = FALSE)
```
