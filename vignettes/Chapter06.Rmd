---
title: "Chapter 6"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Chapter 6}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
knitr::opts_knit$set(global.par = TRUE)
pdfplots <- FALSE # default: FALSE; set this to TRUE only if you like pdf figures
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2.6, 2.6, 2.6, .4), lwd = 1)
```
# Example 6.1
# Movie data 

We use movie data provided as "movies.rda" to illustrate Bayesian analysis of a regression model. The data set is 
a preprocessed version of the one provided by Lehrer and Xi (2017).
First of all, as there is only one film of genre G, we set the baseline for the categorical covariate genre to 
G or PG by removing PG from the data set. 

```{r}
library("BayesianLearningCode")
data("movies", package = "BayesianLearningCode")
movies["PG"] <- NULL
```

Next we prepare the variables for regression analysis. We define the response variable OpenBoxOffice as y and center the covariates at zero. 

```{r}
y<- movies[, "OpenBoxOffice"]

data.cen <- movies[,-1]
 vbin<- c("Action","Adventure","Animation","Comedy","Crime", 
   "Drama", "Family","Fantasy", "Mystery", "Romance",
   "Sci-Fi", "Thriller", "PG13", "R") 
data.cen[,vbin]<-scale(movies[,vbin],scale=FALSE)

 vmetr<- c("Budget", "Weeks", "Screens", 
           "S-21-27","S-14-20","S-7-13","S-4-6","S-1-3",
            "Vol-21-27","Vol-14-20","Vol-7-13","Vol-4-6",
          "Vol-1-3")
 
data.cen[,vmetr]<-scale(movies[,vmetr],scale=FALSE)

N=length(y)  # number of Observations
X=cbind(rep(1,N),as.matrix(data.cen)) # Regressor matrix
d=dim(X)[2]-1 # regression effects except the intercept
```

We first estimate the parameters of the regression model under a rather flat semi-conjugate prior.

```{r}
# define prior parameters
B0.inv=diag(c(1/10000,rep(1/100,d)), nrow=d+1)
b0=rep(0,d+1)

c0=2.5 
C0=1.5

# define quantities for the Gibbs sampler
XX <- crossprod(X)
Xy<- t(X)%*%y
cN=c0+N/2

#define burnin and M
burnin=1000
M=5000

# prepare storing of results
betas=matrix(NA, nrow=burnin+M, ncol=d+1)
sigma2s=rep(NA,burnin+M )

require("mvtnorm")

# starting value for sigma2
sigma2=var(y)/2

for (m in 1:(burnin+M)){
    # sample  beta from the full conditional
    Bn <- solve(B0.inv+XX/sigma2) 
    bn <- Bn%*%(B0.inv%*%b0+Xy/sigma2)
    beta=t(rmvnorm(n=1,mean=bn,sigma=Bn))
    
    # sample sigma^2 from its full conditional
    eps<-y-X%*%beta
    CN<-C0+ crossprod(eps)/2
    sigma2<-rinvgamma(1,cN,CN)
    
    betas[m,]<-beta
    sigma2s[m]<- sigma2
}
```
To summarize the results nicely we  compute equal tailed 95% confidence intervals. 
```{r}
eq.tails<-function(x)c(quantile(x,0.025), quantile(x,0.975))

beta.res1<- betas[burnin+(1:M),]
beta.eq1<-t(apply(beta.res1,2,eq.tails))

res1 <- cbind(colMeans(beta.res1), beta.eq1)
rownames(res1) <- c("Intercept", colnames(data.cen))
colnames(res1) <- c("post.mean", "2.5%", "97.5" )
print(round(res1,3))

sigma2.res1<- sigma2s[burnin+(1:M)]
print(round(c(mean(sigma2.res1), eq.tails(sigma2.res1)),3))
```
Next we use the horseshoe prior to analyse the data. We use 
the same prior on the intercept but specify the horseshoe prior via latent Inverse Gamma parameters. 

```{r,echo=-(1:2)}
par(mfrow = c(1, 3))
set.seed(42)

B0inv.intcpt=1/10000
b0=rep(0,d+1)

c0=2.5 
C0=1.5

# define quantities for the Gibbs sampler
XX <- crossprod(X)
Xy<- t(X)%*%y
cN=c0+N/2

#define burnin and M
burnin=1000
M=5000

# prepare storing of results
betas=matrix(NA, nrow=burnin+M, ncol=d+1)
lambda2s=rep(NA, burnin+M)
nus=rep(NA, burnin+M)
tau2s=matrix(NA, nrow=burnin+M, ncol=d)
xis=matrix(NA, nrow=burnin+M, ncol=d)
sigma2s=rep(NA,burnin+M)

# starting value for sigma2
sigma2=var(y)/2

lambda2<- 1
xi<-1

tau2<-rep(1,d)
nu<- rep(1,d)

for (m in 1:(burnin+M)){
    # sample  beta from the full conditional
    B0.inv=diag(c(B0inv.intcpt, 1/(lambda2*tau2)))
    Bn <- solve(B0.inv+XX/sigma2) 
    bn <- Bn%*%(B0.inv%*%b0+Xy/sigma2)
    beta=t(rmvnorm(n=1,mean=bn,sigma=Bn))
    
    # sample lambda2 and nu
    lambda2=rinvgamma(1,d/2,
                      1/xi+sum(beta[-1]^2/(2*lambda2)))
    nu=rinvgamma(1,1,1+1/lambda2)
    
    #sample tau2 and xi
    tau2 <- rinvgamma(d,1,
                   1/nu+(beta[-1]^2)/(2*tau2))
    xi <- rinvgamma(d,1,1+1/tau2)
    
    # sample sigma^2 from its full conditional
    eps <- y-X%*%beta
    CN <- C0+ crossprod(eps)/2
    sigma2 <- rinvgamma(1,cN,CN)
    
    # Save the draws
    betas[m,]<-beta
    lambda2s[m] <- lambda2
    nus[m]<- nu
    tau2s[m,]<- tau2
    xis[m,] <- xi
    sigma2s[m]<- sigma2
}
```
Again we compose a table with posterior mean estimates and
equal tailed 95% credibility intervals

```{r}
beta.res2<- betas[burnin+(1:M),]
beta.eq2<-t(apply(beta.res2,2,eq.tails))
res2 <- cbind(colMeans(beta.res2), beta.eq2)
rownames(res2) <- c("Intercept", colnames(data.cen))
colnames(res2) <- c("post.mean", "2.5%", "97.5" )

print(round(res2,3))

sigma2.res2<- sigma2s[burnin+(1:M)]
print(round(c(mean(sigma2.res2), eq.tails(sigma2.res2)),3))
```
We next have a look at the posterior distributions: first under the semi-conjugate priors and then under the Horseshoe prior
```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("6-4_1.pdf", width = 8, height = 5)
  par(mar = c(2.2, 1.6, 1.6, .2), mgp = c(1.2, .5, 0))
}
par(mfrow=c(7,4))
for (i in 1:(d+1)){
 hist(beta.res1[,i])
}

for (i in 1:(d+1)){
 hist(beta.res2[,i])
} 

```
Whereas the posterior distributions are symmetric under the semi-conjugate prior this is not the case under the Horseshoe prior. 

We next investigate now the MCMC plots. 
```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("6-4_2.pdf", width = 8, height = 5)
  par(mar = c(2.2, 1.6, 1.6, .2), mgp = c(1.2, .5, 0))
}
par(mfrow=c(7,4))
for (i in 1:(d+1)){
 plot(beta.res1[,i], type="l")
}

for (i in 1:(d+1)){
 plot(beta.res2[,i], type="l")
} 
```
Finally have a look at boxplots of the posterior distributions of the local shrinkage parameters $\tau_j$under the horseshoe prior  


```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("6-4_3.pdf", width = 8, height = 5)
  par(mar = c(2.2, 1.6, 1.6, .2), mgp = c(1.2, .5, 0))
}
tau2.res<-tau2s[burnin+(1:M),]

require(graphics)
boxplot(tau2.res,ylim=c(0,100))
```

The residuals of the model are however not normally distributed.
```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("6-4_4.pdf", width = 8, height = 5)
  par(mar = c(2.2, 1.6, 1.6, .2), mgp = c(1.2, .5, 0))
}
res1=y-X%*%colMeans(beta.res1)
res2=y-X%*%colMeans(beta.res2)
par(mfrow=c(1,2))
qqnorm(res1)
qqnorm(res2)

```
