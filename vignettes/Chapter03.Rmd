---
title: "Chapter 3"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Chapter 3}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
knitr::opts_knit$set(global.par = TRUE)
pdfplots <- FALSE # default: FALSE; set this to TRUE only if you like pdf figures
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2.6, 2.6, 2.6, .4), lwd = 1)
```

# Section 3.1
## Figure 3.1: Posteriors under the beta-binomial model

To reproduce the posteriors in this figure, we simply need to plug in
respective counts into the expression for the posterior density and visualize
it accordingly.

```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("3-1_1.pdf", width = 8, height = 5)
  par(mgp = c(1, .5, 0), mar = c(2.2, 1.5, 2, .2), lwd = 2)
}
par(mfrow = c(3, 2))
trueprop <- c(0, .1, .5)
N <- c(100, 400)
xs <- seq(0, 1, .001)

for (p in trueprop) {
  for (n in N) {
    aN <- n*p + 1
    bN <- n - n*p + 1
    plot(xs, dbeta(xs, aN, bN), type = "l", xlab = expression(vartheta), ylab = "",
         main = bquote(N == .(n) ~ "and" ~ S[N] == .(n*p)))
  }
}
```

## Table 3.1: Posterior credible intervals under the beta-binomial model

We now proceed to computing credible intervals. Note that R is (generally)
vectorized, so we can compute all posterior parameters in one line, without
the need for a loop.

```{r}
gamma <- .95
alpha <- 1 - gamma

Ns <- rep(N, each = length(trueprop))
SNs <- Ns * rep(trueprop, length(N))

aN <- SNs + 1    
bN <- Ns - SNs + 1
    
# Equal-ended 95% credible intervals
leftEE <- qbeta(alpha/2, aN, bN)
rightEE <- qbeta(1 - alpha/2, aN, bN)

# HPDs
resolution <- 10000
grid <- seq(0, 1, length.out = resolution + 1)
dist <- gamma * resolution

leftHPD <- rightHPD <- rep(NA_real_, length(aN))
for (i in seq_along(aN)) {
  qs <- qbeta(grid, aN[i], bN[i])
  minimizer <- which.min(diff(qs, lag = dist))
  leftHPD[i] <- qs[minimizer]
  rightHPD[i] <- qs[minimizer + dist]
}

res <- cbind(leftEE, rightEE, leftHPD, rightHPD)
```

All the desired intervals are now stored and can be displayed.

```{r}
knitr::kable(round(res, 4))
```

We can also compare their lengths.

```{r}
res <- cbind(lengthEE = rightEE - leftEE, lengthHPD = rightHPD - leftHPD)
knitr::kable(round(res, 4))
```

## Figure 3.2: One-sided hypthesis testing

We now move forward to assessing visualizing the posterior probability of
$\vartheta$ (the proportion of defective items) being less than $1/20 = 0.05$
for $N = 100$ and $S_N \in \{1,3,6,10\}$.

```{r, echo=-(1:2)}
if (pdfplots) {
  pdf("3-1_2.pdf", width = 10, height = 7)
  par(mgp = c(1, .5, 0), mar = c(2.2, 1.5, 2, .2), lwd = 2)
}
par(mfrow = c(2, 2))
theta <- seq(0, .15, .001)
N <- 100
SN <- c(1, 3, 6, 10)
aN <- SN + 1
bN <- N - SN + 1
for (i in seq_along(SN)) {
  plot(theta, dbeta(theta, aN[i], bN[i]), type = "l", ylim = c(0, 40),
       xlab = expression(vartheta), ylab = "",
       main = bquote(N == 100 ~ "and" ~ S[N] == .(SN[i]) ~ "     " ~
                       P(vartheta <= 0.05 ~ "|" ~ bold(y)) ~ "=" ~
                       .(round(pbeta(0.05, aN[i], bN[i]), 3))))
  abline(h = 0, lty = 2)
  polygon(c(theta[theta <= 0.05], 0.05),
          c(dbeta(theta[theta <= 0.05], aN[i], bN[i]), 0), col = "red")
}
```

## Example 3.4: Bag of Words
### Preparing the Data

After reading in the quote, we do some simple manipulation such as converting
to lower case, getting rid of newlines, and splitting the string into individual
words. Finally, we remove potential empty words and display the resulting
frequencies.

```{r}
string <- "You can fool some of the people all of the time,
and all of the people some of the time,
but you can not fool all of the people all of the time."

tmp <- tolower(string)                  ## convert to lower case
tmp <- gsub("\n", " ", tmp)             ## replace newlines with spaces
tmp <- unlist(strsplit(tmp, " |,|\\.")) ## split at spaces, commas, or stops
dat <- tmp[tmp != ""]                   ## remove empty words
tab <- table(dat)
knitr::kable(t(tab))
```

To define our universe of possible words, we first look at the *words* dataset
(shipped with this package). It contains 1000 most common English words,
alongside their frequency of usage among a total of 100 million occurrences
(cf. <https://www.eapfoundation.com/vocab/general/bnccoca/>).

```{r}
library(BayesianLearningCode)
head(words)
```

For illustration purposes, we only use the top 50 words and
augmented those with a question mark for all other words. This leaves us with 51
possible outcomes (our *universe*).

```{r}
top50 <- tail(words[order(words$frequency),], 50)
universe <- c("?", sort(top50$word))
freq <- c(10^8 - sum(top50$frequency), top50$frequency[order(top50$word)])
universe
```

Now, we check if there are any words in our data set which are not included
in our universe (here, only *fool* and *people*) and replace these with a question mark.

```{r}
dat[!(dat %in% universe)] <- "?"
knitr::kable(t(table(dat)))
```

### Computing the Posterior Under the Uniform Prior

Under a uniform prior (i.e., all 51 elements of our universe receive one
pseudo-count), the posterior mean for each word occurrence probability is given
by
$$
E(\eta_k|\boldsymbol{y}) = \frac{1+N_k}{ 51 + N}, \quad k=1,\ldots,51,
$$
where $N_k$ stands for the number of data occurrences of the $k$th word in the
universe, and $N$ is the sum of all $N_k$s. This can be very easily implemented
by merging the universe and the data,
and simply counting the resulting frequencies.

```{r}
merged <- c(universe, dat)
counts <- table(merged) - 1L
post_uniform_unnormalized <- 1 + counts
post_uniform <- post_uniform_unnormalized / sum(post_uniform_unnormalized)
```

### Computing the Posterior Under a Less Informative Prior

Note that the above strategy implicitly assumes that we have exactly 51
pseudo-observations. To render the prior less influential, we can rescale
it to, e.g., 1 pseudo-observation. Then, the posterior expectation is
$$
E(\eta_k|\boldsymbol{y}) = \frac{1/51+N_k}{1 + N}, \quad k=1,\ldots,51.
$$
To compute this expectation we simply add counts and the new pseudo-counts
$\gamma_{0,1}, = \gamma_{0,2} = \dots = \gamma_{0,K} = 1/K$.

```{r}
K <- length(universe)
N0 <- 1
gamma0 <- rep(N0 / K, length(universe))
post_lessinformative_unnormalized <-  gamma0 + counts
post_lessinformative <-
  post_lessinformative_unnormalized / sum(post_lessinformative_unnormalized)
```

Alternatively, we could use a loop.

```{r}
post_lessinformative_unnormalized2 <- rep(NA_real_, length(universe))
for (i in seq_along(universe)) {
  Nk <- sum(dat == universe[i])
  post_lessinformative_unnormalized2[i] <- gamma0[i] + Nk
}
post_lessinformative2 <-
  post_lessinformative_unnormalized2 / sum(post_lessinformative_unnormalized2)
```

The results must be numerically equivalent, and we can verify this easily.

```{r}
all(abs(post_lessinformative - post_lessinformative2) < 1e-10)
```

Summing up what we have so far.

```{r}
dirichlet_sd = function(gamma) {
  mean <- gamma / sum(gamma)
  sd <- sqrt((mean * (1 - mean)) / (sum(gamma) + 1))
  sd
}

resfull <- cbind(prior_mean = rep(1/K, K),
                 prior_sd_uniform = dirichlet_sd(rep(1, K)),
                 prior_sd_lessinformative = dirichlet_sd(rep(1/K, K)),
                 rel_freq = counts / sum(counts),
                 posterior_mean_uniform = post_uniform,
                 posterior_sd_uniform = dirichlet_sd(post_uniform),
                 posterior_mean_lessinformative = post_lessinformative,
                 posterior_sd_lessinformative = dirichlet_sd(post_lessinformative))

unseen <- counts == 0L
res <- rbind(resfull[!unseen,], UNSEEN = resfull[which(unseen)[1],])
knitr::kable(t(round(res, 4)))
```

### Computing the Posterior Under an Informed Prior

One might consider using yet another prior (which we label *informed*).
For instance, we might want to fix
the total number of pseudo-counts $N_0$ to, say, one fifth of the number of
observations $N$
(this implies a data to prior ratio of 5 to 1). Each word
in the universe is then weighted according to its frequency of appearance in the
English language. Remember that the prior probability for a word outside of the
top 50 English words is $10^8$ (the total number of words in the corpus)
minus the sum of the top 50 counts. To compute the posterior, we can again
add the actual counts and the new pseudo-counts.

```{r}
N <- length(dat)
N0 <- N/5
gamma0 <- N0 * freq / 10^8

post_informed_unnormalized <- gamma0 + counts
post_informed <- post_informed_unnormalized / sum(post_informed_unnormalized)
```

Alternatively, we might want to prefer heavier shrinkage with respect to the
base rate (here, the overall word distribution in the English language).
This can simply be accomplished by increasing the total
number of pseudo-counts $N_0$.

```{r}
N0 <- 5*N
gamma0 <- N0 * freq / 10^8

post_informed_unnormalized2 <- counts + gamma0
post_informed2 <- post_informed_unnormalized2 / sum(post_informed_unnormalized2)
```

### Visualizing the Results

To display the results, we use simple bar plots. For didactic purposes,
we also add the prior probabilities of each word via green circles.

```{r, echo=-(1:2), fig.height = 8}
if (pdfplots) {
  pdf("3-3_1.pdf", width = 7, height = 9)
  par(mgp = c(1, .6, 0), mar = c(2.8, 2.3, 1.5, 0), lwd = 1)
}
par(mfrow = c(4, 1), xpd = TRUE)
midpts <- barplot(post_uniform, las = 2,
                  ylim = c(0, max(counts/N, post_uniform)))
points(midpts, counts/N, col = 2, pch = 1)
points(midpts, rep(1/51, length(midpts)), col = 3, pch = 2)
title("Uniform prior")
legend("topright", c("Observed frequencies", "Prior expectations"),
       col = 2:3, pch = 1:2)

midpts <- barplot(post_lessinformative, las = 2,
                  ylim = c(0, max(counts/N, post_lessinformative)))
points(midpts, counts/N, col = 2, pch = 1)
points(midpts, rep(1/51, length(midpts)), col = 3, pch = 2)
title("Less informative prior")

midpts <- barplot(post_informed, las = 2,
                   ylim = c(0, max(counts/N, post_informed, freq/10^8)))
points(midpts, counts/N, col = 2, pch = 1)
points(midpts, freq / 10^8, col = 3, pch = 2)
title("Informed prior (light shrinkage)")

midpts <- barplot(post_informed2, las = 2,
                   ylim = c(0, max(counts/N, post_informed2, freq/10^8)))
points(midpts, counts/N, col = 2, pch = 1)
points(midpts, freq /10^8, col = 3, pch = 2)
title("Informed prior (heavy shrinkage)")
```

In the top panel (based on the uniform prior), we can nicely see the
*add-one smoothing* effect, whereas the second panel shows almost no
smoothing, and unseen words are estimated to be extremely unlikely. The two bottom
panels show the posteriors under the informed priors, one with with light and
one heavy shrinkage.
