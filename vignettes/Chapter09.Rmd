---
title: "Chapter 9: Bayesian Predictive Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Chapter 9: Bayesian Predictive Analysis}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
knitr::opts_knit$set(global.par = TRUE)
pdfplots <- FALSE # default: FALSE; set this to TRUE only if you like pdf figures
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2.6, 2.6, 2.6, .4), lwd = 1)
```

# Section 9.1: From Bayesian Posterior to Bayesian Predictive Inference
## Example 9.1: Road Safety Data

We load the data and extract the observations for the senior people in
Linz. We then plot the pdf and cdf for the predictive distribution
which corresponds under the flat prior to 
$$
y_f|\mathbf{y} \sim \mathcal NB(N\bar y + 1, N).
$$

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("9-1_1.pdf", width = 10, height = 4)
  par(mar = c(2.5, 2, .1, .5), mgp = c(1.6, .6, 0))
}
par(mfrow = c(1, 2))
data("accidents", package = "BayesianLearningCode")
y <- accidents[, "seniors_accidents"]
(aN <- sum(y) + 1)
(bN <- length(y))
mu <- aN / bN
yf <- 0:20
plot(yf, dnbinom(yf, size = aN, mu = mu),
     type = "h", xlab = bquote(y[f]), ylab = "", lwd = 2)
plot(yf, pnbinom(yf, size = aN, mu = mu),
     type = "h", xlab = bquote(y[f]), ylab = "", lwd = 2)
probs <- c(0.025, 0.975)
abline(h = probs, lty = 3)
mtext(probs, side = 2, at = probs, adj = c(0, 1), cex = .8, col = "dimgrey")
```

## Example 9.2: Exchange Rate Data

We load the data and then plot the pdf and cdf for the predictive
distribution which corresponds under the improper prior to
$$
y_f|\mathbf{y} \sim \mathcal t_{2c_N}\left(\bar y, \frac{N s_y^2}{N-1}\left(1 + \frac{1}{N}\right)\right).
$$

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("9-1_2.pdf", width = 10, height = 4)
  par(mar = c(2.5, 2, .1, .5), mgp = c(1.6, .6, 0))
}
par(mfrow = c(1, 2))
library("BayesianLearningCode")
data("exrates", package = "stochvol")
y <- 100 * diff(log(exrates$USD / exrates$CHF))
ybar <- mean(y)
N <- length(y)
b0 <- 0
N0 <- 0
c0 <- -1/2
C0 <- 0
BN <- 1 / (N + N0)
bN <- BN * (N * ybar + N0 * b0)
cN <- c0 + N/2
CN <- C0 + .5 * sum((y - ybar)^2) + N0 * N / (2 * (N0 + N)) * (b0 - ybar)^2
x <- seq(-3, 3, length.out = 200)
scale <- sqrt(CN / cN * (BN + 1))
plot(x, dstudt(x, location = bN, scale = scale, df = 2 * cN),
     type = "l", xlab = bquote(y[f]), ylab = "", lwd = 1.5)
plot(x, pstudt(x, location = bN, scale = scale, df = 2 * cN),
     type = "l", xlab = bquote(y[f]), ylab = "", lwd = 1.5)
probs <- c(0.025, 0.975)
abline(h = probs, lty = 3)
mtext(probs, side = 2, at = probs, adj = c(0, 1), cex = .8, col = "dimgrey")
```

We inspect the parameters of the Student-t distribution.

```{r}
round(c(mu = bN, sigma2 = scale^2, df = 2 * cN), digits = 3)
```

## Example 9.3: Exchange Rate Data cont'd

To compare with a method that ignores parameter uncertainty, we now plot
the posterior predictive alongside the "classical" forecasting distribution
for varying $N$.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("9-1_3.pdf", width = 8, height = 5)
  par(mar = c(2.5, 2, 1.5, .5), mgp = c(1.6, .6, 0))
}
par(mfrow = c(2, 2))
Ns <- c(5, 10, 30, N)

for (i in seq_along(Ns)) {
  N <- Ns[i]
  yshort <- y[1:N]
  ybar <- mean(yshort)
  BN <- 1 / (N + N0)
  bN <- BN * (N * ybar + N0 * b0)
  cN <- c0 + N/2
  CN <- C0 + .5 * sum((yshort - ybar)^2) + N0 * N / (2 * (N0 + N)) * (b0 - ybar)^2
  scale <- sqrt(CN / cN * (BN + 1))
  plot(x, dnorm(x, ybar, sd(yshort)), lty = 2, type = "l", lwd = 1.5,
       xlab = bquote(y[f]), ylab = "", main = paste("N =", N))
  lines(x, dstudt(x, location = bN, scale = scale, df = 2 * cN), lwd = 1.5)
}
```

## Example 9.4: Road Safety Data cont'd

We verify that a 95% predictive interval is given by [1, 9] using the
cdf and compute the effective coverage.

```{r}
pnbinom(9, size = aN, mu = mu) - pnbinom(0, size = aN, mu = mu)
```

## Example 9.5: Exchange Rate Data cont'd

We determine the 95% predictive interval using the quantile function.

```{r}
round(qstudt(probs, location = bN, scale = scale, df = 2 * cN), digits = 3)
```

We now work out the effective coverage of the naive interval which ignores
parameter uncertainty.

```{r}
coverage <- rep(NA_real_, length(Ns))
names(coverage) <- Ns
for (i in seq_along(Ns)) {
  N <- Ns[i]
  yshort <- y[1:N]
  quants <- qnorm(probs, mean(yshort), sd(yshort))
  coverage[i] <- diff(pstudt(quants, mean(yshort), sd(yshort) * sqrt(1 + 1/N),
                             2 * (c0 + N/2)))
}
knitr::kable(t(round(coverage, 4)))
```

# Section 9.3: A Sampling-Based Approach to Prediction
## Example 9.8: Sampling-based prediction for the CHF/USD exchange rates

We proceed exactly as in Chapter 4. For the Gaussian distribution, the posterior
of $\sigma^2$ is inverse gamma, and we can easily generate iid draws.

```{r}
set.seed(2)
ndraws <- 10000
sigma2draws_normal <- rinvgamma(ndraws, N / 2, sum(y^2) / 2)
```

For the Student-$t$ model, we can use inverse transform sampling. First, we draw
uniformly from the interval spanned by 0 and the maximum of the non-normalized
cumulative posterior. Then, for each draw, we find the interval of our pointwise
cdf approximation of the posterior, and interpolate linearly between the
interval boundaries.

```{r}
# We need the nonnormalized cumulative posterior distribution
post_nonnormalized_nonvec <- function(sigma2, y, nu, log = FALSE) {
  logdens <- -length(y) / 2 * log(sigma2) -
    (nu + 1) / 2 * sum(log(1 + y^2 / (nu * sigma2))) - log(sigma2)
  if (log) logdens else exp(logdens)
}
post_nonnormalized <- Vectorize(post_nonnormalized_nonvec, "sigma2")

# Now, we compute the pdf and cdf on a reasonably chosen grid
nu <- 7
sigma2 <- seq(0.25, 0.45, length.out = 3000)
pdf_u <- post_nonnormalized(sigma2, y = y, nu = nu)
pdf_u <- pdf_u / max(pdf_u)
cdf_u <- cumsum(pdf_u) / sum(pdf_u)

# Now we can perform inverse transpose sampling
unifdraws <- runif(ndraws, 0, cdf_u[length(cdf_u)])
leftind <- findInterval(unifdraws, cdf_u)
rightind <- leftind + 1L
distprop <- (unifdraws - cdf_u[leftind]) / (cdf_u[rightind] - cdf_u[leftind])
sigma2draws_t <- sigma2[leftind] + distprop *
  (sigma2[rightind] - sigma2[leftind])
```

Next, we simulate draws from the posterior predictive. 

```{r}
yf_normal <- rnorm(ndraws, 0, sqrt(sigma2draws_normal))
yf_t <- rstudt(ndraws, 0, sqrt(sigma2draws_t), 7)
```

Now we can compute their empirical quantiles, and those of the data.

```{r}
quants <- c(0.01, 0.05, 0.25, 0.4, 0.5, 0.6, 0.75, 0.95, 0.99)
q_normal <- quantile(yf_normal, quants)
q_t <- quantile(yf_t, quants)
q_y <- quantile(y, quants)

knitr::kable(round(t(cbind("Data" = q_y, "Student t" = q_t,
                           "Gaussian" = q_normal)), 3))
```

We conclude by visualizing the data and the predictive distributions.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("9-3_1.pdf", width = 10, height = 4)
  par(mar = c(2.7, 2, 1.5, .5), mgp = c(1.6, .6, 0))
}
par(mfrow = c(1, 2))
minmax <- ceiling(10 * max(abs(y))) / 10
grid <- seq(-minmax, minmax, length.out = 50)
hist(y, freq = FALSE, breaks = grid, border = NA,
     main = "Histogram and predictive densitites")
lines(density(yf_normal, adjust = 2), col = 4, lty = 1, lwd = 1.5)
lines(density(yf_t, adjust = 2), col = 2, lty = 2, lwd = 1.5)
legend("topleft", c("Normal", "Student t"), lty = 1:2, col = c(4,2), lwd = 1.5)
ts.plot(y, main = "Time series plot and some predictive quantiles")
abline(h = q_normal, col = 4, lty = 1, lwd = 1.5)
abline(h = q_t, col = 2, lty = 2, lwd = 1.5)
legend("topleft", c("Normal", "Student t"), lty = 1:2, col = c(4,2), lwd = 1.5)
```


## Example 9.9: Predicting yearly maxima for the road safety data

We use a sampling-based approach to obtain draws from posterior predictive
by first drawing from the posterior $\mu|\mathbb{y} \sim \mathcal{G}(a_N, b_N)$.
Then, using these draws as mean parameters for the Poisson likelihood, we draw
12 times each to obtain yearly predictions. Of these, we take the maxima.

```{r}
set.seed(1)
y <- accidents[, "seniors_accidents"]
aN <- sum(y) + 1
bN <- length(y)
mus <- rgamma(ndraws, aN, bN)
yfs <- matrix(rpois(12 * ndraws, mus), ncol = 12)
Us <- apply(yfs, 1, max)
```

Now we visualize.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("9-3_2.pdf", width = 10, height = 4)
  par(mar = c(2.5, 2, .5, .5), mgp = c(1.6, .6, 0))
}
par(mfrow = c(1, 2))
plot(tab <- proportions(table(Us)), xlab = "U", ylab = "")
plot(as.table(cumsum(tab)), type = "h", xlab = "U", ylab = "")
probs <- c(0.025, 0.975)
abline(h = probs, lty = 3)
mtext(probs, side = 2, at = probs, adj = c(0, 1), cex = .8, col = "dimgrey")
```

## Example 9.11

We illustrate the variance of the purely sampling-based estimator versus
the Rao-Blackwellized version by running several experiments.

```{r}
set.seed(42)

a0 <- 1
b0 <- 1
N <- 100
SN <- 42
H <- 10
M <- 100
nsim <- 1000

# compute the posterior parameters:
aN <- a0 + SN
bN <- b0 + N - SN
mu <- aN / bN

pk1 <- pk2 <- matrix(NA_real_, nsim, H + 1)
colnames(pk1) <- colnames(pk2) <- 0:10

for (i in seq_len(nsim)) {
  # purely sampling-based:
  thetas <- rbeta(M, aN, bN)
  Sfs <- rbinom(M, H, thetas)
  pk1[i, ] <- proportions(tabulate(Sfs + 1, nbins = 11)) # tabulate starts at 1
  
  # Rao-Blackwellized:
  for (k in 0:H) {
    pk2[i, k + 1] <- mean(dbinom(k, H, thetas))
  }
}
```

To compute the exact probabilities, we need the density function of the
beta-binomial distribution (which is not part of base R).

```{r}
dbetabinom <- function(x, n, a, b, log = FALSE) {
  logdens <- lchoose(n, x) + lbeta(x + a, n - x + b) - lbeta(a, b)
  if (log) logdens else exp(logdens)
}
```

Now we can easily evaluate the probabilities.

```{r}
pk3 <- dbetabinom(0:H, H, aN, bN)
```

To concluse, we visualize.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("9-3_3.pdf", width = 10, height = 4)
  par(mar = c(2.5, 2, 1.5, .5), mgp = c(1.6, .6, 0))
}
par(mfrow = c(1, 2))
boxplot(pk1, xlab = "k", range = 0, main = "Purely sampling-based")
points(pk3, col = 3, cex = 1.5, pch = 16)

boxplot(pk2, xlab = "k", range = 0, main = "Rao-Blackwellized",
        ylim = range(pk1))
points(pk3, col = 3, cex = 1.5, pch = 16)
```


# Section 9.5: Bayesian Forecasting of Time Series

## Example 2.14: One-step-ahead forecasting of GDP

For creating the design matrix for an AR model, we re-use the function from
Chapter 7.

```{r}
ARdesignmatrix <- function(dat, p = 1) {
  d <- p + 1
  N <- length(dat) - p

  Xy <- matrix(NA_real_, N, d)
  Xy[, 1] <- 1
  for (i in seq_len(p)) {
    Xy[, i + 1] <- dat[(p + 1 - i) : (length(dat) - i)]
  }
  Xy
}
```

We compute the one-step-ahead posterior predictive for various AR($p$) models
under the improper prior.

```{r}
data(gdp)
logret <- diff(log(gdp))

means <- scales <- dfs <- rep(NA_real_, 4)
for (p in 1:4) {
  y <- tail(logret, -p)
  X <- ARdesignmatrix(logret, p)
  
  N <- nrow(X)
  d <- ncol(X)
  
  BN <- solve(crossprod(X))
  bN <- tcrossprod(BN, X) %*% y
  SSR <- sum((y - X %*% bN)^2)
  cN <- (N - d) / 2
  CN <- SSR / 2
    
  xf <- c(1, rev(tail(y, p)))
  
  means[p] <- xf %*% bN
  scales[p] <- sqrt((crossprod(xf, BN) %*% xf + 1) * CN / cN)
  dfs[p] <- 2 * cN
}
```

And we visualize.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("9-5_1.pdf", width = 8, height = 4)
  par(mar = c(2.7, 2, 1.5, .5), mgp = c(1.6, .6, 0))
}
par(mfrow = c(1, 1))
library(BayesianLearningCode)
grid <- seq(min(means - 4 * scales), max(means + 4 * scales), length.out = 100)
plot(grid, dstudt(grid, means[1], scales[1], dfs[1]), type = "l",
     ylab = "", xlab = "Quarterly U.S. GDP growth", lwd = 1.5)
title("Forecast for Q1 2020")
legend("topright", paste0("AR(", 1:4, ")"), lty = 1:4, col = 1:4, lwd = 1.5)
for (p in 2:4) {
  lines(grid, dstudt(grid, means[p], scales[p], dfs[p]), col = p, lty = p,
        lwd = 1.5)
}
```

