---
title: "Chapter 8: Bayesian Learning Beyond Standard Regression Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Chapter 8: Bayesian Learning Beyond Standard Regression Analysis}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
knitr::opts_knit$set(global.par = TRUE)
pdfplots <- FALSE # default: FALSE; set this to TRUE only if you like pdf figures
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2.6, 2.6, 2.6, .4), lwd = 1)
```

# Section 8.1
We illustrate probit regression analysis for the labor market data.

```{r}
library("BayesianLearningCode")
data("labor", package = "BayesianLearningCode")
```
We model the binary variable unemployment and use as covariates the variables female (binary), wcollar (binary),  age18 (quantitative, centered at 18 years) and unemployed in 1997 (binary). The baseline person is hence an 18 year old  male blue collar work who was employed in 1997. 

```{r}
y <- labor$unemp98
N <- length(y)  # number of observations 

X <- cbind("intercept" = rep(1, N), "female"=labor$female,"age18"=labor$age-18,
           "wcollar"=labor$wcollar, "unemp97"=labor$unemp97) # regressor matrix
```

The regression coefficients are estimated using data augmentation and Gibbs 
sampling. We define a function yielding posterior draws using the algorithm detailed in Chapter 8.1.1.


```{r}

probit <- function(y, X,  b0 = 0, B0 = 10000,
                       burnin = 1000L, M = 5000L) {
  
  d <- dim(X)[2] # number regression effects 
  p <- d - 1 # number of regression effects without intercept

  B0.inv <- diag(rep(1/B0, d), nrow = d) 
  b0 <- rep(0, d) 
  B0inv.b0 <- B0.inv%*%b0

  betas <- matrix(NA,nrow=M, ncol=d)

  # define quantities for the Gibbs sampler
  XX <- crossprod(X)
  BN <- solve(B0.inv + XX)

  ind0=(y==0) # indicators for zeros and ones
  ind1=(y==1)

  # starting values
  beta <- rep(0,d)
  z <- rep(NA_real_,N)


   for (m in seq_len(burnin+M)) {
    
      # Draw z conditional on y and beta
      u <- runif(N)
      eta <- X %*% beta
      pi<- pnorm(eta) 
   
      z[ind0] <- eta[ind0] + qnorm(u[ind0]*(1-pi[ind0]))
      z[ind1] <- eta[ind1] + qnorm (1-u[ind1]*pi[ind1])
  
      # sample beta from the full conditional 
      bN <- BN %*% (B0inv.b0 + t(X) %*% z)
      beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))
    
     # Store the beta draws
     if (m > burnin) {
        betas[m-burnin, ] <- beta
    }
 }
 return(betas)
}
```

We specify  the  prior on the regression effects as a rather flat independence
Normal prior.

```{r}
betas <- probit(y,X,b0=0, B0=10000)
```

```{r}
res.mcmc <- function(x, lower = 0.025, upper = 0.975)
  c(quantile(x, lower), mean(x), quantile(x, upper))

res_beta<- t(apply(betas,2, res.mcmc))
colnames(res_beta) <- c("2.5%", "Mean", "97.5%") 
rownames(res_beta) <- colnames(X)

knitr::kable(round(t(res_beta),3))

(p_unemploy_base=pnorm(res_beta[1,2]))
```
The estimated risk of unemployment  for a baseline person is low and it is even lower for a white collar worker. It is higher for females, older persons and particularily for those unemployed in 1997.



A plot of the autocorrelation  of the draws shows
that though  they are autocorrelated,  autocorrelation vanishes after a few lags.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_1.pdf", width = 8, height = 5)
}
par(mfrow = c(2, 3), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
for (j in seq_len(ncol(X))) {
  hist(betas[, j], freq = FALSE, main = "",
       xlab = colnames(X)[j], ylab = "")
}
par(mfrow = c(2, 3), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
for (j in seq_len(ncol(X))) {
     acf(betas[, j],main = "", xlab = colnames(X)[j], ylab = "")
}
```

This is different when the response variable  contains either only few or very many successes. To illustrate this issue we use data where in N=1000 trials  only  1 success  is observed  or 999 sucesses are observed.
.

```{r}
N=1000
X <- matrix(1,nrow=N)

y1=c(0, rep(1,N-1))
betas1 <- probit(y1,X,b0=0, B0=10000)

y2=c(rep(0,N-1),1)
betas2 <- probit(y2,X,b0=0, B0=10000) 
```

In both cases the autocorrelation of the draws decrease very slowly.
```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_2.pdf", width = 8, height = 5)
}
par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
plot(betas1,type="l", main = "", xlab = "", ylab = "")
acf(betas1)
plot(betas2,type="l", main = "", xlab = "", ylab = "")
acf(betas2)
```
High autocorrelated draws can also occur in cases of seperation, i.e. when a covariate (or a linear combination of covariates) perfectly  allows to predict 
sucesses and failures. 

To illustrate this issue we again simulate
$N=1000$ observations with 500 successes and 500 failures. 
We add a binary predictor $x$ where for $x=1$ we observe only successes and hence there is complete seperation of successes and failures.

```{r}
N <- 1000
x <-  c(rep(1,10),rep(0,N-10))
y <- c(rep(1,N/2),rep(0,N/2))

table(y,x)
```
We estÃ­mate the  model parameters and plot the acf of the draws. 

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_3.pdf", width = 8, height = 5)
}
X <- matrix(cbind(rep(1,N),x), ncol=2)
betas=probit(y, X,b0=0, B0=10000)

par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)

plot(betas[,1], type="l", main = "", xlab = "", ylab = "")
acf(betas[,1])

plot(betas[,2], type="l", main = "", xlab = "", ylab = "")
acf(betas[,2])
```
While the autocorrelation of the intercept decreases quickly, 
the autocorrelation of the covariate effect remains high. 

If there is seperation in the data  the likelihood is monotone and the  MLE does not exist. In a Bayesian approach  using a  flat,  improper prior on the regression effects will result in an improper posterior distribution. Hence   a proper prior is required to avoid improper posteriors in case of seperation. 
If  more information is added to the data by the prior, the autocorrelations 
are lower. This can be seen in the next figure, where the simulated data are 
analysed using a Normal prior that is tighter around zero.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_4.pdf", width = 8, height = 5)
}
betas=probit(y, X,b0=0, B0=2.5^2)

par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)

plot(betas[,1], type="l", main = "", xlab = "", ylab = "")
acf(betas[,1])

plot(betas[,2], type="l", main = "", xlab = "", ylab = "")
acf(betas[,2])
```

