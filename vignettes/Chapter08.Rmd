---
title: "Chapter 8: Bayesian Learning Beyond Standard Regression Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Chapter 8: Bayesian Learning Beyond Standard Regression Analysis}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 8,
  fig.height = 6
)
knitr::opts_knit$set(global.par = TRUE)
pdfplots <- FALSE # default: FALSE; set this to TRUE only if you like pdf figures
```

```{r, include = FALSE}
par(mgp = c(1.6, .6, 0), mar = c(2.6, 2.6, 2.6, .4), lwd = 1)
```

# Section 8.1.1: Probit Model 
## Example 8.1: Labour Market Data
We illustrate probit regression analysis for the labor market data.

```{r}
library("BayesianLearningCode")
data("labor", package = "BayesianLearningCode")
```
We model the binary variable unemployment and use as covariates the variables female (binary), wcollar (binary),  age18 (quantitative, centered at 18 years) and unemployed in 1997 (binary). The baseline person is hence an 18 year old  male blue collar worker who was employed in 1997. 

```{r}
y.unemp <- labor$unemp98
N.unemp <- length(y.unemp)  # number of observations 

X.unemp <- cbind("intercept" = rep(1, N.unemp), "female"=labor$female,"age18"=labor$age-18,
           "wcollar"=labor$wcollar, "unemp97"=labor$unemp97) # regressor matrix
```

The regression coefficients are estimated using data augmentation and Gibbs 
sampling. We define a function yielding posterior draws using the algorithm detailed in Chapter 8.1.1.


```{r}
probit <- function(y, X,  b0 = 0, B0 = 10000,
                   burnin = 1000L, M = 5000L) {
  N <-length(y)
  d <- dim(X)[2] # number regression effects 
  p <- d - 1 # number of regression effects without intercept

  B0.inv <- diag(rep(1/B0, d), nrow = d) 
  b0 <- rep(0, d) 
  B0inv.b0 <- B0.inv%*%b0

  betas <- matrix(NA,nrow=M, ncol=d)

  # define quantities for the Gibbs sampler
  XX <- crossprod(X)
  BN <- solve(B0.inv + XX)

  ind0=(y==0) # indicators for zeros and ones
  ind1=(y==1)

  # starting values
  beta <- rep(0,d)
  z <- rep(NA_real_,N)


   for (m in seq_len(burnin+M)) {
    
      # Draw z conditional on y and beta
      u <- runif(N)
      eta <- X %*% beta
      pi<- pnorm(eta) 
   
      z[ind0] <- eta[ind0] + qnorm(u[ind0]*(1-pi[ind0]))
      z[ind1] <- eta[ind1] + qnorm (1-u[ind1]*pi[ind1])
  
      # sample beta from the full conditional 
      bN <- BN %*% (B0inv.b0 + t(X) %*% z)
      beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))
    
     # Store the beta draws
     if (m > burnin) {
        betas[m-burnin, ] <- beta
    }
 }
 return(betas)
}
```

We specify  the  prior on the regression effects as a rather flat independence
Normal prior.

```{r}
betas <- probit(y.unemp,X.unemp,b0=0, B0=10000)
```

```{r}
res.mcmc <- function(x, lower = 0.025, upper = 0.975)
  c(quantile(x, lower), mean(x), quantile(x, upper))

res_beta<- t(apply(betas,2, res.mcmc))
colnames(res_beta) <- c("2.5%", "Mean", "97.5%") 
rownames(res_beta) <- colnames(X.unemp)

knitr::kable(round(t(res_beta),3))

(p_unemploy_base=pnorm(res_beta[1,2]))
```
The estimated risk of unemployment  for a baseline person is low and it is even lower for a white collar worker. It is higher for females, older persons and particularily for those unemployed in 1997.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_1.pdf", width = 8, height = 5)
}

par(mfrow = c(2, 3), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)

for (j in seq_len(ncol(X.unemp))) {
  hist(betas[, j], freq = FALSE, main = "",
       xlab = colnames(X.unemp)[j], ylab = "")
}
```

A plot of the autocorrelation of the draws shows that although there is some autocorrelation, it vanishes after a few lags.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_2.pdf", width = 8, height = 5)
}

par(mfrow = c(2, 3), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)

for (j in seq_len(ncol(X.unemp))) {
     acf(betas[, j],main = "", xlab = colnames(X.unemp)[j], ylab = "")
}
```
The sampler is easy to implement, however there  might be problems  when the response variable  contains either only few or very many successes.
To illustrate this issue, we use data where in N=500 trials  only  1 success  or only 1 failure is observed.

```{r}
N=500
X <- matrix(1,nrow=N)

y1=c(0, rep(1,N-1))
betas1 <- probit(y1,X,b0=0, B0=10000)

y2=c(rep(0,N-1),1)
betas2 <- probit(y2,X,b0=0, B0=10000) 
```

In both cases the autocorrelation of the draws decrease very slowly.
```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_3.pdf", width = 8, height = 5)
}
par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)
plot(betas1,type="l", main = "", xlab = "", ylab = "")
acf(betas1)
plot(betas2,type="l", main = "", xlab = "", ylab = "")
acf(betas2)
```
High autocorrelated draws in probit models not only occur  if successes or failures are very rare, but also when a covariate (or a linear combination of covariates) perfectly  allows to predict successes and/or failures.
Complete seperation means that both successes and failures can be perfectly predicted by a covariate, whereas with quasi-complete seperation only either successes or failures can be predicted perfectly.

To illustrate posterior sampling, in the case of complete seperation, we  simulate
$N=500$ observations with 10 successes and 490 failures. 
We add a binary predictor $x$ where for $x=1$ we observe only successes and for $x=0$ only failures.


```{r}
N <- 500
ns <- 10
x<-  c(rep(1,ns),rep(0,N-ns))
y <-  c(rep(1,ns),rep(0,N-ns))

table(y,x)
```
We estÃ­mate the  model parameters and plot the acf of the draws. Again the autocorrelations remain high even for lag 35.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_4.pdf", width = 8, height = 5)
}
X <- matrix(cbind(rep(1,N),x), ncol=2)
betas=probit(y, X,b0=0, B0=10000)

par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)

plot(betas[,1], type="l", main = "", xlab = "", ylab = "")
acf(betas[,1])

plot(betas[,2], type="l", main = "", xlab = "", ylab = "")
acf(betas[,2])
```
Finally we  simulate another data set of  $N=500$ observations with 10 successes and 490 failures. In this data  for $x=1$  only successes but  for $x=0$ successes as well as failures are observed and hence there is quasi-seperation.

```{r}
N <- 500
x <-  c(rep(1,ns),rep(0,N-ns))
y <- c(rep(1,N/2),rep(0,N/2))

table(y,x)
```
A plot of the acf of the draws shows low autocorrelation for the intercept but autocorrelations for the covariate effect are again high.
```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_5.pdf", width = 8, height = 5)
}
X <- matrix(cbind(rep(1,N),x), ncol=2)
betas=probit(y, X,b0=0, B0=10000)

par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)

plot(betas[,1], type="l", main = "", xlab = "", ylab = "")
acf(betas[,1])

plot(betas[,2], type="l", main = "", xlab = "", ylab = "")
acf(betas[,2])
```

High autocorrelations typically indicate problems with the sampler. If there is complete or quasi-complete seperation in the data  the likelihood is monotone and the  MLE does not exist. In a Bayesian approach  using a  flat,  improper prior on the regression effects will result in an improper posterior distribution. Hence   a proper prior is required to avoid improper posteriors in case of seperation. 
In the  examples above we used a rather  proper prior which is rather flat. With a  more  informative prior, the autocorrelations of the draw  are lower.  
This can be seen in the next figure, where the simulated data under quasi-seperation are re-analysed with a Normal prior that is tighter around zero.

```{r, echo = -c(1:2)}
if (pdfplots) {
  pdf("8-1_6.pdf", width = 8, height = 5)
}
betas=probit(y, X,b0=0, B0=2.5^2)

par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)

plot(betas[,1], type="l", main = "", xlab = "", ylab = "")
acf(betas[,1])

plot(betas[,2], type="l", main = "", xlab = "", ylab = "")
acf(betas[,2])
```

# Section 8.1.2
We now estimate a logistic regression model for the labor market data
using the two-block Polya-Gamma sampler.

```{r}
install.packages("pgdraw",repos = "http://cran.us.r-project.org")
```

```{r}
logit<- function(y, X,  b0 = 0, B0 = 10000,
                   burnin = 1000L, M = 5000L) {
  
  N <-length(y)
  d <- dim(X)[2] # number regression effects 
  p <- d - 1 # number of regression effects without intercept

  B0.inv <- diag(rep(1/B0, d), nrow = d) 
  b0 <- rep(0, d) 
  B0inv.b0 <- B0.inv%*%b0

  betas <- matrix(NA,nrow=M, ncol=d)

  # define quantities for the Gibbs sampler
  ind0=(y==0) # indicators for zeros and ones
  ind1=(y==1)
  
  # starting values
  beta <- rep(0,d)
  z <- rep(NA_real_,N)
  omega <-rep(NA_real_,N)


   for (m in seq_len(burnin+M)) {
    
      # Draw z conditional on y and beta
      eta <- X %*% beta
      pi<- plogis(eta) 
      
      u <- runif(N)
      z[ind0] <- eta[ind0] + qlogis(u[ind0]*(1-pi[ind0]))
      z[ind1] <- eta[ind1] + qlogis (1-u[ind1]*pi[ind1])
      
      
      # Draw omega conditional on y, beta and z
      omega <- pgdraw::pgdraw(b = 1, c = z-eta)
  
      # sample beta from the full conditional 
      Xomega <- matrix(omega,ncol=d, nrow=N) *X
      BN <- solve(B0.inv + t(Xomega)%*%X)
      bN <- BN %*% (B0inv.b0 + t(Xomega)%*% z)
      beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))
    
     # Store the beta draws
     if (m > burnin) {
        betas[m-burnin, ] <- beta
    }
 }
 return(betas)
}
```
We again use   the flat independence
Normal prior on the regression effects and estimate the model.

```{r}
betas_logit <- logit(y.unemp,X.unemp,b0=0, B0=10000)
print(str(betas_logit))

res_beta_logit<- t(apply(betas_logit,2, res.mcmc))
colnames(res_beta_logit) <- c("2.5%", "Mean", "97.5%") 
rownames(res_beta_logit) <- colnames(X.unemp)
 
knitr::kable(round(t(res_beta_logit),3))

(p_unemploy_base=plogis(res_beta_logit[1,2]))
```

Note that  the logistic distribution  has a variance of $\pi^2/3$ and hence the 
regression effects are absolutely larger than in the probit model. However any
probability computed from the two models  will be very close, compare the  
probability to be unemployed for a baseline person.

We can compare the posterior estimates of the  coefficients in the probit model 
$\beta$ those  of of logit model by multiplying them with $\pi/\sqrt{3}$ and see
that there is not much difference.

```{r}
knitr::kable(round(t(res_beta*pi/sqrt(3)),3))
```

# Section 8.2 
## Example 8.3: Road Safety Data

#small model with intercept, intervention effect and holiday dummy (activated in
#July/August)
# large model with intercept, intervention effect, linear trend, seasonal pattern 
#with monthly dummies in
# Study how the acceptance rate detoriates, if d increases. ADD
We load the data and extract the observations for the children in
Linz. Then we define the regressor matrix.

```{r}
data("accidents", package = "BayesianLearningCode")
y <- accidents[, "children_accidents"]
N <- length(y)

intervention <-  c(rep(0,7*12+9),rep(1,8*12+3))
holiday <- rep(c(rep(0,6), rep(1,2), rep(0,4)),16)
X <- cbind(rep(1,N),intervention, holiday)
```
