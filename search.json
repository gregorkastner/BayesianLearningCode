[{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"GNU General Public License","title":"GNU General Public License","text":"Version 3, 29 June 2007Copyright ¬© 2007 Free Software Foundation, Inc.¬†<http://fsf.org/> Everyone permitted copy distribute verbatim copies license document, changing allowed.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"preamble","dir":"","previous_headings":"","what":"Preamble","title":"GNU General Public License","text":"GNU General Public License free, copyleft license software kinds works. licenses software practical works designed take away freedom share change works. contrast, GNU General Public License intended guarantee freedom share change versions program‚Äìmake sure remains free software users. , Free Software Foundation, use GNU General Public License software; applies also work released way authors. can apply programs, . speak free software, referring freedom, price. General Public Licenses designed make sure freedom distribute copies free software (charge wish), receive source code can get want , can change software use pieces new free programs, know can things. protect rights, need prevent others denying rights asking surrender rights. Therefore, certain responsibilities distribute copies software, modify : responsibilities respect freedom others. example, distribute copies program, whether gratis fee, must pass recipients freedoms received. must make sure , , receive can get source code. must show terms know rights. Developers use GNU GPL protect rights two steps: (1) assert copyright software, (2) offer License giving legal permission copy, distribute /modify . developers‚Äô authors‚Äô protection, GPL clearly explains warranty free software. users‚Äô authors‚Äô sake, GPL requires modified versions marked changed, problems attributed erroneously authors previous versions. devices designed deny users access install run modified versions software inside , although manufacturer can . fundamentally incompatible aim protecting users‚Äô freedom change software. systematic pattern abuse occurs area products individuals use, precisely unacceptable. Therefore, designed version GPL prohibit practice products. problems arise substantially domains, stand ready extend provision domains future versions GPL, needed protect freedom users. Finally, every program threatened constantly software patents. States allow patents restrict development use software general-purpose computers, , wish avoid special danger patents applied free program make effectively proprietary. prevent , GPL assures patents used render program non-free. precise terms conditions copying, distribution modification follow.","code":""},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_0-definitions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"0. Definitions","title":"GNU General Public License","text":"‚ÄúLicense‚Äù refers version 3 GNU General Public License. ‚ÄúCopyright‚Äù also means copyright-like laws apply kinds works, semiconductor masks. ‚ÄúProgram‚Äù refers copyrightable work licensed License. licensee addressed ‚Äú‚Äù. ‚ÄúLicensees‚Äù ‚Äúrecipients‚Äù may individuals organizations. ‚Äúmodify‚Äù work means copy adapt part work fashion requiring copyright permission, making exact copy. resulting work called ‚Äúmodified version‚Äù earlier work work ‚Äúbased ‚Äù earlier work. ‚Äúcovered work‚Äù means either unmodified Program work based Program. ‚Äúpropagate‚Äù work means anything , without permission, make directly secondarily liable infringement applicable copyright law, except executing computer modifying private copy. Propagation includes copying, distribution (without modification), making available public, countries activities well. ‚Äúconvey‚Äù work means kind propagation enables parties make receive copies. Mere interaction user computer network, transfer copy, conveying. interactive user interface displays ‚ÄúAppropriate Legal Notices‚Äù extent includes convenient prominently visible feature (1) displays appropriate copyright notice, (2) tells user warranty work (except extent warranties provided), licensees may convey work License, view copy License. interface presents list user commands options, menu, prominent item list meets criterion.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_1-source-code","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"1. Source Code","title":"GNU General Public License","text":"‚Äúsource code‚Äù work means preferred form work making modifications . ‚ÄúObject code‚Äù means non-source form work. ‚ÄúStandard Interface‚Äù means interface either official standard defined recognized standards body, , case interfaces specified particular programming language, one widely used among developers working language. ‚ÄúSystem Libraries‚Äù executable work include anything, work whole, () included normal form packaging Major Component, part Major Component, (b) serves enable use work Major Component, implement Standard Interface implementation available public source code form. ‚ÄúMajor Component‚Äù, context, means major essential component (kernel, window system, ) specific operating system () executable work runs, compiler used produce work, object code interpreter used run . ‚ÄúCorresponding Source‚Äù work object code form means source code needed generate, install, (executable work) run object code modify work, including scripts control activities. However, include work‚Äôs System Libraries, general-purpose tools generally available free programs used unmodified performing activities part work. example, Corresponding Source includes interface definition files associated source files work, source code shared libraries dynamically linked subprograms work specifically designed require, intimate data communication control flow subprograms parts work. Corresponding Source need include anything users can regenerate automatically parts Corresponding Source. Corresponding Source work source code form work.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_2-basic-permissions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"2. Basic Permissions","title":"GNU General Public License","text":"rights granted License granted term copyright Program, irrevocable provided stated conditions met. License explicitly affirms unlimited permission run unmodified Program. output running covered work covered License output, given content, constitutes covered work. License acknowledges rights fair use equivalent, provided copyright law. may make, run propagate covered works convey, without conditions long license otherwise remains force. may convey covered works others sole purpose make modifications exclusively , provide facilities running works, provided comply terms License conveying material control copyright. thus making running covered works must exclusively behalf, direction control, terms prohibit making copies copyrighted material outside relationship . Conveying circumstances permitted solely conditions stated . Sublicensing allowed; section 10 makes unnecessary.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_3-protecting-users-legal-rights-from-anti-circumvention-law","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"3. Protecting Users‚Äô Legal Rights From Anti-Circumvention Law","title":"GNU General Public License","text":"covered work shall deemed part effective technological measure applicable law fulfilling obligations article 11 WIPO copyright treaty adopted 20 December 1996, similar laws prohibiting restricting circumvention measures. convey covered work, waive legal power forbid circumvention technological measures extent circumvention effected exercising rights License respect covered work, disclaim intention limit operation modification work means enforcing, work‚Äôs users, third parties‚Äô legal rights forbid circumvention technological measures.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_4-conveying-verbatim-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"4. Conveying Verbatim Copies","title":"GNU General Public License","text":"may convey verbatim copies Program‚Äôs source code receive , medium, provided conspicuously appropriately publish copy appropriate copyright notice; keep intact notices stating License non-permissive terms added accord section 7 apply code; keep intact notices absence warranty; give recipients copy License along Program. may charge price price copy convey, may offer support warranty protection fee.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_5-conveying-modified-source-versions","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"5. Conveying Modified Source Versions","title":"GNU General Public License","text":"may convey work based Program, modifications produce Program, form source code terms section 4, provided also meet conditions: ) work must carry prominent notices stating modified , giving relevant date. b) work must carry prominent notices stating released License conditions added section 7. requirement modifies requirement section 4 ‚Äúkeep intact notices‚Äù. c) must license entire work, whole, License anyone comes possession copy. License therefore apply, along applicable section 7 additional terms, whole work, parts, regardless packaged. License gives permission license work way, invalidate permission separately received . d) work interactive user interfaces, must display Appropriate Legal Notices; however, Program interactive interfaces display Appropriate Legal Notices, work need make . compilation covered work separate independent works, nature extensions covered work, combined form larger program, volume storage distribution medium, called ‚Äúaggregate‚Äù compilation resulting copyright used limit access legal rights compilation‚Äôs users beyond individual works permit. Inclusion covered work aggregate cause License apply parts aggregate.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_6-conveying-non-source-forms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"6. Conveying Non-Source Forms","title":"GNU General Public License","text":"may convey covered work object code form terms sections 4 5, provided also convey machine-readable Corresponding Source terms License, one ways: ) Convey object code , embodied , physical product (including physical distribution medium), accompanied Corresponding Source fixed durable physical medium customarily used software interchange. b) Convey object code , embodied , physical product (including physical distribution medium), accompanied written offer, valid least three years valid long offer spare parts customer support product model, give anyone possesses object code either (1) copy Corresponding Source software product covered License, durable physical medium customarily used software interchange, price reasonable cost physically performing conveying source, (2) access copy Corresponding Source network server charge. c) Convey individual copies object code copy written offer provide Corresponding Source. alternative allowed occasionally noncommercially, received object code offer, accord subsection 6b. d) Convey object code offering access designated place (gratis charge), offer equivalent access Corresponding Source way place charge. need require recipients copy Corresponding Source along object code. place copy object code network server, Corresponding Source may different server (operated third party) supports equivalent copying facilities, provided maintain clear directions next object code saying find Corresponding Source. Regardless server hosts Corresponding Source, remain obligated ensure available long needed satisfy requirements. e) Convey object code using peer--peer transmission, provided inform peers object code Corresponding Source work offered general public charge subsection 6d. separable portion object code, whose source code excluded Corresponding Source System Library, need included conveying object code work. ‚ÄúUser Product‚Äù either (1) ‚Äúconsumer product‚Äù, means tangible personal property normally used personal, family, household purposes, (2) anything designed sold incorporation dwelling. determining whether product consumer product, doubtful cases shall resolved favor coverage. particular product received particular user, ‚Äúnormally used‚Äù refers typical common use class product, regardless status particular user way particular user actually uses, expects expected use, product. product consumer product regardless whether product substantial commercial, industrial non-consumer uses, unless uses represent significant mode use product. ‚ÄúInstallation Information‚Äù User Product means methods, procedures, authorization keys, information required install execute modified versions covered work User Product modified version Corresponding Source. information must suffice ensure continued functioning modified object code case prevented interfered solely modification made. convey object code work section , , specifically use , User Product, conveying occurs part transaction right possession use User Product transferred recipient perpetuity fixed term (regardless transaction characterized), Corresponding Source conveyed section must accompanied Installation Information. requirement apply neither third party retains ability install modified object code User Product (example, work installed ROM). requirement provide Installation Information include requirement continue provide support service, warranty, updates work modified installed recipient, User Product modified installed. Access network may denied modification materially adversely affects operation network violates rules protocols communication across network. Corresponding Source conveyed, Installation Information provided, accord section must format publicly documented (implementation available public source code form), must require special password key unpacking, reading copying.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_7-additional-terms","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"7. Additional Terms","title":"GNU General Public License","text":"‚ÄúAdditional permissions‚Äù terms supplement terms License making exceptions one conditions. Additional permissions applicable entire Program shall treated though included License, extent valid applicable law. additional permissions apply part Program, part may used separately permissions, entire Program remains governed License without regard additional permissions. convey copy covered work, may option remove additional permissions copy, part . (Additional permissions may written require removal certain cases modify work.) may place additional permissions material, added covered work, can give appropriate copyright permission. Notwithstanding provision License, material add covered work, may (authorized copyright holders material) supplement terms License terms: ) Disclaiming warranty limiting liability differently terms sections 15 16 License; b) Requiring preservation specified reasonable legal notices author attributions material Appropriate Legal Notices displayed works containing ; c) Prohibiting misrepresentation origin material, requiring modified versions material marked reasonable ways different original version; d) Limiting use publicity purposes names licensors authors material; e) Declining grant rights trademark law use trade names, trademarks, service marks; f) Requiring indemnification licensors authors material anyone conveys material (modified versions ) contractual assumptions liability recipient, liability contractual assumptions directly impose licensors authors. non-permissive additional terms considered ‚Äúrestrictions‚Äù within meaning section 10. Program received , part , contains notice stating governed License along term restriction, may remove term. license document contains restriction permits relicensing conveying License, may add covered work material governed terms license document, provided restriction survive relicensing conveying. add terms covered work accord section, must place, relevant source files, statement additional terms apply files, notice indicating find applicable terms. Additional terms, permissive non-permissive, may stated form separately written license, stated exceptions; requirements apply either way.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_8-termination","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"8. Termination","title":"GNU General Public License","text":"may propagate modify covered work except expressly provided License. attempt otherwise propagate modify void, automatically terminate rights License (including patent licenses granted third paragraph section 11). However, cease violation License, license particular copyright holder reinstated () provisionally, unless copyright holder explicitly finally terminates license, (b) permanently, copyright holder fails notify violation reasonable means prior 60 days cessation. Moreover, license particular copyright holder reinstated permanently copyright holder notifies violation reasonable means, first time received notice violation License (work) copyright holder, cure violation prior 30 days receipt notice. Termination rights section terminate licenses parties received copies rights License. rights terminated permanently reinstated, qualify receive new licenses material section 10.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_9-acceptance-not-required-for-having-copies","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"9. Acceptance Not Required for Having Copies","title":"GNU General Public License","text":"required accept License order receive run copy Program. Ancillary propagation covered work occurring solely consequence using peer--peer transmission receive copy likewise require acceptance. However, nothing License grants permission propagate modify covered work. actions infringe copyright accept License. Therefore, modifying propagating covered work, indicate acceptance License .","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_10-automatic-licensing-of-downstream-recipients","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"10. Automatic Licensing of Downstream Recipients","title":"GNU General Public License","text":"time convey covered work, recipient automatically receives license original licensors, run, modify propagate work, subject License. responsible enforcing compliance third parties License. ‚Äúentity transaction‚Äù transaction transferring control organization, substantially assets one, subdividing organization, merging organizations. propagation covered work results entity transaction, party transaction receives copy work also receives whatever licenses work party‚Äôs predecessor interest give previous paragraph, plus right possession Corresponding Source work predecessor interest, predecessor can get reasonable efforts. may impose restrictions exercise rights granted affirmed License. example, may impose license fee, royalty, charge exercise rights granted License, may initiate litigation (including cross-claim counterclaim lawsuit) alleging patent claim infringed making, using, selling, offering sale, importing Program portion .","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_11-patents","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"11. Patents","title":"GNU General Public License","text":"‚Äúcontributor‚Äù copyright holder authorizes use License Program work Program based. work thus licensed called contributor‚Äôs ‚Äúcontributor version‚Äù. contributor‚Äôs ‚Äúessential patent claims‚Äù patent claims owned controlled contributor, whether already acquired hereafter acquired, infringed manner, permitted License, making, using, selling contributor version, include claims infringed consequence modification contributor version. purposes definition, ‚Äúcontrol‚Äù includes right grant patent sublicenses manner consistent requirements License. contributor grants non-exclusive, worldwide, royalty-free patent license contributor‚Äôs essential patent claims, make, use, sell, offer sale, import otherwise run, modify propagate contents contributor version. following three paragraphs, ‚Äúpatent license‚Äù express agreement commitment, however denominated, enforce patent (express permission practice patent covenant sue patent infringement). ‚Äúgrant‚Äù patent license party means make agreement commitment enforce patent party. convey covered work, knowingly relying patent license, Corresponding Source work available anyone copy, free charge terms License, publicly available network server readily accessible means, must either (1) cause Corresponding Source available, (2) arrange deprive benefit patent license particular work, (3) arrange, manner consistent requirements License, extend patent license downstream recipients. ‚ÄúKnowingly relying‚Äù means actual knowledge , patent license, conveying covered work country, recipient‚Äôs use covered work country, infringe one identifiable patents country reason believe valid. , pursuant connection single transaction arrangement, convey, propagate procuring conveyance , covered work, grant patent license parties receiving covered work authorizing use, propagate, modify convey specific copy covered work, patent license grant automatically extended recipients covered work works based . patent license ‚Äúdiscriminatory‚Äù include within scope coverage, prohibits exercise , conditioned non-exercise one rights specifically granted License. may convey covered work party arrangement third party business distributing software, make payment third party based extent activity conveying work, third party grants, parties receive covered work , discriminatory patent license () connection copies covered work conveyed (copies made copies), (b) primarily connection specific products compilations contain covered work, unless entered arrangement, patent license granted, prior 28 March 2007. Nothing License shall construed excluding limiting implied license defenses infringement may otherwise available applicable patent law.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_12-no-surrender-of-others-freedom","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"12. No Surrender of Others‚Äô Freedom","title":"GNU General Public License","text":"conditions imposed (whether court order, agreement otherwise) contradict conditions License, excuse conditions License. convey covered work satisfy simultaneously obligations License pertinent obligations, consequence may convey . example, agree terms obligate collect royalty conveying convey Program, way satisfy terms License refrain entirely conveying Program.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_13-use-with-the-gnu-affero-general-public-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"13. Use with the GNU Affero General Public License","title":"GNU General Public License","text":"Notwithstanding provision License, permission link combine covered work work licensed version 3 GNU Affero General Public License single combined work, convey resulting work. terms License continue apply part covered work, special requirements GNU Affero General Public License, section 13, concerning interaction network apply combination .","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_14-revised-versions-of-this-license","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"14. Revised Versions of this License","title":"GNU General Public License","text":"Free Software Foundation may publish revised /new versions GNU General Public License time time. new versions similar spirit present version, may differ detail address new problems concerns. version given distinguishing version number. Program specifies certain numbered version GNU General Public License ‚Äúlater version‚Äù applies , option following terms conditions either numbered version later version published Free Software Foundation. Program specify version number GNU General Public License, may choose version ever published Free Software Foundation. Program specifies proxy can decide future versions GNU General Public License can used, proxy‚Äôs public statement acceptance version permanently authorizes choose version Program. Later license versions may give additional different permissions. However, additional obligations imposed author copyright holder result choosing follow later version.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_15-disclaimer-of-warranty","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"15. Disclaimer of Warranty","title":"GNU General Public License","text":"WARRANTY PROGRAM, EXTENT PERMITTED APPLICABLE LAW. EXCEPT OTHERWISE STATED WRITING COPYRIGHT HOLDERS /PARTIES PROVIDE PROGRAM ‚Äú‚Äù WITHOUT WARRANTY KIND, EITHER EXPRESSED IMPLIED, INCLUDING, LIMITED , IMPLIED WARRANTIES MERCHANTABILITY FITNESS PARTICULAR PURPOSE. ENTIRE RISK QUALITY PERFORMANCE PROGRAM . PROGRAM PROVE DEFECTIVE, ASSUME COST NECESSARY SERVICING, REPAIR CORRECTION.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_16-limitation-of-liability","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"16. Limitation of Liability","title":"GNU General Public License","text":"EVENT UNLESS REQUIRED APPLICABLE LAW AGREED WRITING COPYRIGHT HOLDER, PARTY MODIFIES /CONVEYS PROGRAM PERMITTED , LIABLE DAMAGES, INCLUDING GENERAL, SPECIAL, INCIDENTAL CONSEQUENTIAL DAMAGES ARISING USE INABILITY USE PROGRAM (INCLUDING LIMITED LOSS DATA DATA RENDERED INACCURATE LOSSES SUSTAINED THIRD PARTIES FAILURE PROGRAM OPERATE PROGRAMS), EVEN HOLDER PARTY ADVISED POSSIBILITY DAMAGES.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"id_17-interpretation-of-sections-15-and-16","dir":"","previous_headings":"TERMS AND CONDITIONS","what":"17. Interpretation of Sections 15 and 16","title":"GNU General Public License","text":"disclaimer warranty limitation liability provided given local legal effect according terms, reviewing courts shall apply local law closely approximates absolute waiver civil liability connection Program, unless warranty assumption liability accompanies copy Program return fee. END TERMS CONDITIONS","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/LICENSE.html","id":"how-to-apply-these-terms-to-your-new-programs","dir":"","previous_headings":"","what":"How to Apply These Terms to Your New Programs","title":"GNU General Public License","text":"develop new program, want greatest possible use public, best way achieve make free software everyone can redistribute change terms. , attach following notices program. safest attach start source file effectively state exclusion warranty; file least ‚Äúcopyright‚Äù line pointer full notice found. Also add information contact electronic paper mail. program terminal interaction, make output short notice like starts interactive mode: hypothetical commands show w show c show appropriate parts General Public License. course, program‚Äôs commands might different; GUI interface, use ‚Äúbox‚Äù. also get employer (work programmer) school, , sign ‚Äúcopyright disclaimer‚Äù program, necessary. information , apply follow GNU GPL, see <http://www.gnu.org/licenses/>. GNU General Public License permit incorporating program proprietary programs. program subroutine library, may consider useful permit linking proprietary applications library. want , use GNU Lesser General Public License instead License. first, please read <http://www.gnu.org/philosophy/--lgpl.html>.","code":"<one line to give the program's name and a brief idea of what it does.> Copyright (C) <year>  <name of author>  This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.  This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more details.  You should have received a copy of the GNU General Public License along with this program.  If not, see <http://www.gnu.org/licenses/>. <program>  Copyright (C) <year>  <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type 'show w'. This is free software, and you are welcome to redistribute it under certain conditions; type 'show c' for details."},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter01.html","id":"example-1-3-and-1-4-classifying-categorical-observations-into-two-states","dir":"Articles","previous_headings":"Section 1.3: Naive Bayes classifiers","what":"Example 1.3 and 1.4: Classifying categorical observations into two states","title":"Chapter 1: From Bayes‚Äô Rule to Bayes‚Äô Theorem","text":"two firms, reliable firm AA Pr()=0.7Pr() = 0.7 less reliable firm ACA^C Pr(AC)=1‚àí0.7=0.3Pr(^C) = 1 - 0.7 = 0.3. failure rate, .e., probability faulty item, 0.01 reliable firm AA 0.05 less reliable firm ACA^C. Assume observe number failures yy 0 6 (N=100N = 100 observations total), don‚Äôt know whether items company AA ACA^C. can now compute probability dealing items company AA using Bayes‚Äô theorem. Note , course, probabilities dealing items company AA ACA^C complementary (.e., sum one). Assuming prior information probability occurrence company AA, simply set Pr()=Pr(AC)=0.5Pr() = Pr(^C) = 0.5 recompute posterior probabilities.","code":"PrA <- 0.7 PrAC <- 1 - PrA PrFA <- 0.01 PrFAC <- 0.05 N <- 100 y <- 0:6 theta1_y_unnormalized <- PrFA ^ y * (1 - PrFA) ^ (N - y) * PrA theta0_y_unnormalized <- PrFAC ^ y * (1 - PrFAC) ^ (N - y) * PrAC normalizer <- theta1_y_unnormalized + theta0_y_unnormalized  res <- rbind(\"reliable company\" = theta1_y_unnormalized / normalizer,              \"less reliable company\" = theta0_y_unnormalized / normalizer) colnames(res) <- y knitr::kable(round(res, 3)) PrA <- 0.5 PrAC <- 1 - PrA  theta1_y_unnormalized <- PrFA ^ y * (1 - PrFA) ^ (N - y) * PrA theta0_y_unnormalized <- PrFAC ^ y * (1 - PrFAC) ^ (N - y) * PrAC normalizer <- theta1_y_unnormalized + theta0_y_unnormalized  res <- rbind(theta1_y_unnormalized/ normalizer,              theta0_y_unnormalized/ normalizer) colnames(res) <- y rownames(res) <- c(\"reliable company\", \"less reliable company\") knitr::kable(round(res, 3))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter01.html","id":"example-1-5-classifying-continuous-observations-into-two-states","dir":"Articles","previous_headings":"Section 1.3: Naive Bayes classifiers","what":"Example 1.5: Classifying continuous observations into two states","title":"Chapter 1: From Bayes‚Äô Rule to Bayes‚Äô Theorem","text":"Bernoulli random variable œë\\vartheta prior probability Pr(œë)=0.55,Pr(\\vartheta) = 0.55, know females y|œë1‚àºExp(Œª),y|\\vartheta_1 \\sim Exp(\\lambda), males y|œë0‚àºG(Œ±,Œ≤),y|\\vartheta_0 \\sim G(\\alpha, \\beta), Œª=1/2.2\\lambda = 1/2.2, Œ±=5\\alpha = 5, Œ≤=5/5.5\\beta = 5/5.5. now plot conditional densities y‚àà[0,10]y \\[0,10]. , compute non-normalized posteriors y‚àà{1,2,3,6,8}y \\\\{1, 2, 3, 6, 8\\}, mark plot, add lines visualize non-normalized posteriors.  Finally, posterior probabilities. Note ratio pairs corresponding posterior probabilities ratio heights vertically corresponding points graph. also plot posterior probabilities distance values yy ranging 0 10 steps 0.2 stacked barplot.","code":"Prtheta = 0.55 lambda = 1/2.2 alpha = 5 beta = 5/5.5 y_all <- seq(0, 10, 0.2) plot(y_all, dexp(y_all, lambda), type = \"l\", xlab = \"Distance\", ylab = \"\",       main = \"Conditional densities and non-normalized posteriors\") lines(y_all, dgamma(y_all, alpha, beta), col = \"red\")  y <- c(1, 2, 3, 6, 8) theta1_y_unnormalized <- dexp(y, lambda) * Prtheta theta0_y_unnormalized <- dgamma(y, alpha, beta) * (1 - Prtheta)  points(y, theta1_y_unnormalized) points(y, theta0_y_unnormalized, col = \"red\", pch = 2)  theta1_y_all_unnormalized <- dexp(y_all, lambda) * Prtheta theta0_y_all_unnormalized <- dgamma(y_all, alpha, beta) * (1 - Prtheta)  lines(y_all, theta1_y_all_unnormalized, lty = 2) lines(y_all, theta0_y_all_unnormalized, lty = 2, col = \"red\")  legend(\"topright\", c(\"Conditional density (female)\",                      \"Conditional density (male)\",                      \"Non-normalized posterior (female)\",                      \"Non-normalized posterior (male)\"),        col = rep(c(\"black\", \"red\"), 2), lty = rep(1:2, each = 2),        pch = c(NA, NA, 1, 2)) normalizer <- theta1_y_unnormalized + theta0_y_unnormalized theta1_y <- theta1_y_unnormalized / normalizer theta0_y <- theta0_y_unnormalized / normalizer res <- rbind(female = theta1_y, male = theta0_y) colnames(res) <- y knitr::kable(round(res, 3)) normalizer <- theta1_y_all_unnormalized + theta0_y_all_unnormalized theta1_y_all <- theta1_y_all_unnormalized / normalizer theta0_y_all <- theta0_y_all_unnormalized / normalizer res <- rbind(female = theta1_y_all, male = theta0_y_all) colnames(res) <- y_all barplot(res, main = \"Posterior probabilities\", xlab = \"Distance\",         ylab = \"Probabilities\", col = c(\"black\", \"red\"))"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter02.html","id":"example-2-2-road-safety-data","dir":"Articles","previous_headings":"Section 2.1","what":"Example 2.2: Road Safety Data","title":"Chapter 2: A First Bayesian Analysis of Count Data","text":"Let us first take look data.","code":"data(\"accidents\", package = \"BayesianLearningCode\") summary(accidents) #>  children_accidents children_exposure seniors_accidents seniors_exposure #>  Min.   :0.000      Min.   :8171      Min.   : 0.00     Min.   :42854    #>  1st Qu.:1.000      1st Qu.:8678      1st Qu.: 3.00     1st Qu.:43574    #>  Median :2.000      Median :8900      Median : 5.00     Median :44097    #>  Mean   :1.839      Mean   :8856      Mean   : 5.25     Mean   :43890    #>  3rd Qu.:3.000      3rd Qu.:9103      3rd Qu.: 7.00     3rd Qu.:44264    #>  Max.   :5.000      Max.   :9204      Max.   :15.00     Max.   :44671 plot(accidents[, c(\"children_accidents\", \"children_exposure\")],      mar = c(0, 0, 0, 0), oma = c(3, 3, 1.5, .1),      main = \"\", xlab = \"\", ylab = \"\", ann = FALSE) title(\"Road accidents involving pedestrian children\", line = 1.7) mtext(c(\"Accidents\", \"Exposure\"), side = 2, line = 1.5, adj = c(0.77, 0.29)) mtext(\"Time\", side = 1, line = 1.5, at = 1995.6) plot(accidents[, c(\"seniors_accidents\", \"seniors_exposure\")],      mar = c(0, 0, 0, 0), oma = c(3, 3, 1.5, .1),      main = \"\", xlab = \"\", ylab = \"\", ann = FALSE) title(\"Road accidents involving pedestrian seniors\", line = 1.7) mtext(c(\"Accidents\", \"Exposure\"), side = 2, line = 1.5, adj = c(0.77, 0.29)) mtext(\"Time\", side = 1, line = 1.5, at = 1995.6)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter02.html","id":"example-2-3-posterior-inference-for-the-road-safety-data-flat-prior","dir":"Articles","previous_headings":"Section 2.1","what":"Example 2.3: Posterior inference for the Road Safety Data (flat prior)","title":"Chapter 2: A First Bayesian Analysis of Count Data","text":"posterior flat prior Œº|ùê≤‚àºùí¢(Ny‚Äæ+1,N), \\mu|\\mathbf{y} \\sim \\mathcal G(N\\bar y + 1, N),  visualize seniors, along 0.025-, 0.5-, 0.975-quantile.","code":"y <- accidents[, \"seniors_accidents\"] aN <- sum(y) + 1 bN <- length(y)  mu <- seq(4.5, 6, by = 0.01) plot(mu, dgamma(mu, aN, bN), type = \"l\", xlab = expression(mu), ylab = \"\",      main = \"Posterior density and quantiles\") abline(h = 0, lty = 3)  probs <- c(0.025, .5, .975) qs <- qgamma(probs, aN, bN) round(qs, digits = 3) #> [1] 4.936 5.253 5.584 ds <- dgamma(qs, aN, bN)  for (i in seq_along(probs)) {   lines(c(qs[i], qs[i]), c(0, ds[i]), lty = 2, col = \"dimgrey\") } mtext(round(qs, 3), side = 1, at = qs, line = -1, cex = 0.8, col = \"dimgrey\")  plot(mu, pgamma(mu, aN, bN), type = \"l\", xlab = expression(mu), ylab = \"\",      main = \"Posterior cdf and quantiles\") abline(h = c(0, 1), lty = 3) mtext(round(probs, 3), side = 2, at = probs, adj = c(0, .5, 1), cex = .8, col = \"dimgrey\") mtext(round(qs, 3), side = 1, at = qs, line = -1, cex = 0.8, col = \"dimgrey\")  for (i in seq_along(probs)) {   lines(c(.9 * min(mu), qs[i]), c(probs[i], probs[i]), lty = 2, col = \"dimgrey\")   lines(c(qs[i], qs[i]), c(probs[i], 0), lty = 2, col = \"dimgrey\") }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter02.html","id":"example-2-4-posterior-inference-for-the-road-safety-data-gamma-prior","dir":"Articles","previous_headings":"Section 2.1","what":"Example 2.4: Posterior inference for the Road Safety Data (gamma prior)","title":"Chapter 2: A First Bayesian Analysis of Count Data","text":"choose several values m0m_0 a0a_0. Note , formally, choosing m0=‚àûm_0 = \\infty a0=1a_0 = 1 gives improper prior .","code":"m0 <- c(Inf, mean(y), mean(y), mean(y), 0.5, 3, 10) a0 <- c(1, 0.5, 2, 10, 3, 3, 3) plot(mu, dgamma(mu, a0[1] + sum(y), a0[1]/m0[1] + length(y)), type = \"l\",      xlab = expression(mu), ylab = \"\", main = \"Posterior density\") for (i in 2:4) {   lines(mu, dgamma(mu, a0[i] + sum(y), a0[i]/m0[i] + length(y)),         col = i, lty = i) } legend(\"topright\", legend = c(paste0(\"a0 = \", a0[2:4]), \"improper\"),        col = c(2:4, 1), lty = c(2:4, 1))  plot(mu, dgamma(mu, a0[1] + sum(y), a0[1]/m0[1] + length(y)), type = \"l\",      xlab = expression(mu), ylab = \"\", main = \"Posterior density\") for (i in 5:7) {   lines(mu, dgamma(mu, a0[i] + sum(y), a0[i]/m0[i] + length(y)),         col = i - 3, lty = i - 3) } legend(\"topright\", legend = c(paste0(\"m0 = \", m0[5:7]), \"improper\"),        col = c(5:7 - 3, 1), lty = c(5:7 - 3, 1)) b0 <- a0 / m0 aN <- a0 + sum(y) bN <- b0 + length(y) res <- cbind(a0 = a0, b0 = b0, m0 = m0,              postmean = aN / bN,              postSD = sqrt(aN / bN^2),              leftpostquant = qgamma(0.025, aN, bN),              rightpostquant = qgamma(0.975, aN, bN)) knitr::kable(round(res, 3))"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter02.html","id":"example-2-8-including-exposures-for-the-road-safety-data","dir":"Articles","previous_headings":"Section 2.2","what":"Example 2.8: Including exposures for the Road Safety Data","title":"Chapter 2: A First Bayesian Analysis of Count Data","text":"now include exposure time ii, eie_i, estimate monthly risk Œª\\lambda children killed seriously injured via following likelihood assumption: yi‚àºùí´(Œªei),=1,‚Ä¶,N. y_i \\sim \\mathcal P(\\lambda e_i), \\quad = 1,\\dots,N.  Note still assume data independently (identically!) distributed. proceed computing visualizing posterior different prior hyperparameter choices.  data-driven priors, get posterior mean. credible intervals differ slightly.","code":"y <- accidents[, \"children_accidents\"] exp <- accidents[, \"children_exposure\"] lambdahat <- mean(y)/mean(exp) a0 <- c(1, 0.5, 1, 2) b0 <- c(0, a0[-1]/lambdahat) aN <- a0 + sum(y) bN <- b0 + sum(exp)  lambda <- seq(0.00016, 0.00026, by = .000001) plot(lambda, dgamma(lambda, aN[1], bN[1]), type = \"l\",      xlab = expression(lambda), ylab = \"\",      main = \"Posterior densities for various priors\") for (i in 2:length(aN)) lines(lambda, dgamma(lambda, aN[i], bN[i]), lty = i, col = i)  hyperparams <- paste0(\"a0 = \", formatC(a0, 1, format = \"f\"),                       \", \", \"b0 = \", round(b0))  legend(\"topright\", hyperparams, lty = 1:4, col = 1:4) postmean <- aN/bN leftpostquant <- qgamma(.025, aN, bN) rightpostquant <- qgamma(.975, aN, bN) res <- cbind(leftpostquant, postmean, rightpostquant) rownames(res) <- hyperparams res #>                     leftpostquant     postmean rightpostquant #> a0 = 1.0, b0 = 0     0.0001870605 0.0002081851   0.0002304234 #> a0 = 0.5, b0 = 2409  0.0001865176 0.0002075970   0.0002297886 #> a0 = 1.0, b0 = 4817  0.0001865321 0.0002075970   0.0002297725 #> a0 = 2.0, b0 = 9634  0.0001865610 0.0002075970   0.0002297405"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter02.html","id":"example-2-9-including-a-structural-break-for-the-road-safety-data","dir":"Articles","previous_headings":"Section 2.2","what":"Example 2.9: Including a structural break for the Road Safety Data","title":"Chapter 2: A First Bayesian Analysis of Count Data","text":"now continue Poisson model (known) structural break =94i = 94 (, October 1994). data stored time series (ts) object, can use window function conveniently extract two time frames.","code":"accidents1 <- window(accidents, end = c(1994, 9)) accidents2 <- window(accidents, start = c(1994, 10))  a01 <- a02 <- 1 b01 <- b02 <- 0  aN1 <- a01 + sum(accidents1[, \"children_accidents\"]) aN2 <- a02 + sum(accidents2[, \"children_accidents\"]) bN1 <- b01 + length(accidents1[, \"children_accidents\"]) bN2 <- b02 + length(accidents2[, \"children_accidents\"])  post <- function(x1, x2, aN1, aN2, bN1, bN2) {   dgamma(x1, aN1, bN1) * dgamma(x2, aN2, bN2) }  mu1 <- seq(1, 3, by = 0.03) mu2 <- mu1 z <- outer(mu1, mu2, post, aN1 = aN1, aN2 = aN2, bN1 = bN1, bN2 = bN2)  nrz <- nrow(z) ncz <- ncol(z)  # Generate the desired number of colors from this palette nbcol <- 20 color <- topo.colors(nbcol)  # Compute the z-value at the facet centres  zfacet <- z[-1, -1] + z[-1, -ncz] + z[-nrz, -1] + z[-nrz, -ncz] # Recode facet z-values into color indices facetcol <- cut(zfacet, nbcol)  persp(mu1, mu2, z, col = color[facetcol], ticktype = \"detailed\", zlab = \"\",       xlab = \"mu1\", ylab = \"mu2\",       phi = 20, theta = -30)  contour(mu1, mu2, z, col = color, xlab = bquote(mu[1]), ylab = bquote(mu[2])) abline(0,1)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter02.html","id":"example-2-10-obtaining-posterior-draws-for-the-road-safety-data","dir":"Articles","previous_headings":"Section 2.2","what":"Example 2.10: Obtaining posterior draws for the Road Safety Data","title":"Chapter 2: A First Bayesian Analysis of Count Data","text":"now proceed simple Monte Carlo approximation (nonlinear) functionals posterior.","code":"set.seed(1)  nsamp <- 2000 mu1 <- rgamma(nsamp, aN1, bN1) mu2 <- rgamma(nsamp, aN2, bN2) delta1 <- mu2 - mu1 delta2 <- 100 * (mu2 - mu1) / mu1   ts.plot(delta1, main = bquote(\"Trace plot of \" ~ delta[1]),         ylab = expression(delta[1]), xlab = \"Iteration\") ts.plot(delta2, main = bquote(\"Trace plot of \" ~ delta[2]),         ylab = expression(delta[2]), xlab = \"Iteration\") hist(delta1, breaks = 20, prob = TRUE, xlab = expression(delta[1]), ylab = \"\",      main = bquote(\"Histogram of \" ~ delta[1])) hist(delta2, breaks = 20, prob = TRUE, xlab = expression(delta[2]), ylab = \"\",      main = bquote(\"Histogram of \" ~ delta[2]))"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"figure-3-1-posteriors-under-the-beta-binomial-model","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model","what":"Figure 3.1: Posteriors under the beta-binomial model","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"reproduce posteriors figure, simply need plug respective counts expression posterior density visualize accordingly.","code":"trueprop <- c(0, .1, .5) N <- c(100, 400) theta <- seq(0, 1, .001)  for (p in trueprop) {   for (n in N) {     aN <- n * p + 1     bN <- n - n * p + 1     plot(theta, dbeta(theta, aN, bN), type = \"l\", xlab = expression(vartheta),          ylab = \"\", main = bquote(N == .(n) ~ \"and\" ~ S[N] == .(n * p)))   } }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"calibrating-the-beta-prior","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model","what":"Calibrating the Beta prior","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"Let m0=a0a0+b0m_0 = \\frac{a_0}{a_0 + b_0} denote prior mean N0=a0+b0N_0 = a_0 + b_0 strength prior information.","code":"m0 <- c(.5, .1, .2, .2) N0 <- c(2, 2, 2, 1) a <- N0 * m0 b <- N0 * (1 - m0) N <- 400 SN <- c(40, 4)  for (i in seq_along(SN)) {   aN <- SN[i] + a   bN <- N - SN[i] + b   theta <- seq(0, 3 * SN[i] / N, length.out = 200)   plot(theta, dbeta(theta, aN[1], bN[1]), type = \"l\", ylab = \"\",        xlab = expression(vartheta), ylim = range(dbeta(theta, aN, bN)),        main = bquote(\"Posterior when\" ~ S[N] == .(SN[i]) ~ \"and\" ~ N == .(N)))   for (j in 2:4) {     lines(theta, dbeta(theta, aN[j], bN[j]), col = j, lty = j)   }   legend(\"topright\", c(\"flat\", paste(paste0(\"m0 = \", m0[2:4]),                                      paste0(\" N0 = \", N0[2:4]), sep = \",\")),          lty = 1:4, col = 1:4) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"example-3-1-figure-3-2-uncertainty-quantification-for-market-shares","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model","what":"Example 3.1 / Figure 3.2: Uncertainty quantification for market shares","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"city , 40 400 questioned people purchase certain product, rural community, 4 400. Assuming uniform prior, now compute equal-tailed intervals highest posterior density (HPD) intervals. Note R (generally) vectorized, can compute equal-tailed intervals without using loop. Now visualize findings.","code":"m0 <- 0.05 N0 <- 40 a <- N0 * m0 b <- N0 * (1 - m0) N <- 400 SN <- c(\"City A\" = 40, \"Rural area\" = 4) aN <- SN + a bN <- N - SN + b  gamma <- .95 alpha <- 1 - gamma  # Equal-tailed credible intervals leftET <- qbeta(alpha/2, aN, bN) rightET <- qbeta(1 - alpha/2, aN, bN)  # HPD intervals resolution <- 10000 grid <- seq(0, 1, length.out = resolution + 1) dist <- gamma * resolution  leftHPD <- rightHPD <- rep(NA_real_, length(aN)) for (i in seq_along(aN)) {   qs <- qbeta(grid, aN[i], bN[i])   minimizer <- which.min(diff(qs, lag = dist))   leftHPD[i] <- qs[minimizer]   rightHPD[i] <- qs[minimizer + dist] } left <- cbind(leftET, leftHPD) right <- cbind(rightET, rightHPD)  for (i in seq_along(aN)) {   for (j in seq_len(ncol(left))) {     len <- right[i,j] - left[i,j]     theta <- seq(0, right[i,j] + 1.2 * len, length.out = 100)        plot(theta, dbeta(theta, aN[i], bN[i]), type = \"l\",          xlab = expression(vartheta), ylab = \"\",          main = bquote(.(names(aN[i])) * \":\" ~ Pr(.(round(left[i,j], 4)) <=                        {vartheta <= .(round(right[i,j], 4))}) == .(gamma)))     lines(theta, dbeta(theta, a, b), lty = 2)     abline(h = 0, lty = 3)        polygon(c(left[i,j], left[i,j],               theta[theta > left[i,j] & theta < right[i,j]],               right[i,j], right[i,j]),             c(0, dbeta(left[i,j], aN[i], bN[i]),               dbeta(theta[theta >= left[i,j] & theta <= right[i,j]], aN[i], bN[i]),               dbeta(right[i,j], aN[i], bN[i]), 0),           col = \"red\")     arrows(x0 <- max(left[i,j] - .5 * len, 0.001),            y0 <- .3 * diff(par(\"usr\")[3:4]),            x1 <- left[i,j] - .03 * len,            y1 <- .03 * diff(par(\"usr\")[3:4]),            length = .05)     text(x0, y0, paste0(round(100 * pbeta(left[i,j], aN[i], bN[i]), 1), \"%\"),          pos = 3)          arrows(x0 <- right[i,j] + .5 * len,            y0 <- .3 * diff(par(\"usr\")[3:4]),            x1 <- right[i,j] + .05 * len,            y1 <- .03 * diff(par(\"usr\")[3:4]),            length = .05)     text(x0, y0, paste0(round(100 * (1 - pbeta(right[i,j], aN[i], bN[i])), 1), \"%\"),          pos = 3)   } }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"example-3-2-table-3-1-posterior-credible-intervals-under-the-beta-binomial-model","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model","what":"Example 3.2 / Table 3.1: Posterior credible intervals under the beta-binomial model","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"now proceed computing credible intervals synthetic example. desired intervals now stored can displayed. can also compare lengths.","code":"Ns <- rep(N, each = length(trueprop)) SNs <- Ns * rep(trueprop, length(N))  aN <- SNs + 1     bN <- Ns - SNs + 1      # Equal-tails credible intervals leftET <- qbeta(alpha/2, aN, bN) rightET <- qbeta(1 - alpha/2, aN, bN)  # HPD intervals resolution <- 10000 grid <- seq(0, 1, length.out = resolution + 1) dist <- gamma * resolution  leftHPD <- rightHPD <- rep(NA_real_, length(aN)) for (i in seq_along(aN)) {   qs <- qbeta(grid, aN[i], bN[i])   minimizer <- which.min(diff(qs, lag = dist))   leftHPD[i] <- qs[minimizer]   rightHPD[i] <- qs[minimizer + dist] }  res <- cbind(leftET, rightET, leftHPD, rightHPD) knitr::kable(round(res, 4)) res <- cbind(lengthET = rightET - leftET, lengthHPD = rightHPD - leftHPD) knitr::kable(round(res, 4))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"figure-3-3-one-sided-hypothesis-testing","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model","what":"Figure 3.3: One-sided hypothesis testing","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"now move forward assessing visualizing posterior probability œë\\vartheta (proportion defective items) less 1/20=0.051/20 = 0.05 N=100N = 100 SN‚àà{1,3,6,10}S_N \\\\{1,3,6,10\\}.","code":"theta <- seq(0, .15, .001) N <- 100 SN <- c(1, 3, 6, 10) aN <- SN + 1 bN <- N - SN + 1 for (i in seq_along(SN)) {   plot(theta, dbeta(theta, aN[i], bN[i]), type = \"l\", ylim = c(0, 40),        xlab = expression(vartheta), ylab = \"\",        main = bquote(N == 100 ~ \"and\" ~ S[N] == .(SN[i]) ~ \"     \" ~                        P(vartheta <= 0.05 ~ \"|\" ~ bold(y)) ~ \"=\" ~                        .(round(pbeta(0.05, aN[i], bN[i]), 3))))   abline(h = 0, lty = 3)   polygon(c(theta[theta <= 0.05], 0.05),           c(dbeta(theta[theta <= 0.05], aN[i], bN[i]), 0), col = \"red\") }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"example-3-4-labor-market-data","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model","what":"Example 3.4: Labor market data","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"load dataset package extract data needed anlaysis: Cross-tabuling unemployed (.e., income) white collar work gives: estimate risk woman positive income, income next year assume risk different white- blue-collar workers. four relevant data summaries given : uniform prior, parameters posterior distribution given : posterior expectations given visualize joint posterior (œë1,œë2)(\\vartheta_1, \\vartheta_2) using contour plot well density estimate difference œë1‚àíœë2\\vartheta_1 - \\vartheta_2.","code":"data(\"labor\", package = \"BayesianLearningCode\") labor <- subset(labor,                 income_1997 != \"zero\" & female,                 c(income_1998, wcollar_1986)) labor <- with(labor,               data.frame(unemployed = income_1998 == \"zero\",                          wcollar = wcollar_1986)) table(labor) #>           wcollar #> unemployed FALSE TRUE #>      FALSE   368  634 #>      TRUE     62   59 (N1 <- with(labor, sum(wcollar))) #> [1] 693 (S_N1 <- with(labor, sum(wcollar & unemployed))) #> [1] 59 (N2 <- with(labor, sum(!wcollar))) #> [1] 430 (S_N2 <- with(labor, sum(!wcollar & unemployed))) #> [1] 62 aN1 <- 1 + S_N1 bN1 <- 1 + N1 - S_N1 aN2 <- 1 + S_N2 bN2 <- 1 + N2 - S_N2 aN1 / (aN1 + bN1) #> [1] 0.08633094 aN2 / (aN2 + bN2) #> [1] 0.1458333 if (pdfplots) {   pdf(\"3-2_1.pdf\", width = 8, height = 5)   par(mgp = c(1, .5, 0), mar = c(2.2, 1.5, 2, .2), lwd = 2) } par(mfrow = c(1, 2)) post <- function(x1, x2, aN1, aN2, bN1, bN2) {   dbeta(x1, aN1, bN1) * dbeta(x2, aN2, bN2) }  vartheta1 <- seq(0.04, 0.13, length.out = 100) vartheta2 <- seq(0.08, 0.22, length.out = 100) z <- outer(vartheta1, vartheta2, post, aN1 = aN1, aN2 = aN2, bN1 = bN1, bN2 = bN2)  nrz <- nrow(z) ncz <- ncol(z)  # Generate the desired number of colors from this palette nbcol <- 20 color <- topo.colors(nbcol)  # Compute the z-value at the facet centres  zfacet <- z[-1, -1] + z[-1, -ncz] + z[-nrz, -1] + z[-nrz, -ncz] # Recode facet z-values into color indices facetcol <- cut(zfacet, nbcol)  contour(vartheta1, vartheta2, z, col = color,         xlab = bquote(vartheta[1]), ylab = bquote(vartheta[2]),         main = expression(paste(\"Posterior of (\", vartheta[1],                                 \", \", vartheta[2], \")\"))) abline(0,1) postdraws <- data.frame(vartheta1 = rbeta(10^6, aN1, bN1),                         vartheta2 = rbeta(10^6, aN2, bN2)) postdraws$diff <- with(postdraws, vartheta2 - vartheta1) plot(density(postdraws$diff),      xlab = bquote(vartheta[1] - vartheta[2]),      main = expression(paste(\"Posterior density p(\", vartheta[1],                                 \" - \", vartheta[2], \"| y)\"))) if (pdfplots) {   pdf(\"3-2_1.pdf\", width = 8, height = 5)   par(mgp = c(1, .5, 0), mar = c(2.2, 1.5, 2, .2), lwd = 2) } par(mfrow = c(1, 3)) labels <- c(bquote(vartheta[1]),             bquote(vartheta[2]),             bquote(vartheta[2] - vartheta[1])) for (i in 1:3) {     plot(postdraws[1:1000, i], type = \"l\", ylim = c(0, 0.25),          xlab = \"draw\", ylab = labels[i]) }"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"preparing-the-data","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model > Example 3.5: Bag of words","what":"Preparing the data","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"reading quote, simple manipulation converting lower case, getting rid newlines, splitting string individual words. Finally, remove potential empty words display resulting frequencies. define universe possible words, first look words dataset (shipped package). contains 1000 common English words, alongside frequency usage among total 100 million occurrences (cf.¬†https://www.eapfoundation.com/vocab/general/bnccoca/). illustration purposes, use top 50 words augment question mark words. leaves us 51 possible outcomes (universe). Now, check words data set included universe (, fool people) replace question mark.","code":"string <- \"You can fool some of the people all of the time, and all of the people some of the time, but you can not fool all of the people all of the time.\"  tmp <- tolower(string)                  ## convert to lower case tmp <- gsub(\"\\n\", \" \", tmp)             ## replace newlines with spaces tmp <- unlist(strsplit(tmp, \" |,|\\\\.\")) ## split at spaces, commas, or stops dat <- tmp[tmp != \"\"]                   ## remove empty words tab <- table(dat) knitr::kable(t(tab)) data(\"words\", package = \"BayesianLearningCode\") head(words) #>       word frequency #> 1        a   2525253 #> 2     able     47760 #> 3    about    192168 #> 4    above     25370 #> 5 absolute      9284 #> 6   accept     29026 top50 <- tail(words[order(words$frequency),], 50) universe <- c(\"?\", sort(top50$word)) freq <- c(10^8 - sum(top50$frequency), top50$frequency[order(top50$word)]) universe #>  [1] \"?\"     \"a\"     \"all\"   \"and\"   \"as\"    \"at\"    \"be\"    \"but\"   \"by\"    #> [10] \"can\"   \"do\"    \"for\"   \"from\"  \"get\"   \"go\"    \"have\"  \"he\"    \"i\"     #> [19] \"if\"    \"in\"    \"it\"    \"make\"  \"more\"  \"no\"    \"not\"   \"of\"    \"on\"    #> [28] \"one\"   \"or\"    \"out\"   \"say\"   \"she\"   \"so\"    \"some\"  \"that\"  \"the\"   #> [37] \"there\" \"they\"  \"this\"  \"time\"  \"to\"    \"up\"    \"we\"    \"what\"  \"when\"  #> [46] \"which\" \"who\"   \"will\"  \"with\"  \"would\" \"you\" dat[!(dat %in% universe)] <- \"?\" knitr::kable(t(table(dat)))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"computing-the-posterior-under-the-uniform-prior","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model > Example 3.5: Bag of words","what":"Computing the posterior under the uniform prior","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"uniform prior (.e., 51 elements universe receive one pseudo-count), posterior mean word occurrence probability given E(Œ∑k|ùê≤)=1+Nk51+N,k=1,‚Ä¶,51, E(\\eta_k|\\boldsymbol{y}) = \\frac{1+N_k}{ 51 + N}, \\quad k=1,\\ldots,51,  NkN_k stands number data occurrences kkth word universe, NN sum NkN_ks. can easily implemented merging universe data, simply counting resulting frequencies.","code":"merged <- c(universe, dat) counts <- table(merged) - 1L post_uniform_unnormalized <- 1 + counts post_uniform <- post_uniform_unnormalized / sum(post_uniform_unnormalized)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"computing-the-posterior-under-a-less-informative-indifference-prior","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model > Example 3.5: Bag of words","what":"Computing the posterior under a less informative indifference prior","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"Note strategy implicitly assumes exactly 51 pseudo-observations. render prior less influential, can rescale , e.g., 1 pseudo-observation. , posterior expectation E(Œ∑k|ùê≤)=1/51+Nk1+N,k=1,‚Ä¶,51. E(\\eta_k|\\boldsymbol{y}) = \\frac{1/51+N_k}{1 + N}, \\quad k=1,\\ldots,51.  compute expectation simply add counts new pseudo-counts Œ≥0,1,=Œ≥0,2=‚Ä¶=Œ≥0,K=1/K\\gamma_{0,1}, = \\gamma_{0,2} = \\dots = \\gamma_{0,K} = 1/K. Alternatively, use loop. results must numerically equivalent, can verify easily. Summing far.","code":"K <- length(universe) N0 <- 1 gamma0 <- rep(N0 / K, length(universe)) post_lessinformative_unnormalized <-  gamma0 + counts post_lessinformative <-   post_lessinformative_unnormalized / sum(post_lessinformative_unnormalized) post_lessinformative_unnormalized2 <- rep(NA_real_, length(universe)) for (i in seq_along(universe)) {   Nk <- sum(dat == universe[i])   post_lessinformative_unnormalized2[i] <- gamma0[i] + Nk } post_lessinformative2 <-   post_lessinformative_unnormalized2 / sum(post_lessinformative_unnormalized2) all(abs(post_lessinformative - post_lessinformative2) < 1e-10) #> [1] TRUE dirichlet_sd <- function(gamma) {   mean <- gamma / sum(gamma)   sd <- sqrt((mean * (1 - mean)) / (sum(gamma) + 1))   sd }  resfull <- cbind(prior_mean = rep(1/K, K),                  prior_sd_uniform = dirichlet_sd(rep(1, K)),                  prior_sd_lessinformative = dirichlet_sd(rep(1/K, K)),                  rel_freq = counts / sum(counts),                  posterior_mean_uniform = post_uniform,                  posterior_sd_uniform = dirichlet_sd(post_uniform),                  posterior_mean_lessinformative = post_lessinformative,                  posterior_sd_lessinformative = dirichlet_sd(post_lessinformative))  unseen <- counts == 0L res <- rbind(resfull[!unseen,], UNSEEN = resfull[which(unseen)[1],]) knitr::kable(t(round(res, 4)))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"computing-the-posterior-under-an-informed-prior","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model > Example 3.5: Bag of words","what":"Computing the posterior under an informed prior","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"One might consider using yet another prior (label informed). instance, might want fix total number pseudo-counts N0N_0 , say, one fifth number observations NN (implies data prior ratio 5 1). word universe weighted according frequency appearance English language. Remember prior probability word outside top 50 English words 10810^8 (total number words corpus) minus sum top 50 counts. compute posterior, can add actual counts new pseudo-counts. Alternatively, might want prefer heavier shrinkage respect base rate (, overall word distribution English language). can simply accomplished increasing total number pseudo-counts N0N_0.","code":"N <- length(dat) N0 <- N/5 gamma0 <- N0 * freq / 10^8  post_informed_unnormalized <- gamma0 + counts post_informed <- post_informed_unnormalized / sum(post_informed_unnormalized) N0 <- 5*N gamma0 <- N0 * freq / 10^8  post_informed_unnormalized2 <- counts + gamma0 post_informed2 <- post_informed_unnormalized2 / sum(post_informed_unnormalized2)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter03.html","id":"visualizing-the-results","dir":"Articles","previous_headings":"Section 3.1: Data arising from a homogeneous population: The beta-binomial model > Example 3.5: Bag of words","what":"Visualizing the results","title":"Chapter 3: A First Bayesian Analysis of Unknown Probabilities","text":"display results, use bar plots. also add prior probabilities word via green triangles observed relative frequencies via red circles. Sorting done via frequency counts.  top panel (based uniform prior), can nicely see add-one smoothing effect, .e., strong shrinkage towards uniform prior, whereas second panel shows almost smoothing/shrinkage, unseen words estimated extremely unlikely. two bottom panels show posteriors informed priors, one light one heavy shrinkage towards prior.","code":"ord <- order(counts, decreasing = TRUE) midpts <- barplot(post_uniform[ord], las = 2,                   ylim = c(0, max(counts/N, post_uniform))) points(midpts, counts[ord]/N, col = 2, pch = 1) points(midpts, rep(1/51, length(midpts)), col = 3, pch = 2) title(\"Indifference prior (uniform)\") legend(\"topright\", c(\"Observed frequencies\", \"Prior expectations\"),        col = 2:3, pch = 1:2)  midpts <- barplot(post_lessinformative[ord], las = 2,                   ylim = c(0, max(counts/N, post_lessinformative))) points(midpts, counts[ord]/N, col = 2, pch = 1) points(midpts, rep(1/51, length(midpts)), col = 3, pch = 2) title(\"Indifference prior (light shrinkage)\")  midpts <- barplot(post_informed[ord], las = 2,                    ylim = c(0, max(counts/N, post_informed, freq/10^8))) points(midpts, counts[ord]/N, col = 2, pch = 1) points(midpts, freq[ord] / 10^8, col = 3, pch = 2) title(\"Informed prior (light shrinkage)\")  midpts <- barplot(post_informed2[ord], las = 2,                    ylim = c(0, max(counts/N, post_informed2, freq/10^8))) points(midpts, counts[ord]/N, col = 2, pch = 1) points(midpts, freq[ord] /10^8, col = 3, pch = 2) title(\"Informed prior (heavy shrinkage)\")"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter04.html","id":"example-4-1-the-data","dir":"Articles","previous_headings":"Swiss franc versus US dollar","what":"Example 4.1: The data","title":"Chapter 4: A First Bayesian Analysis of Continuous Data","text":"use daily exchange rate data contained package stochvol, covering period January 3, 2000, April 4, 2012. interested percentage log returns Swiss franc (CHF) US dollar (USD).","code":"data(\"exrates\", package = \"stochvol\") y <- 100 * diff(log(exrates$USD / exrates$CHF)) N <- length(y) hist(y, breaks = 50, main = \"Histogram\",      xlab = \"CHF/USD daily percentage log returns\") ts.plot(y, main = \"Time series plot\", ylab = \"CHF/USD percentage log returns\",         xlab = \"Time (days)\")"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter04.html","id":"example-4-2-a-first-posterior","dir":"Articles","previous_headings":"Swiss franc versus US dollar","what":"Example 4.2: A first posterior","title":"Chapter 4: A First Bayesian Analysis of Continuous Data","text":"first joint inference Œº\\mu œÉ2\\sigma^2, assume Gaussian likelihood, p(ùê≤|Œº,œÉ2)=‚àè=1Np(yi|Œº,œÉ2)=(12œÄœÉ2)N/2exp(‚àí1œÉ2‚àë=1N(yi‚àíŒº)22). p(\\mathbf y|\\mu,\\sigma^2) = \\prod_{=1}^N p(y_i|\\mu, \\sigma^2)=    \\left(\\frac{1}{2 \\pi \\sigma^2}\\right)^{N/2}  \\exp \\left( - \\frac{1}{\\sigma^2} \\sum_{=1}^{N} \\frac{(y_i-\\mu)^2}{2} \\right). addition, assume improper prior p(Œº,œÉ2)‚àù1œÉ2, p(\\mu, \\sigma^2) \\propto \\frac{1}{\\sigma^2},  yielding posterior p(Œº,œÉ2|ùê≤)=fN(Œº;y‚Äæ,œÉ2N)fùí¢‚àí1(œÉ2;N‚àí12,Nsy22). p(\\mu,\\sigma^2|\\mathbf y) = f_N\\left(\\mu;\\bar{y},\\frac{\\sigma^2}{N}\\right) f_{\\mathcal{G}^{-1}}\\left(\\sigma^2;\\frac{N-1}{2},\\frac{N s_y^2}{2}\\right).  able visualize posterior, need little bit preparation work. base R ship density function inverse gamma distribution, define using transformation law densities. also add cumulative distribution function quantile function, need later. Note drawing inverse gamma variates amounts drawing gamma variates taking reciprocal. can now define posterior density function. Note R, Gaussian distribution parameterized terms mean standard deviation (mean variance). Now can visualize.","code":"dinvgamma <- function(x, a, b, log = FALSE) {   logdens <- dgamma(1/x, a, b, log = TRUE) - 2 * log(x)   if (log) logdens else exp(logdens) }  pinvgamma <- function(q, a, b) {   1 - pgamma(1 / q, a, b) }  qinvgamma <- function(p, a, b) {   1 / qgamma(1 - p, a, b) }  rinvgamma <- function(ndraws, a, b) {   1 / rgamma(ndraws, a, b) } posterior <- function(mu, sigma2, ybar, s2, N) {   dnorm(mu, ybar, sqrt(sigma2 / N)) *     dinvgamma(sigma2, (N - 1) / 2, N * s2 / 2) } mu <- seq(-.25, .25, length.out = 30) sigma2 <- seq(.4, .7, length.out = 30)  # Generate the desired number of colors from a palette nbcol <- 20 color <- topo.colors(nbcol)  # Plot for different sample sizes for (n in c(50, 100, 200, 500, 1000, N)) {   ytmp <- head(y, n)   z <- outer(mu, sigma2, posterior, ybar = mean(ytmp),              s2 = var(ytmp) * (n - 1) / n, N = n)    # Compute the density values at the facet centers   zfacet <- z[-1, -1] + z[-1, -ncol(z)] + z[-nrow(z), -1] +     z[-nrow(z), -ncol(z)]    # Recode facet density values into color indices   facetcol <- cut(zfacet, nbcol)    # Perspective plot   pmat <- persp(mu, sigma2, z, col = color[facetcol], ticktype = \"detailed\",                 main = paste0(\"N = \", n), nticks = 4, zlab = \"\", xlab = \"\",                 ylab = \"\", phi = 20, theta = -30, r = 100)      # Add axis labels (done manually because persp does not support expressions)   loc <- trans3d(mean(mu) - .15 * diff(range(mu)),                  min(sigma2) - .4 * diff(range(sigma2)),                  0, pmat = pmat)   text(loc$x, loc$y, expression(mu), cex = 1.3)      loc <- trans3d(min(mu) - .35 * diff(range(mu)),                  mean(sigma2) - .1 * diff(range(sigma2)),                  0, pmat = pmat)   text(loc$x, loc$y, expression(sigma^2), cex = 1.3) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter04.html","id":"example-4-3-posterior-marginals","dir":"Articles","previous_headings":"Swiss franc versus US dollar","what":"Example 4.3: Posterior marginals","title":"Chapter 4: A First Bayesian Analysis of Continuous Data","text":"now want visualize univariate marginals bivariate posterior. , need preparations, R natively cater generalized Student-tt distribution. first define (cumulative) density quantile functions via original Student-tt distribution. Now visualize marginals quantiles thereof.  Posterior expectation equal-tailed 95% Bayesian CI œÉ2|ùê≤\\sigma^2|\\mathbf{y} easily computed. find HPD can use grid search.","code":"dstudt <- function(x, location = 0, scale = 1, df, log = FALSE) {   logdens <- dt((x - location) / scale, df = df, log = TRUE) - log(scale)   if (log) logdens else exp(logdens) }  pstudt <- function(x, location = 0, scale = 1, df) {   pt((x - location) / scale, df = df) }  qstudt <- function(p, location = 0, scale = 1, df) {   location + scale * qt(p, df = df) }  rstudt <- function(n, location = 0, scale = 1, df) {   location + scale * rt(n, df = df) } location <- mean(y) scale <- sqrt(var(y) / N) df <- N - 1 probs <- c(0.025, .5, .975)  mu <- seq(-.06, .06, length.out = 300) plot(mu, dstudt(mu, location, scale, df), type = \"l\", xlab = expression(mu),      ylab = \"\", main = \"Posterior density and quantiles\") abline(h = 0, lty = 3)  qs <- qstudt(probs, location, scale, df) ds <- dstudt(qs, location, scale, df)  for (i in seq_along(probs)) {   lines(c(qs[i], qs[i]), c(0, ds[i]), lty = 2) } mtext(round(qs, 3), side = 1, line = -1, at = qs, cex = .5)  plot(mu, pstudt(mu, location, scale, df), type = \"l\", xlab = expression(mu),      ylab = \"\", main = \"Posterior cdf and quantiles\") abline(h = c(0, 1), lty = 3)  myxlim <- par(\"usr\")[1] - .1 * diff(par(\"usr\")[1:2]) for (i in seq_along(probs)) {   lines(c(myxlim, qs[i]), c(probs[i], probs[i]), lty = 2)   lines(c(qs[i], qs[i]), c(probs[i], 0), lty = 2) } mtext(round(qs, 3), side = 1, line = -1, at = qs, cex = .5) mtext(probs, side = 2, at = probs, cex = .5, adj = c(0, .5, 1))  cN <- (N - 1) / 2 CN <- var(y) * (N - 1) / 2  sigma2 <- seq(0.45, .6, length.out = 300) plot(sigma2, dinvgamma(sigma2, cN, CN), type = \"l\", xlab = expression(sigma^2),      ylab = \"\", main = \"Posterior density and quantiles\") abline(h = 0, lty = 3)  qs <- qinvgamma(probs, cN, CN) ds <- dinvgamma(qs, cN, CN)  for (i in seq_along(probs)) {   lines(c(qs[i], qs[i]), c(0, ds[i]), lty = 2) } mtext(round(qs, 3), side = 1, line = -1, at = qs, cex = .5)  plot(sigma2, pinvgamma(sigma2, cN, CN), type = \"l\", xlab = expression(sigma^2),      ylab = \"\", main = \"Posterior cdf and quantiles\") abline(h = c(0, 1), lty = 3)  myxlim <- par(\"usr\")[1] - .1 * diff(par(\"usr\")[1:2])  for (i in seq_along(probs)) {   lines(c(myxlim, qs[i]), c(probs[i], probs[i]), lty = 2)   lines(c(qs[i], qs[i]), c(probs[i], 0), lty = 2) } mtext(round(qs, 3), side = 1, line = -1, at = qs, cex = .5) mtext(probs, side = 2, at = probs, cex = .5, adj = c(0, .5, 1)) round(CN / (cN - 1), 3) #> [1] 0.528 round(qinvgamma(c(.025, .975), cN, CN), 4) #> [1] 0.5029 0.5553 resolution <- 10000 grid <- seq(0, 1, length.out = resolution + 1) dist <- 0.95 * resolution  qs <- qinvgamma(grid, cN, CN) minimizer <- which.min(diff(qs, lag = dist)) HPD <- qs[c(minimizer, minimizer + dist)] round(HPD, 4) #> [1] 0.5025 0.5548"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter04.html","id":"example-4-4-fitting-a-student-t-distribution-with-known-mean-and-known-degrees-of-freedom","dir":"Articles","previous_headings":"Swiss franc versus US dollar","what":"Example 4.4: Fitting a Student-tt distribution with known mean and known degrees of freedom","title":"Chapter 4: A First Bayesian Analysis of Continuous Data","text":"first define post_nonnormalized_nonvec, function evaluating non-normalized posterior density œÉ2\\sigma^2. function vectorized first argument, œÉ2\\sigma^2. Rather, expects scalar sigma2 data vector y. can, however, vectorize using Vectorize (fasted methods R, just fine use-case). Now can plot. Note (arbitrarily) standardize non-normalized posterior maximum one. addition, visualize non-normalized version prior. Concerning cumulative distribution function, simply divide highest (last) value achieve normalization.  determine normalizing constant CC, can perform numerical integration via trapezoid rule. Now can numerically approximate posterior expectation standard deviation, using trapezoid rule.","code":"post_nonnormalized_nonvec <- function(sigma2, y, nu, log = FALSE) {   logdens <- -length(y) / 2 * log(sigma2) -     (nu + 1) / 2 * sum(log(1 + y^2 / (nu * sigma2))) - log(sigma2)   if (log) logdens else exp(logdens) } post_nonnormalized <- Vectorize(post_nonnormalized_nonvec, \"sigma2\") nu <- 7 sigma2 <- seq(0.25, 0.45, length.out = 3000) pdf_u <- post_nonnormalized(sigma2, y = y, nu = nu) pdf_u <- pdf_u / max(pdf_u)  plot(sigma2, pdf_u, type = \"l\", xlab = expression(sigma^2),      ylab = \"\", main = \"Non-normalized densities\") abline(h = 0, lty = 3) lines(sigma2, 0.05 * 1/sigma2, lty = 2, col = 2) legend(\"topright\", c(\"Posterior\", \"Prior\"), col = 1:2, lty = 1:2)  cdf_u <- cumsum(pdf_u) / sum(pdf_u) plot(sigma2, cdf_u, type = \"l\", xlab = expression(sigma^2),      ylab = \"\", main = \"Cumulative posterior\") abline(h = c(0, cdf_u[length(cdf_u)]), lty = 3) resolution <- 100 grid <- seq(0.25, 0.45, length.out = resolution + 1) integrand <- post_nonnormalized(grid, y = y, nu = nu) C <- sum(diff(grid) * .5 * (head(integrand, -1) + tail(integrand, -1))) integrand2 <- grid * integrand e <- sum(diff(grid) * .5 * (head(integrand2, -1) + tail(integrand2, -1))) / C round(e, 3) #> [1] 0.354  integrand3 <- grid^2 * integrand m2 <- sum(diff(grid) * .5 * (head(integrand3, -1) + tail(integrand3, -1))) / C v <- m2 - e^2 round(sqrt(v), 3) #> [1] 0.011"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter04.html","id":"example-4-5-bayesian-learning-of-quantiles","dir":"Articles","previous_headings":"Swiss franc versus US dollar","what":"Example 4.5: Bayesian learning of quantiles","title":"Chapter 4: A First Bayesian Analysis of Continuous Data","text":"learn distribution certain low quantile, Value--Risk (VaR), can generate draws . Gaussian model mean zero p(œÉ2)‚àùœÉ‚àí2p(\\sigma^2) \\propto \\sigma^{-2}, simulate ùí¢‚àí1(N/2,‚àëyi2/2)\\mathcal{G}^{-1}(N/2, \\sum{y_i^2}/2) , conditionally draws, compute quantiles interest ùí©(0,œÉ2)\\mathcal{N}(0, \\sigma^2). Note, , R‚Äôs vectorization ability comes handy. , particular, rnorm can handle -called varying parameter case. Student-tt model, can use inverse transform sampling. First, draw uniformly interval spanned 0 maximum non-normalized cumulative posterior. , draw, find interval pointwise cdf approximation posterior, interpolate linearly interval boundaries. Let us quick graphical check whether draws pdf align.  Now, can draw quantile distribution plot draws.  conclude, let us compute point interval estimates VaR.","code":"set.seed(2) alpha <- 0.005 ndraws <- 100000 sigma2draws <- rinvgamma(ndraws, N / 2, sum(y^2) / 2) qnormdraws <- qnorm(alpha, 0, sqrt(sigma2draws)) unifdraws <- runif(ndraws, 0, cdf_u[length(cdf_u)]) leftind <- findInterval(unifdraws, cdf_u) rightind <- leftind + 1L distprop <- (unifdraws - cdf_u[leftind]) / (cdf_u[rightind] - cdf_u[leftind]) sigma2draws <- sigma2[leftind] + distprop *   (sigma2[rightind] - sigma2[leftind]) myhist <- hist(sigma2draws, breaks = 100, xlab = expression(sigma^2),                ylab = \"\", main = \"Histogram of draws and non-normalized pdf\") lines(sigma2, max(myhist$counts) / max(pdf_u) * pdf_u, col = 2) qtdraws <- sqrt(sigma2draws) * qt(alpha, df = nu) minmax <- range(qnormdraws, qtdraws, quantile(y, alpha)) mybreaks <- seq(minmax[1] - .01 * diff(minmax),                 minmax[2] + .01 * diff(minmax),                 length.out = 100) hist(qnormdraws, breaks = mybreaks, ylab = \"\", xlab = bquote(Q[.(alpha)]),      main = bquote(\"Histogram of posterior\" ~ .(100 * alpha) *                    \"%-VaR draws under the\" ~ N(0, sigma^2) ~                    \"likelihood assumption\"), prob = TRUE) points(quantile(y, alpha), 0, col = 2, pch = 16, cex = 1.5) hist(qtdraws, breaks = mybreaks, ylab = \"\", xlab = bquote(Q[.(alpha)]),      main = bquote(\"Histogram of posterior\" ~ .(100 * alpha) *                    \"%-VaR draws under the\" ~ t[.(nu)](0, sigma^2) ~                    \"likelihood assumption\"), prob = TRUE) points(quantile(y, alpha), 0, col = 2, pch = 16, cex = 1.5) round(mean(qtdraws), 3) #> [1] -2.083 round(quantile(qtdraws, c(.025, .975)), 3) #>   2.5%  97.5%  #> -2.145 -2.022"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"example-5-3-figure-5-1-the-adaptive-nature-of-bayesian-learning","dir":"Articles","previous_headings":"Section 5.2: Conjugate or non-conjugate?","what":"Example 5.3 / Figure 5.1: The adaptive nature of Bayesian learning","title":"Chapter 5: Learning More About Bayesian Learning","text":"illustrate adaptive nature Bayesian learning (also referred sequential updating -line learning) via Beta-Bernoulli model earlier.","code":"set.seed(42)  a0 <- 1 b0 <- 1 N <- 100  thetatrue <- c(0, .1, .5) theta <- seq(0, 1, length.out = 201)  for (i in seq_along(thetatrue)) {   plot(theta, dbeta(theta, a0, b0), type = 'l', ylim = c(0, 11),        col = rgb(0, 0, 0, .2), xlab = expression(vartheta), ylab = '',        main = bquote(vartheta[true] == .(thetatrue[i])))      succ <- fail <- 0L   for (j in seq_len(N)) {     if (rbinom(1, 1, thetatrue[i])) succ <- succ + 1L else fail <- fail + 1L     lines(theta, dbeta(theta, a0 + succ, b0 + fail), col = rgb(0, 0, 0, .2 + .4*j/N))   }      legend(\"topright\", paste(\"N =\", c(0, 20, 40, 60, 80, 100)), lty = 1,          col = rgb(0, 0, 0, .2 + .4*c(0, 20, 40, 60, 80, 100)/N)) }"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"example-5-5-quantifying-and-visualizing-posterior-uncertainty-for-the-mean-and-the-variance-of-a-normal-distribution","dir":"Articles","previous_headings":"Section 5.3.2: Uncertainty quantification","what":"Example 5.5: Quantifying and visualizing posterior uncertainty for the mean and the variance of a normal distribution","title":"Chapter 5: Learning More About Bayesian Learning","text":"re-use posterior established Chapter 4, modeled CHF/USD exchange rate normal distribution unknown mean variance. First, read data . Now, simulate posterior Monte Carlo, .e., draw many times marginal posterior œÉ2|ùê≤\\sigma^2|\\mathbf{y}, , conditioning draws, Œº|œÉ2,ùê≤\\mu|\\sigma^2,\\mathbf{y}. Next, need fix Œ±\\alpha. approximate bivariate HPD regions posterior, obtain functional values keep top 99, 95, 90 percent, respectively. visualize, compute convex hull remaining draws. Note randomly exclude draws visualization order avoid plotting many overlapping points. also draw projections two axes, yielding corresponding univariate regions (intervals). addition, also draw univariate regions (intervals) corresponding marginals.","code":"library(\"BayesianLearningCode\") posteriorjoint <- function(mu, sigma2, ybar, s2, N) {   dnorm(mu, ybar, sqrt(sigma2 / N)) *     dinvgamma(sigma2, (N - 1) / 2, N * s2 / 2) } data(\"exrates\", package = \"stochvol\") y <- 100 * diff(log(exrates$USD / exrates$CHF)) N <- length(y) ybar <- mean(y) s2 <- var(y) * (N - 1) / N ndraws <- 100000 sigma2draws <- rinvgamma(ndraws, (N - 1) / 2, N * s2 / 2) mudraws <- rnorm(ndraws, ybar, sqrt(sigma2draws / N)) draws <- data.frame(mu = mudraws, sigma2 = sigma2draws) alpha <- c(.01, .05, .1) densjoint <- posteriorjoint(draws$mu, draws$sigma2, ybar, s2, N) HPDdrawsjoint <- vector(\"list\", length(alpha)) for (i in seq_along(alpha)) {   HPDdrawsjoint[[i]] <- draws[rank(densjoint) > floor(ndraws * alpha[i]),] } toplot <- sample.int(nrow(draws), 1000) plot(draws[toplot,], pch = 16, col = rgb(0, 0, 0, .3), cex = 2,      xlab = expression(mu), ylab = expression(sigma^2)) legend(\"topright\", paste0(100 * (1 - alpha), \"%\"),        fill = rgb(1, 0, 0, c(.2, .4, .6)), border = NA, bty = \"n\") legend(\"topright\", rep(\"          \", 3),        fill = rgb(0, 0, 1, c(.2, .4, .6)), border = NA, bty = \"n\")  for (i in seq_along(alpha)) {   hullind <- chull(HPDdrawsjoint[[i]])      polygon(HPDdrawsjoint[[i]][hullind,], col = rgb(1, 0, 0, .2), border = NA)      rect(min(HPDdrawsjoint[[i]]$mu),        par(\"usr\")[3] + .03 * diff(par(\"usr\")[3:4]),        max(HPDdrawsjoint[[i]]$mu),        par(\"usr\")[3] + .06 * diff(par(\"usr\")[3:4]),        col = rgb(1, 0, 0, .2), border = NA)      rect(par(\"usr\")[1] + .03 * 5 / 8 * diff(par(\"usr\")[1:2]),        min(HPDdrawsjoint[[i]]$sigma2),        par(\"usr\")[1] + .06 * 5 / 8 * diff(par(\"usr\")[1:2]),        max(HPDdrawsjoint[[i]]$sigma2),        col = rgb(1, 0, 0, .2), border = NA) }  posteriormu <- function(mu, ybar, s2, N) {   dstudt(mu, ybar, sqrt(s2 / (N - 1)), N - 1) }  posteriorsigma2 <- function(sigma2, s2, N) {   dinvgamma(sigma2, (N - 1) / 2, N * s2 / 2) }  HPDdrawsmu <- HPDdrawssigma2 <- vector(\"list\", length(alpha)) for (i in seq_along(alpha)) {   densmu <- posteriormu(draws$mu, ybar, s2, N)   HPDdrawsmu[[i]] <- draws$mu[rank(densmu) > floor(ndraws * alpha[i])]   rect(min(HPDdrawsmu[[i]]),        par(\"usr\")[3],        max(HPDdrawsmu[[i]]),        par(\"usr\")[3] + .03 * diff(par(\"usr\")[3:4]),        col = rgb(0, 0, 1, .2), border = NA)       denssigma2 <- posteriorsigma2(draws$sigma2, s2, N)   HPDdrawssigma2[[i]] <- draws$sigma2[rank(denssigma2) > floor(ndraws * alpha[i])]   rect(par(\"usr\")[1],        min(HPDdrawssigma2[[i]]),        par(\"usr\")[1] + .03 * 5 / 8 * diff(par(\"usr\")[1:2]),        max(HPDdrawssigma2[[i]]),        col = rgb(0, 0, 1, .2), border = NA) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"example-5-6-the-law-of-large-numbers-in-a-bayesian-context","dir":"Articles","previous_headings":"Section 5.3.2: Uncertainty quantification","what":"Example 5.6: The law of large numbers in a Bayesian context","title":"Chapter 5: Learning More About Bayesian Learning","text":"revisit CHF-USD exchange rate data previous chapter. assumed yi‚àºt7(0,œÉ2), y_i \\sim t_7(0, \\sigma^2),  implemented unnormalized version œÉ2|ùê≤\\sigma^2|\\mathbf{y}. compute normalizing constant CC, use trapezoid rule. addition, compute posterior expectation. Now show histogram posterior draws converges posterior density, sample average posterior draws converges posterior expectation. First, obtain posterior draws, previous chapter. Now, visualize batches draws.","code":"post_unnormalized_nonvec <- function(sigma2, y, nu, log = FALSE) {   logdens <- -length(y) / 2 * log(sigma2) -     (nu + 1) / 2 * sum(log(1 + y^2 / (nu * sigma2))) - log(sigma2)   if (log) logdens else exp(logdens) } post_unnormalized <- Vectorize(post_unnormalized_nonvec, \"sigma2\")  nu <- 7 sigma2 <- seq(0.25, 0.45, length.out = 3000) pdf_u <- post_unnormalized(sigma2, y = y, nu = nu) pdf_u <- pdf_u / max(pdf_u) cdf <- cumsum(pdf_u) / sum(pdf_u) resolution <- 100 grid <- seq(0.25, 0.45, length.out = resolution + 1) integrand <- post_unnormalized(grid, y = y, nu = nu) C <- sum(diff(grid) * .5 * (head(integrand, -1) + tail(integrand, -1)))  integrand2 <- grid * integrand e <- sum(diff(grid) * .5 * (head(integrand2, -1) + tail(integrand2, -1))) / C Mtotal <- 10000 unifdraws <- runif(Mtotal, 0, cdf[length(cdf)]) leftind <- findInterval(unifdraws, cdf) rightind <- leftind + 1L distprop <- (unifdraws - cdf[leftind]) / (cdf[rightind] - cdf[leftind]) sigma2draws <- sigma2[leftind] + distprop *   (sigma2[rightind] - sigma2[leftind]) for (M in c(100, 1000, 10000)) {   hist(sigma2draws[seq_len(M)], probability = TRUE, xlab = expression(sigma^2),        ylab = \"\", main = paste0(\"M = \", M),        breaks = seq(0.29, 0.41, length.out = 42))   lines(sigma2, post_unnormalized(sigma2, y, nu = nu) / C) }  nlines <- 10 M <- Mtotal / nlines plot(seq_len(M), NULL, ylim = c(0.33, 0.37), xlab = \"M\", ylab = \"Sample mean\",      log = \"x\", main = paste(\"Posterior sample means for\", nlines,                              \"Monte Carlo iterations and\", M, \"draws each\")) for (i in seq_len(nlines)) {   indices <- seq(1 + (i - 1) * M, i * M)   lines(seq_len(M), cumsum(sigma2draws[indices]) / seq_len(M), col = i + 1) } abline(h = e, lty = 2)"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"figure-5-4-bayesian-asymptotics-1","dir":"Articles","previous_headings":"Section 5.4","what":"Figure 5.4: Bayesian asymptotics 1","title":"Chapter 5: Learning More About Bayesian Learning","text":"reproduce figure, re-use theory Chapter 3 (Beta-Bernoulli model).","code":"set.seed(2) thetatrue <- c(0.02, 0.25) N <- c(25, 100, 400) settings <- expand.grid(N = N, thetatrue = thetatrue) SN <- matrix(rbinom(6, settings$N, settings$thetatrue), nrow = 3, ncol = 2) theta <- seq(0, 1, .0005)  for (i in seq_along(N)) {   for (j in seq_along(thetatrue)) {     aN <- SN[i,j] + 1     bN <- N[i] - SN[i,j] + 1     plot(theta, dbeta(theta, aN, bN), type = \"l\", xlab = expression(vartheta),          ylab = \"\", main = bquote(vartheta[true] == .(thetatrue[j]) ~                                   \"and\" ~ N == .(N[i]): ~ S[N] == .(SN[i,j])),          xlim = c(0, 1.1 * sqrt(thetatrue[j])),          ylim = c(0, 9.5 / sqrt(thetatrue[j] + 0.008)))          aN <- SN[i,j] + 2     bN <- N[i] - SN[i,j] + 4     lines(theta, dbeta(theta, aN, bN), lty = 2, col = 2)          legend(\"topright\", c(\"Beta(1,1)\", \"Beta(2,4)\"), lty = c(1, 2),            col = 1:2)   } }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"figure-5-5-bayesian-asymptotics-2","dir":"Articles","previous_headings":"Section 5.4","what":"Figure 5.5: Bayesian asymptotics 2","title":"Chapter 5: Learning More About Bayesian Learning","text":", just higher sample size.","code":"bigN <- 1000000 SbigN <- rbinom(2, bigN, thetatrue)  for (i in seq_along(thetatrue)) {   aN <- SbigN[i] + 1   bN <- bigN - SbigN[i] + 1   asySD <- sqrt(thetatrue[i] * (1 - thetatrue[i]) / bigN)   theta <- seq(SbigN[i] / bigN - 25 * asySD, SbigN[i] / bigN + 25 * asySD,                length.out = 333)   plot(theta, dbeta(theta, aN, bN), type = \"l\", xlab = expression(vartheta),        ylab = \"\",        main = bquote(vartheta[true] == .(thetatrue[i]) ~ \"and\" ~ N ==                      .(format(bigN, scientific = FALSE)): ~ S[N] == .(SbigN[i])))        aN <- SbigN[i] + 2   bN <- bigN - SbigN[i] + 4   lines(theta, dbeta(theta, aN, bN), lty = 2, col = 2)        legend(\"topright\", c(\"Beta(1,1)\", \"Beta(2,4)\"), lty = c(1, 2), col = 1:2) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"figure-5-6-bayesian-asymptotics-3","dir":"Articles","previous_headings":"Section 5.4","what":"Figure 5.6: Bayesian asymptotics 3","title":"Chapter 5: Learning More About Bayesian Learning","text":"now want approximate log posteriors via quadratic polynomials visualize .","code":"for (i in seq_along(N)) {   for (j in seq_along(thetatrue)) {     aN <- SN[i,j] + 1     bN <- N[i] - SN[i,j] + 1     asySD <- sqrt(thetatrue[j] * (1 - thetatrue[j]) / N[i])     theta <- seq(max(0.001, SN[i,j] / N[i] - 2 * asySD),                  SN[i,j] / N[i] + 2 * asySD,                  length.out = 222)     plot(theta, dbeta(theta, aN, bN), type = \"l\", xlab = expression(vartheta),          ylab = \"\", main = bquote(vartheta[true] == .(thetatrue[j]) ~                                   \"and\" ~ N == .(N[i]): ~ S[N] == .(SN[i,j])),          log = \"y\")          approx <- function(theta, thetastar, n, log = FALSE) {       res <- dbeta(thetastar, aN, bN, log = TRUE) -         0.5 * n * 1 / (thetastar * (1 - thetastar)) * (theta - thetastar)^2       if (log) res else exp(res)     }          lines(theta, approx(theta, SN[i,j] / N[i], N[i]), col = 2, lty = 2)          if (i == 1L && j == 1L) {       legend(\"bottom\", \"Posterior\", lty = 1)     } else {       legend(\"bottom\", c(\"Posterior\", \"Approximation\"), lty = c(1, 2), col = 1:2)     }   } }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"table-5-1-point-estimates","dir":"Articles","previous_headings":"Section 5.4","what":"Table 5.1: Point estimates","title":"Chapter 5: Learning More About Bayesian Learning","text":"now compute various point estimates several settings. Note use number 1s (SNS_N) .","code":"options(knitr.kable.NA = \"\") a0 <- c(1, 2) b0 <- c(1, 4) set <- expand.grid(N = N, thetatrue = thetatrue, a0 = a0) set$b0 <- rep(b0, each = nrow(set) / 2) set$SN <- rep(SN, 2)  set$aN <- set$SN + set$a0 set$bN <- set$N - set$SN + set$b0 postmean <- set$aN / (set$aN + set$bN) postmode <- (set$aN - 1) / (set$aN + set$bN - 2) thetaML <- postmode[1:6]  res <- cbind(thetatrue = c(thetatrue[1], NA, NA, thetatrue[2], NA, NA),              N = rep(N, 2),              SN = as.vector(SN),              ML = thetaML,              meanB11 = postmean[1:6],              modeB11 = postmode[1:6],              meanB42 = postmean[7:12],              modeB42 = postmode[7:12])  knitr::kable(round(res, 3))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"table-5-2-interval-estimates","dir":"Articles","previous_headings":"Section 5.4","what":"Table 5.2: Interval estimates","title":"Chapter 5: Learning More About Bayesian Learning","text":"now compute frequentist Bayesian CIs.","code":"# Confidence intervals leftconf <- thetaML - qnorm(.975) * sqrt(thetaML * (1 - thetaML) / set$N[1:6]) rightconf <- thetaML + qnorm(.975) * sqrt(thetaML * (1 - thetaML) / set$N[1:6])  # HPD intervals resolution <- 1000 grid <- seq(0, 1, length.out = resolution + 1) dist <- 0.95 * resolution  leftHPD <- rightHPD <- rep(NA_real_, 6) for (i in 1:6) {   qs <- qbeta(grid, set$aN[i], set$bN[i])   minimizer <- which.min(diff(qs, lag = dist))   leftHPD[i] <- qs[minimizer]   rightHPD[i] <- qs[minimizer + dist] }  res <- cbind(res[,1:3], leftconf, rightconf, leftHPD, rightHPD)  knitr::kable(round(res, 3))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"example-5-10-table-5-3-coverage-probabilities-under-the-beta-bernoulli-model","dir":"Articles","previous_headings":"Section 5.4","what":"Example 5.10 / Table 5.3: Coverage probabilities under the Beta-Bernoulli model","title":"Chapter 5: Learning More About Bayesian Learning","text":"(frequentist) estimate coverage probabilities, simulate many data sets parameter configurations check often intervals contain true parameter.","code":"alpha <- 0.05 nrep <- 10000 resolution <- 1000 grid <- matrix(seq(0, 1, length.out = resolution + 1),                nrow = resolution + 1,                ncol = nrep) dist <- (1 - alpha) * resolution inconf <- inHPD <- inequal <- lenconf <- lenHPD <- lenequal <-   matrix(NA, nrep, 6)  for (i in 1:6) {   SN <- rbinom(nrep, set$N[i], set$thetatrue[i])   aN <- SN + set$a0[i]   bN <- set$N[i] - SN + set$b0[i]   thetaML <- (aN - 1) / (aN + bN - 2)    # Asymptotic intervals   leftconf <- thetaML - qnorm(1 - alpha / 2) *     sqrt(thetaML * (1 - thetaML) / set$N[i])   rightconf <- thetaML + qnorm(1 - alpha / 2) *     sqrt(thetaML * (1 - thetaML) / set$N[i])   inconf[, i] <- leftconf <= set$thetatrue[i] & set$thetatrue[i] <= rightconf   lenconf[, i] <- rightconf - leftconf      # HPD intervals   leftHPD <- rightHPD <- rep(NA_real_, nrep)   qs <- qbeta(grid,               rep(aN, each = resolution + 1), rep(bN, each = resolution + 1))   minimizer <- apply(diff(qs, lag = dist), 2, which.min)   # minimizer <- max.col(-t(diff(qs, lag = dist))) # chooses at random in case of ties   selector <- minimizer + (seq_along(minimizer) - 1) * (resolution + 1)   leftHPD <- qs[selector]   rightHPD <- qs[selector + dist]   inHPD[, i] <- leftHPD <= set$thetatrue[i] & set$thetatrue[i] <= rightHPD   lenHPD[, i] <- rightHPD - leftHPD      # equal-tailed intervals   leftequal <- qbeta(alpha / 2, aN, bN)   rightequal <- qbeta(1 - alpha / 2, aN, bN)   inequal[, i] <- leftequal <= set$thetatrue[i] & set$thetatrue[i] <= rightequal   lenequal[, i] <- rightequal - leftequal }  res <- cbind(res[,1:2],              asy_cover = colMeans(inconf),              HPD_cover = colMeans(inHPD),              equal_cover = colMeans(inequal),              asy_len = colMeans(lenconf),              HPD_len = colMeans(lenHPD),              equal_len = colMeans(lenequal)) knitr::kable(round(res, 2))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"example-5-11-coverage-probabilities-under-the-poisson-gamma-model","dir":"Articles","previous_headings":"Section 5.4","what":"Example 5.11: Coverage probabilities under the Poisson-Gamma model","title":"Chapter 5: Learning More About Bayesian Learning","text":", data ùí´(5)\\mathcal{P}(5)-distribution. estimate mean N=24N = 24 data points construct CIs. Recall posterior flat prior Œº|ùê≤‚àºùí¢(Ny‚Äæ+1,N), \\mu|\\mathbf{y} \\sim \\mathcal G(N\\bar y + 1, N), start simulating data computing sample means sample variances. Next, compute asymptotic intervals based sample means sample variances. Bayesian variants, compute equal-tailed HPD intervals. Finally, compile print results.","code":"mutrue <- 5 N <- 24 dat <- matrix(rpois(N * nrep, mutrue), nrep, N) means <- rowMeans(dat) vars <- apply(dat, 1, var) leftconf1 <- means - qnorm(1 - alpha / 2) * sqrt(means / N) rightconf1 <- means + qnorm(1 - alpha / 2) * sqrt(means / N) inconf1 <- leftconf1 <= mutrue & mutrue <= rightconf1  leftconf2 <- means - qnorm(1 - alpha / 2) * sqrt(vars / N) rightconf2 <- means + qnorm(1 - alpha / 2) * sqrt(vars / N) inconf2 <- leftconf2 <= mutrue & mutrue <= rightconf2 leftequal <- qgamma(alpha / 2, N * means + 1, N) rightequal <- qgamma(1 - alpha / 2, N * means + 1, N) inequal <- leftequal <= mutrue & mutrue <= rightequal  qs <- qgamma(grid, rep(N * means + 1, each = resolution + 1), N) minimizer <- apply(diff(qs, lag = dist), 2, which.min) selector <- minimizer + (seq_along(minimizer) - 1) * (resolution + 1) leftHPD <- qs[selector] rightHPD <- qs[selector + dist] inHPD <- leftHPD <= mutrue & mutrue <= rightHPD res <- cbind(asy_mean_coverage = mean(inconf1),              asy_var_coverage = mean(inconf2),              equal_coverage = mean(inequal),              HPD_coverage = mean(inHPD)) knitr::kable(t(round(res, 2)))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"example-5-12-uncertainty-quantification-for-various-count-data","dir":"Articles","previous_headings":"Section 5.4","what":"Example 5.12: Uncertainty quantification for various count data","title":"Chapter 5: Learning More About Bayesian Learning","text":"revisit accidents eye tracking data sets compute means variances. asymptotic Bayesian intervals can computed .","code":"data(\"accidents\", package = \"BayesianLearningCode\") data(\"eyetracking\", package = \"BayesianLearningCode\") y1 <- accidents[, c(\"seniors_accidents\", \"children_accidents\")] y2 <- eyetracking$anomalies N <- c(rep(nrow(y1), ncol(y1)), length(y2)) means <- c(colSums(y1), eyetracking_anomalies = sum(y2)) / N vars <- c(apply(y1, 2, var), var(y2)) # asymptotic intervals based on sample means lconf1 <- means - qnorm(1 - alpha / 2) * sqrt(means / N) rconf1 <- means + qnorm(1 - alpha / 2) * sqrt(means / N)  # asymptotic intervals based on sample means and variances lconf2 <- means - qnorm(1 - alpha / 2) * sqrt(vars / N) rconf2 <- means + qnorm(1 - alpha / 2) * sqrt(vars / N)  # equal-tailed Bayesian intervals under a flat prior lequal <- qgamma(alpha / 2, N * means + 1, N) requal <- qgamma(1 - alpha / 2, N * means + 1, N)  # HPD intervals under a flat prior qs <- qgamma(grid[, seq_along(means)],              rep(N * means + 1, each = resolution + 1),              rep(N, each = resolution + 1)) minimizer <- apply(diff(qs, lag = dist), 2, which.min) selector <- minimizer + (seq_along(minimizer) - 1) * (resolution + 1) lHPD <- qs[selector] rHPD <- qs[selector + dist]  res <- cbind(lconf1, rconf1, lconf2, rconf2, lequal, requal, lHPD, rHPD) knitr::kable(round(res, 3))"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter05.html","id":"example-5-13-figure-5-7-prior-non-invariance-illustration","dir":"Articles","previous_headings":"Section 5.5: The Role of the Prior for Bayesian Learning","what":"Example 5.13 / Figure 5.7: Prior (non-)invariance illustration","title":"Chapter 5: Learning More About Bayesian Learning","text":"First, define function dbetamod, modified version native dbeta return improper kernel parameters 0 (resort original dbeta otherwise). Next, define density function Œ∑=logit(œë)=log(œë1‚àíœë) \\eta = logit(\\vartheta) = \\log\\left(\\frac{\\vartheta}{1-\\vartheta}\\right)  using law transformation densities. Now can plot.","code":"dbetamod <- function(x, a, b) {   if (a == 0 && b == 0) {     x^-1 * (1 - x)^-1   } else {     dbeta(x, a, b)   } } deta <- function(eta, a, b)   dbetamod(exp(eta) / (1 + exp(eta)), a, b) * exp(eta) / (1 + exp(eta))^2 theta <- seq(0, 1, length.out = 200) eta <- seq(-10, 10, length.out = 200) plot(theta, dbetamod(theta, 1, 1), type = \"l\", ylab = \"\",      xlab = expression(vartheta), main = \"B(1, 1)\") plot(theta, dbetamod(theta, 0.5, 0.5), type = \"l\", ylab = \"\",      xlab = expression(vartheta), main = \"B(0.5, 0.5)\") plot(theta, dbetamod(theta, 0, 0), type = \"l\", ylab = \"\",      xlab = expression(vartheta), main = \"B(0, 0)\") plot(eta, deta(eta, 1, 1), type = \"l\", ylab = \"\",      xlab = expression(eta)) plot(eta, deta(eta, 0.5, 0.5), type = \"l\", ylab = \"\",      xlab = expression(eta)) plot(eta, round(deta(eta, 0, 0), 10), type = \"l\", ylab = \"\",      xlab = expression(eta))"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-1-movie-data","dir":"Articles","previous_headings":"Section 6.1 The Standard Linear Regression Model","what":"Example 6.1: Movie data","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"use movie data provided within package illustrate Bayesian analysis regression model. data set preprocessed version one provided Lehrer Xi (2017).","code":"library(\"BayesianLearningCode\") data(\"movies\", package = \"BayesianLearningCode\")"},{"path":[]},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-2-movie-data-analysis-under-improper-prior","dir":"Articles","previous_headings":"Section 6.2 Bayesian Learning for a Standard Linear Regression Model > Section 6.2.1 Bayesian Learning Under Improper Priors","what":"Example 6.2: Movie data: Analysis under improper prior","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"use response y variable OpenBoxOffice, contains box office sales opening weekend Mio.$, covariates budget (Budget, Mio.$) number screens (Screens, 1000) film forecast theaters six weeks prior opening. center covariates Budget Screens means data set. Thus, intercept represents expected box office sales opening weekend (Mio.$) film average budget forecast screened average number screens theaters. define function compute parameters posterior distribution regression effects improper prior p(Œ≤,œÉ2)‚àù1œÉ2p(\\beta, \\sigma^2) \\propto \\frac{1}{\\sigma^2}. following table reports posterior means together equal-tailed 95% credible intervals. Based posterior means, expected box office sales opening weekend equal Mio.$ 19.11 film average Budget forecast shown average number Screens. film expected box office sales opening weekend Mio.$ 0.174 higher budget Mio.$ 1 higher; film average budget shown 1000 screens expected box office sales Mio.$ 1.719 higher. show uncertainty estimates, plot (univariate) marginal posterior distribution intercept well univariate bivariate marginal posterior distributions covariate effects.  completeness finally report also posterior mean error variance equal-tailed 95% credible interval.","code":"y <- movies[, \"OpenBoxOffice\"] covs <- c(\"Budget\", \"Screens\") covs.cen <- scale(movies[, covs], scale = FALSE) # center the covariates  N <- length(y)  # number of observations  X <- as.matrix(cbind(\"Intercept\" = rep(1, N), covs.cen)) # regressor matrix d <- dim(X)[2] # number of regression effects regression_improper <- function(y, X) {       BN <- solve(crossprod(X))    Xy <- crossprod(X, y)    beta.hat <- BN %*% Xy     SSR <- as.numeric(crossprod(y - X %*% beta.hat))     cN <- (N - d) / 2    CN <- SSR / 2     post.var <- (CN / cN) * BN    list(beta.hat = beta.hat, BN = BN, cN = cN, CN = CN, post.var = post.var) } reg.improp <- regression_improper(y, X)  beta.hat <- reg.improp$beta.hat post.sd <- sqrt(diag(reg.improp$post.var)) cN <- reg.improp$cN  knitr::kable(round(cbind(qt(0.025, df = 2 * cN) * post.sd + beta.hat,                          beta.hat,                          qt(0.975, df = 2 * cN) * post.sd + beta.hat), 3),              col.names = c(\"2.5% quantile\", \"posterior mean\", \"97.5% quantile\")) lim <- round(3*post.sd, 2) layout(matrix(c(0, 1, 3, 2, 0, 4), nrow = 2), widths = c(2, 3, 1),        heights = c(1, 3), respect = TRUE) par(mar = c(3, 2, 1, 1)) curve(dt((x - beta.hat[1]) / post.sd[1], df = 2 * cN),       from = beta.hat[1] - lim[1], to = beta.hat[1] + lim[1],       ylab = \"\", xlab = \"\", main = \"\") mtext(\"Intercept\", 1, line = 1.7)  f <- function(x1, x2) {    mvtnorm::dmvt(cbind(x1 - beta.hat[2], x2 - beta.hat[3]),                  sigma = reg.improp$post.var[2:3, 2:3],                  df = 2 * reg.improp$cN, log = FALSE) }  xx1 <- seq(-lim[2], lim[2], length = 201) + beta.hat[2] xx2 <- seq(-lim[3], lim[3], length = 201) + beta.hat[3] z <- outer(xx1, xx2, f)  par(mar = c(3, 3, 1, 1)) contour(xx1, xx2, z, add = FALSE) mtext(rownames(beta.hat)[2], 1, line = 1.7) mtext(rownames(beta.hat)[3], 2, line = 1.7)  par(mar = c(0, 3, 1, 1)) mar.x1 <- dt((xx1 - beta.hat[2]) / post.sd[2], df = 2 * reg.improp$cN,              log = FALSE) plot(xx1, mar.x1,  type = \"l\", xaxt = \"n\", ylab = \"\")  par(mar = c(3, 0, 1, 1)) mar.x2 <- dt((xx2 - beta.hat[3]) / post.sd[3], df = 2 * reg.improp$cN,              log = FALSE) plot(mar.x2, xx2,  type = \"l\", yaxt = \"n\", xlab = \"\") sigma2.hat <- reg.improp$CN /(cN-1) knitr::kable(round(cbind(qinvgamma(0.025, a = cN, b = reg.improp$CN),                          sigma2.hat,                          qinvgamma(0.975, a = cN, b = reg.improp$CN)), 2),              col.names = c(\"2.5% quantile\", \"posterior mean\", \"97.5% quantile\"))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-3-movie-data-prediction","dir":"Articles","previous_headings":"Section 6.2 Bayesian Learning for a Standard Linear Regression Model > Section 6.2.1 Bayesian Learning Under Improper Priors","what":"Example 6.3: Movie data: Prediction","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"now interested predicting box office sales opening weekend. compute predicted box office sales film average number Screens range values Budget. following plot shows point predictions together 50% 80% prediction intervals.  prediction interval symmetric around posterior mean, obviously model adequate, lower limit 80% prediction interval negative budget approximately Mio.$ 55. can take account box office sales always positive fitting linear regression model log transformed sales. estimation results determine point predictions prediction intervals logarithm box office sales exponentiate results obtain predictions sales. following figure shows point prediction 50% 80% prediction intervals box office sales film shown average number Screens range values Budget derived model.  see bounds prediction intervals positive, ‚Äì due exponential transformation ‚Äì prediction intervals longer symmetric around point predictions. Next, compare point predictions models observed box office sales. see predicted values regression model logarithm box office sales slightly better, observed box office sales Mio.$ 152 still predicted much low.","code":"X_new <- cbind(Intercept = 1,                Budget = -30:60,                Screens = 0) ypred <- X_new %*% beta.hat  ypred.var <- sigma2.hat * (rowSums((X_new %*% reg.improp$BN) * X_new) + 1) budget <- X_new[, \"Budget\"] + mean(movies[, \"Budget\"]) plot(budget, ypred, type = \"l\", lwd = 2, ylim = c(-20, 60),      xlab = \"Budget\", ylab = \"Predicted box office sales\")  pred.levels <- c(0.5, 0.8) for (j in seq_along(pred.levels)) {    pred.quantile <- qt(1 - (1-pred.levels[j])/2, df = 2 * cN)    lines(budget, ypred - sqrt(ypred.var)*pred.quantile, lty = j+1)    lines(budget, ypred + sqrt(ypred.var)*pred.quantile, lty = j+1) } legend(\"bottomright\",        c(\"Predicted mean\",          paste0(pred.levels*100, \"% Prediction interval\")),        lty = 1:3, lwd = c(2, 1, 1)) y.pred <- X %*% beta.hat plot(y, y.pred, xlim = c(-20, 160), ylim = c(-20, 160),       xlab = \"observed sales\", ylab = \"predicted sales\") abline(a = 0, b = 1) abline(h = 0, lty = 2) log.y <- log(movies[, \"OpenBoxOffice\"]) reg.lny <- regression_improper(log.y, X)  lny.pred <- X_new %*% reg.lny$beta.hat  sigma2.hat <- reg.lny$CN /(reg.lny$cN-1) lny.pred.var <- sigma2.hat * (rowSums((X_new %*% reg.lny$BN) * X_new) + 1) plot(budget, exp(lny.pred), type = \"l\", ylim = c(-20, 60), col = \"blue\",      xlab = \"Budget\", ylab = \"Predicted box office sales\", lwd = 2)  for (j in seq_along(pred.levels)) {    pred.quantile <- qt(1 - (1-pred.levels[j])/2, df = 2 * reg.lny$cN)    lines(budget, exp(lny.pred - sqrt(lny.pred.var)*pred.quantile),           col = \"blue\", lty = j+1)    lines(budget, exp(lny.pred + sqrt(lny.pred.var)*pred.quantile),           col = \"blue\", lty = j+1) } legend(\"bottomright\",        c(\"Predicted mean\",          paste0(pred.levels*100, \"% Prediction interval\")),        col = \"blue\", lty = 1:3, lwd = c(2, 1, 1)) plot(y, y.pred,xlim = c(-20, 160), ylim = c(-20, 160),      xlab = \"Observed sales\", ylab = \"Predicted sales\") points(y, exp(X %*% reg.lny$beta.hat), col = \"blue\", pch = 16) abline(a = 0, b = 1) abline(h = 0, lty = 2) legend(\"bottomright\",        c(\"Linear regression on y\", \"Linear regression on ln(y)\"),        col = c(\"black\", \"blue\"), lty = 1)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"section-6-2-2-bayesian-learning-under-conjugate-priors","dir":"Articles","previous_headings":"Section 6.2 Bayesian Learning for a Standard Linear Regression Model","what":"Section 6.2.2 Bayesian Learning under Conjugate Priors","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"next consider regression analysis conjugate prior. , first define function yields parameters posterior distribution.","code":"regression_conjugate <- function(y, X, b0 = 0, B0 = 10, c0 = 0.01, C0 = 0.01) {   d <- ncol(X)   if (length(b0) == 1L)     b0 <- rep(b0, d)      if (!is.matrix(B0)) {     if (length(B0) == 1L) {       B0 <- diag(rep(B0, d))     } else {       B0 <- diag(B0)     }   }      B0.inv <- solve(B0)   BN.inv <- B0.inv + crossprod(X)      BN <- solve(BN.inv)   bN <- BN %*% (B0.inv %*% b0 + crossprod(X, y))      cN <- c0 + N / 2   SS.eps <- as.numeric(crossprod(y) + t(b0) %*% B0.inv %*% b0 -                          t(bN) %*% BN.inv %*% bN)   CN <- C0 + SS.eps / 2   list(bN = bN, BN = BN, cN = cN, CN = CN) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-4-movie-data---analysis-under-conjugate-prior","dir":"Articles","previous_headings":"Section 6.2 Bayesian Learning for a Standard Linear Regression Model > Section 6.2.2 Bayesian Learning under Conjugate Priors","what":"Example 6.4: Movie data - Analysis under conjugate prior","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"specify normal prior mean zero ùêÅ0=Œª2ùêà\\mathbf{B}_0=\\lambda^2 \\mathbf{} Œª2=10\\lambda ^2=10 regression effects inverse Gamma prior c0=2.5c_0=2.5 C0=1.5C_0=1.5 œÉ2\\sigma^2. choice prior parameters c0c_0 C0C_0 prior rather uninformative guarantees existence posterior variance. perform regression analysis box office sales yy report posterior mean regression effects together 2.5% 97.5% quantiles posterior distribution following table. illustrate effects tighter prior regression effects, also compute posterior distributions Œª2=1\\lambda^2=1 Œª2=0.1\\lambda^2=0.1. plot marginal posteriors together improper prior. little difference improper prior effects Budget Screens, however intercept shrunk zero Œª2=1\\lambda^2=1 even Œª2=0.1\\lambda^2=0.1. illustrate effect prior compute weight matrix ùêñ\\textbf{W} conjugate prior mean ùüé\\textbf{0} covariance matrix Œª2ùêà\\lambda^2 \\textbf{} Œª2=0.1\\lambda^2=0.1. Obviously weight prior mean much larger intercept effects Budget Screens. Together high value posterior mean intercept explains considerable shrinkage zero intercept, whereas effects two covariates almost unshrunk.","code":"res_conj1 <- regression_conjugate(y, X, b0 = 0, B0 = 10, c0 = 2.5, C0 = 1.5) post.sd.conj1 <- sqrt(diag((res_conj1$CN / res_conj1$cN) * res_conj1$BN))  knitr::kable(round(cbind(    qt(0.025, df = 2 * res_conj1$cN) * post.sd.conj1 + res_conj1$bN,    res_conj1$bN,    qt(0.975, df = 2 * res_conj1$cN) * post.sd.conj1 + res_conj1$bN), 3),     col.names = c(\"2.5 quantile\", \"posterior mean\", \"97.5 quantile\")) res_conj2 <- regression_conjugate(y, X, b0 = 0, B0 = 1, c0 = 2.5, C0 = 1.5) post.sd.conj2 <- sqrt(diag((res_conj2$CN / res_conj2$cN) * res_conj2$BN))  res_conj3 <- regression_conjugate(y, X, b0 = 0, B0 = 0.1, c0 = 2.5, C0 = 1.5) post.sd.conj3 <- sqrt(diag((res_conj3$CN / res_conj3$cN) * res_conj3$BN)) par(mfrow = c(1, 3)) for (i in seq_len(nrow(beta.hat))) {    curve(dt((x - beta.hat[i]) / post.sd[i], df = 2 * cN),          from = beta.hat[i] - 4 * post.sd[i],           to  =  beta.hat[i] + 4 * post.sd[i],          ylab = \"\", xlab = \"\" , main = rownames(beta.hat)[i])    curve(dt((x - res_conj1$bN[i]) / post.sd.conj1[i],              df = 2 * res_conj1$cN),          from = res_conj1$bN[i] - 4 * post.sd.conj1[i],          to  =  res_conj1$bN[i] + 4 * post.sd.conj1[i],          add = TRUE, col = 2, lty = 2, lwd = 2)    curve(dt((x - res_conj2$bN[i]) / post.sd.conj2[i],              df = 2 * res_conj2$cN),          from = res_conj2$bN[i] - 4 * post.sd.conj2[i],          to  =  res_conj2$bN[i] + 4 * post.sd.conj2[i],           add = TRUE, col = 3,lty = 3, lwd = 2)    curve(dt((x - res_conj3$bN[i]) / post.sd.conj3[i],              df = 2 * res_conj3$cN),          from = res_conj3$bN[i] - 4 * post.sd.conj3[i],          to  =  res_conj3$bN[i] + 4 * post.sd.conj3[i],          add = TRUE, col = 4,lty = 4, lwd = 2)    legend(\"topright\",           c(\"improper\",              expression(paste(lambda^2, \"=\", 10)),              expression(paste(lambda^2, \"=\", 1)),             expression(paste(lambda^2, \"=\", 0.1))),           col = 1:4, lty = 1:4, lwd = c(1, 2, 2, 2)) } W <- res_conj3$BN %*% solve(diag(rep(0.1, d))) round(W, 5) #>              [,1]     [,2]     [,3] #> Intercept 0.09615  0.00000  0.00000 #> Budget    0.00000  0.00029 -0.00040 #> Screens   0.00000 -0.00040  0.00451"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"regression-analysis-under-the-semi-conjugate-prior","dir":"Articles","previous_headings":"Section 6.2 Bayesian Learning for a Standard Linear Regression Model","what":"6.3 Regression Analysis under the Semi-Conjugate Prior","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"First, set Gibbs sampler estimate parameters regression model semi-conjugate prior.","code":"reg_semiconj <- function(y, X, b0 = 0, B0 = 10000, c0 = 2.5, C0 = 1.5,                          burnin = 1000L, M = M, start.sigma2) {    d <- dim(X)[2]     B0inv <- diag(rep(1 / B0, d), nrow = d)    b0 <-  rep(b0, length.out = d)     # define quantities for the Gibbs sampler    XX <- crossprod(X)    Xy <- t(X) %*% y    cN <- c0 + N / 2         # prepare storing of results    betas <- matrix(NA_real_, nrow = M, ncol = d)    colnames(betas) <- colnames(X)     sigma2s <- rep(NA_real_,  M)     # starting value for sigma2    if (missing(start.sigma2)) {        start.sigma2 <- var(y) / 2    }    sigma2 <- start.sigma2        for (m in 1:(burnin + M)) {        # sample beta from the full conditional        BN <- solve(B0inv + XX / sigma2)         bN <- BN %*% (B0inv %*% b0 + Xy / sigma2)        beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))             # sample sigma^2 from its full conditional        eps <- y - X %*% beta        CN <- C0 + crossprod(eps) / 2        sigma2 <- rinvgamma(1, cN, CN)               if (m > burnin) {           betas[m - burnin, ] <- beta           sigma2s[m - burnin] <- sigma2        }    }    return(post.draws = list(betas = betas, sigma2s = sigma2s)) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-5-movie-data---traceplots-of-the-gibbs-sampler","dir":"Articles","previous_headings":"Section 6.2 Bayesian Learning for a Standard Linear Regression Model > 6.3 Regression Analysis under the Semi-Conjugate Prior","what":"Example 6.5: Movie data - Traceplots of the Gibbs sampler","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"run sampler two times 1000 draws show convergence posterior distribution start extreme values innovation variance, large small value. trace plots see sampler converges quickly. Even though starting value error variance far posterior distribution burn-phase sampler short.","code":"set.seed(421) M <- 1000L # number of draws after burn-in post.draws1 <- reg_semiconj(y, X, b0 = 0, B0 = 10000, c0 = 2.5, C0 = 1.5,                            burnin = 0L, M = M, start.sigma2 = 10^6) post.draws2 <- reg_semiconj(y, X, b0 = 0, B0 = 10000, c0 = 2.5, C0 = 1.5,                            burnin = 0L, M = M, start.sigma2 = 10^-6) for (i in seq_len(ncol(post.draws1$betas))) {    plot(post.draws1$betas[, i], type = \"l\", xlab = \"Draws\", ylab = \"\",         main = colnames(post.draws1$betas)[i])    lines(post.draws2$betas[, i], col=\"red\") } plot(post.draws1$sigma2s, type = \"l\", xlab = \"Draws\", ylab = \"\",      main = expression(paste(\"Error variance \", sigma^2))) lines(post.draws2$sigma2s, col=\"red\")"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-6-movie-data-analysis-under-the-semi-conjugate-prior","dir":"Articles","previous_headings":"Section 6.2 Bayesian Learning for a Standard Linear Regression Model > 6.3 Regression Analysis under the Semi-Conjugate Prior","what":"Example 6.6: Movie data: Analysis under the semi-conjugate prior","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"now include available covariates regression analysis. one film MPAA rating ‚ÄúG‚Äù, merge two ratings ‚ÄúG‚Äù ‚ÄúPG‚Äù one category define baseline. center covariates zero define regressor matrix. Next, define prior parameters run sampler. summarize results nicely, compute equal-tailed 95% credible intervals regression effects. error variances. Obviously, taking account covariates posterior mean error variance considerably lower model Budget Screens used covariates. different signs effects Vol-4-6 Vol-1-3 deserve comment. two covariates highly correlated. Due high correlation usual interpretation effect changing value one covariate make sense. Hence, predict change box office sales film twitter volume scores 1 unit higher weeks 4-6 well weeks 1-3.","code":"movies[\"PG\"] <- NULL covs <- c(\"Comedy\", \"Thriller\", \"PG13\", \"R\", \"Budget\", \"Weeks\", \"Screens\",            \"S-4-6\", \"S-1-3\", \"Vol-4-6\", \"Vol-1-3\") covs.cen <- scale(movies[, covs], scale = FALSE)  N <- length(y)  # number of observations X <- cbind(\"Intercept\" = rep(1, N), covs.cen) # regressor matrix  d <- dim(X)[2] # number regression effects  p <- d - 1 # number of regression effects without intercept set.seed(421) M <- 20000L # number of draws after burn-in post.draws <- reg_semiconj(y, X, b0 = 0, B0 = 10000, c0 = 2.5, C0 = 1.5,                            burnin = 1000L, M = M) res.mcmc <- function(x, lower = 0.025, upper = 0.975){   res<-c(quantile(x, lower), mean(x), quantile(x, upper))   names(res) <- c(paste0(lower * 100, \"%\"), \"Posterior mean\",                    paste0(upper *   100, \"%\"))   res }  beta.sc <- post.draws$betas res_beta.sc <- t(apply(beta.sc, 2, res.mcmc)) rownames(res_beta.sc) <- c(\"Intercept\", covs)  knitr::kable(round(res_beta.sc, 3)) sigma2.sc <- post.draws$sigma2s res_sigma2.sc <- res.mcmc(sigma2.sc)  knitr::kable(t(round(res_sigma2.sc, 3))) cor(X[, \"Vol-4-6\"], X[, \"Vol-1-3\"]) #> [1] 0.84403  par(mfrow = c(1, 1)) plot(X[, \"Vol-4-6\"], X[, \"Vol-1-3\"],xlab=\"Vol-4-6\", ylab=\"Vol-1-3\") round(sum(res_beta.sc[c(\"Vol-4-6\", \"Vol-1-3\"), \"Posterior mean\"]),        digits = 3) #> [1] 5.629"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"regression-analysis-based-on-the-horseshoe-prior","dir":"Articles","previous_headings":"","what":"6.4 Regression Analysis Based on the Horseshoe Prior","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"comparison normal horseshoe prior shows latter much mass close zero fatter tails.  set Gibbs sampler regression model proper normal prior intercept horseshoe priors covariate effects.","code":"beta <- seq(from = -4, to = 4, by = 0.01)  # Horseshoe prior # Approximated by the result of Theorem 1 in Carvalho and Polson (2010) c <-  1 / sqrt(2 * pi^3) l <- c / 2 * log(1 + 4 / beta^2) u <- c * log(1 + 2 / beta^2) par(mfrow = c(1, 1)) plot(beta, (u + l) / 2, type = \"l\", ylim = c(0, 0.55), xlab = expression(beta),      ylab = \"\", lty = 1, col = \"blue\") lines(beta, dnorm(beta), lty = 2) # Standard normal prior legend(\"topright\", legend = c(\"Horseshoe\", \"Standard normal\"),        lty = 1:2, col = c(\"blue\", \"black\")) reg_hs <- function(y, X,  b0 = 0, B0 = 10000, c0 = 2.5, C0 = 1.5,                    burnin = 1000L, M) {    d <- dim(X)[2]    p <- d - 1        B00inv <- 1 / B0 # prior precision for the intercept    b0 <-  rep(b0, length.out = d)     # prepare storing of results    betas <- matrix(NA_real_, nrow = M, ncol = d)    colnames(betas) <- colnames(X)        sigma2s<- rep(NA_real_, M)    tau2s <- matrix(NA_real_, nrow =  M, ncol = p)    lambda2s <- rep(NA_real_,  M)        # define quantities for the Gibbs sampler    XX <- crossprod(X)    Xy <- crossprod(X, y)    cN=c0+N/2     # set starting values     sigma2 <- var(y) / 2    tau2 <- rep(1, p)    lambda2 <- 1     for (m in seq_len(burnin + M)) {       # sample  beta from the full conditional       B0inv <- diag(c(B00inv, 1 / (lambda2 * tau2)))       BN <- solve(B0inv + XX / sigma2)        bN <- BN %*% (B0inv %*% b0 + Xy / sigma2)          beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))       beta.star <- beta[2:d]          # sample sigma^2 from its full conditional       eps <- y - X %*% beta       CN <- C0 + crossprod(eps) / 2       sigma2 <- rinvgamma(1, cN, CN)          # sample tau^2       xi  <- rexp(p, rate = 1 + 1 / tau2)       tau2 <- rinvgamma(p, 1, xi + 0.5 * beta.star^2 / lambda2)          # sample lambda^2       zeta <- rexp(1, rate = 1 + 1 / lambda2)       lambda2 <- rinvgamma(1, (p + 1) / 2, zeta + 0.5 * sum(beta.star^2 / tau2))          # store results       if (m > burnin) {          betas[m - burnin, ] <- beta          sigma2s[m - burnin] <- sigma2          tau2s[m - burnin,] <- tau2          lambda2s[m - burnin] <- lambda2       }    }    list(betas = betas, sigma2s = sigma2s, tau2s = tau2s, lambda2s = lambda2s) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-7-movie-data---analysis-under-the-horseshoe-prior","dir":"Articles","previous_headings":"6.4 Regression Analysis Based on the Horseshoe Prior","what":"Example 6.7: Movie data - Analysis under the Horseshoe prior","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"estimate parameters regression model prior intercept error variance semi-conjugate prior, horseshoe prior covariate effects. , show posterior mean estimates regression effects together equal-tailed 95% credible intervals table. Estimation results similar semi-conjugate prior effects Budget, Weeks, Screens, Vol-4-6 Vol-1-3. covariates posterior means effects closer zero 95% posterior intervals tighter horseshoe semi-conjugate prior, indicating shrinkage zero. However, estimation results error variance similar semi-conjugate prior. next look posterior distributions. plots left hand side show posterior distribution regression effects semi-conjugate prior, right hand side posterior distributions horseshoe prior. Whereas posterior distributions symmetric semi-conjugate prior, case horseshoe prior.  illustration purposes, overlay four selected marginal posteriors order illustrate shrinkage effect.  Next, investigate trace plots draws posterior. , plots left obtained semi-conjugate prior, right horseshoe prior.  sum , visualize posterior effects corresponding (square root ) shrinkage parameters. visual inspection, create gap plot, remove largest 5% local shrinkage parameter draws mirror around 0. way, can easily identify ‚Äúsignificant‚Äù effects via clear bimodality even gap around zero ‚Äì hence name. left, see posteriors regression effects posteriors, right, visualize gap plot.","code":"set.seed(421) post.draws.hs <- reg_hs(y, X, M = M) beta.hs <- post.draws.hs$betas res_beta.hs <- t(apply(beta.hs, 2, res.mcmc)) rownames(res_beta.hs) <- colnames(X)  knitr::kable(round(res_beta.hs, 3)) sigma2.hs <- post.draws.hs$sigma2s res_sigma2.hs <- res.mcmc(sigma2.hs) names(res_sigma2.hs) <- colnames(res_beta.hs)   knitr::kable(t(round(res_sigma2.hs, 3))) for (i in seq_len(d)) {   br <- seq(min(beta.sc[, i], beta.hs[, i]), max(beta.sc[, i], beta.hs[, i]),             length.out = 100)   hist(beta.sc[, i], main = colnames(X)[i], breaks = br, xlab = \"\", ylab = \"\")   hist(beta.hs[, i], main = colnames(X)[i], breaks = br, xlab = \"\", ylab = \"\") } par(mfrow = c(2, 2)) selection <- c(\"Screens\", \"Weeks\", \"S-1-3\", \"Thriller\") for (i in selection) {    breaks <- seq(min(beta.sc[, i], beta.hs[, i]), max(beta.sc[, i], beta.hs[, i]),                  length.out = 100)    h1 <- hist(beta.sc[, i], breaks = breaks, plot = FALSE)    h2 <- hist(beta.hs[, i], breaks = breaks, plot = FALSE)    col <- c(rgb(0, 0, 1, 0.25), rgb(1, 0, 0, 0.25))    plot(h1, main = i, xlab = \"\", ylab = \"\", freq = FALSE,         ylim = c(0, max(h1$density, h2$density)), col = col[1])    plot(h2, xlab = \"\", ylab = \"\", freq = FALSE, col = col[2],          add = TRUE) } par(mfrow = c(6, 2)) for (i in seq_len(d)) {   plot(beta.sc[, i], type = \"l\", xlab = \"\", ylab = \"\", main = colnames(beta.sc)[i])   plot(beta.hs[, i], type = \"l\", xlab = \"\", ylab = \"\", main = colnames(beta.sc)[i]) } tau2.hs <- post.draws.hs$tau2s alpha <- 0.05 truncate <- function(x, alpha) x[x <= quantile(x, 1 - alpha)]  tau2.hs.trunc <- apply(tau2.hs, 2, truncate, alpha = alpha) tau.hs.trunc.mirrored <- rbind(sqrt(tau2.hs.trunc),                                -sqrt(tau2.hs.trunc)) par(mfrow = c(12, 2)) for (i in seq_len(ncol(beta.hs))) {   breaks <- seq(min(beta.hs[, i]), max(beta.hs[, i]), length.out = 100)   hist(beta.hs[, i], breaks = breaks, xlab = \"\", ylab = \"\",         main = c(\"Intercept\", covs)[i])   if (i == 1) {     plot.new()   } else {     breaks <- seq(min(tau.hs.trunc.mirrored[, i - 1]),                       max(tau.hs.trunc.mirrored[, i - 1]), length.out = 100)     hist(tau.hs.trunc.mirrored[, i - 1], breaks = breaks, xlab = \"\", ylab = \"\",          main = covs[i - 1])   } }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-8-movie-data--check-convergence-by-a-second-mcmc-run","dir":"Articles","previous_headings":"6.4 Regression Analysis Based on the Horseshoe Prior","what":"Example 6.8: Movie data- Check convergence by a second MCMC run","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"verify convergence sampler second run six block sampler Algorithm 6.2. QQ plots draws intercept error variance, draws close identity line hence can conclude sampler converged.","code":"post.draws.hs2 <- reg_hs(y, X, M = M)  par(mfrow = c(1, 2),mar = c(2.0, 2.0, 2.0, .1), mgp = c(1, .2, 0)) qqplot(post.draws.hs$betas[, 1], post.draws.hs2$betas[, 1],         xlim = range(post.draws.hs$betas[, 1], post.draws.hs2$betas[, 1]),        ylim = range(post.draws.hs$betas[, 1], post.draws.hs2$betas[, 1]),        main = \"QQ plot for the intercept\", xlab = \"First run\", ylab = \"Second run\" ) abline(a = 0, b = 1) qqplot(post.draws.hs$sigma2s, post.draws.hs2$sigma2s,        xlim = range(post.draws.hs$sigma2s, post.draws.hs2$sigma2s),        ylim = range(post.draws.hs$sigma2s, post.draws.hs2$sigma2s),        main = \"QQ plot for the error variance\",xlab = \"First run\", ylab = \"Second run\" ) abline(a = 0, b = 1)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-9-movie-data---predictions","dir":"Articles","previous_headings":"6.4 Regression Analysis Based on the Horseshoe Prior","what":"Example 6.9: Movie data - Predictions","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"predict box office sales different movies: film baseline values covariates (), film baseline values covariates except genre Comedy (B) Thriller (C) finally film genre Thriller MPAA rating PG13 (D). plot predicted expectation, median predictive distribution, together vertical bars indicating point-wise equal-tailed 95% predictive interval.","code":"nf <- 4 X_new <- cbind(rep(1, nf), matrix(0, nrow = nf, ncol = p)) colnames(X_new) <- colnames(X)  X_new[2, \"Comedy\"] <- 1 X_new[3:4, \"Thriller\"] <- 1 X_new[3, \"PG13\"] <- 1 X_new[4, \"R\"] <- 1 X_new[, \"Budget\"] <- 10  ypred.sc <- X_new %*% t(beta.sc) +       rep(rnorm(length(sigma2.sc), sd = sqrt(sigma2.sc)), each = nrow(X_new)) pred.int.sc <- apply(ypred.sc,1, quantile, probs = c(0.025, 0.5, 0.975)) pred.mean.sc <- rowMeans(ypred.sc)  ypred.hs <- X_new %*% t(beta.hs) +    rep(rnorm(length(sigma2.hs), sd = sqrt(sigma2.hs)), each = nrow(X_new))   pred.int.hs <- apply(ypred.hs,1, quantile, probs = c(0.025, 0.5, 0.975)) pred.mean.hs <- rowMeans(ypred.hs) matplot(x = t(matrix(1:nf, ncol = 3, nrow = nf)),         y = pred.int.sc, col = \"blue\", type = \"l\", pch = 16, lty = 1,         ylim = c(0, 40), xlim = c(0.5, nf+0.5),         xlab = \"Scenarios\", ylab = \"Predicted box office sales\", xaxt = \"n\") points(x = 1:nf, y = pred.int.sc[2, ], pch = 19, col = \"blue\", cex = 1.2) points(x = 1:nf, y = pred.mean.sc, pch = 16, col = \"red\")  matplot(x = t(matrix((1:nf)+0.2, ncol = 3, nrow = nf)),         y = pred.int.hs, col = \"blue\", type = \"l\", pch = 16, lty = 1, add = TRUE)   points(x = (1:nf)+0.2, y = pred.int.hs[2, ], pch = 19, col = \"blue\", cex = 1.2) points(x = (1:nf)+0.2, y = pred.mean.hs, pch = 16, col = \"red\")       axis(1, at = 1:nf, labels = c(\"A\", \"B\", \"C\", \"D\"))"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"figure-6-11","dir":"Articles","previous_headings":"Section 6.5: Shrinkage beyond the Horseshoe Prior","what":"Figure 6.11","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"next investigate different shrinkage priors plot marginal prior regression coefficient various choices hyperparameters.  shrinkage profiles priors visualized following plot.","code":"# Thanks to Peter Knaus for providing the code # Marginal densities for Triple Gamma, Horseshoe, Double Gamma, and LASSO  # Some functions that return the log-marginal densities ## Log-marginal and marginal for Triple Gamma logmarginal_TG <- function(x, a, c, kappa2) {   -0.5*log(4*pi/kappa2) - lbeta(a, c) + 0.5*(log(a) - log(c)) + lgamma(c + 0.5) +     log(gsl::hyperg_U(c + 0.5, -a + 1.5, kappa2*a*x^2/(4*c), give = FALSE, strict = TRUE)) } ## Log-marginal and marginal for Double Gamma logmarginal_DG <- function(x, a, kappa2) {   0.5*(a + 0.5)*log(a*kappa2)  - 0.5*log(pi) - (a-0.5)*log(2) - lgamma(a) + (a - 0.5)*log(abs(x)) +     log(besselK(sqrt(a*kappa2)*abs(x), a-0.5)) }  # Select color palette color <- RColorBrewer::brewer.pal(5, \"Dark2\")  # Create layout matrix for plots m <- matrix(c(1, 2, 3, 3), nrow = 2, ncol = 2, byrow = TRUE) layout(mat = m, heights = c(0.9, 0.1))  # Plots around the origin # Draw horseshoe marginal curve((logmarginal_TG(x, 0.5, 0.5, 2)),       from = c(-1, to = 1), n = 10000, col = color[1], lwd = 2, ylab = \"\",       xlab = \"\", cex = 3, ylim = c(-4, 4), main = \"\") # Draw double gamma marginal curve((logmarginal_DG(x, 0.1, 2)),       n = 10000, col = color[2], lwd = 2, add = TRUE) # Draw LASSO marginal curve((logmarginal_DG(x, 1, 2)),       n = 10000, col = color[3], lwd = 2, add = TRUE) # Draw triple gamma marginal curve((logmarginal_TG(x, 0.1, 0.1, 2)),       n = 10000, col = color[4], lwd = 2, add = TRUE) # Add labels to x and y axes title(ylab = (expression(log~p(beta))), line = 2, cex.lab = 1.2) title(xlab = expression(beta), line = 3, cex.lab = 1.2)  # Plots in the tails # Draw horseshoe marginal curve((logmarginal_TG(x, 0.5, 0.5, 2)),       from = 6, to = 11, n = 10000, col = color[1], lwd = 2, ylab = \"\",       xlab = \"\", cex = 3, ylim = c(-18,-5), main = \"\") # Draw double gamma marginal curve((logmarginal_DG(x, 0.1, 2)),       n = 10000, col = color[2], lwd = 2, add = TRUE) # Draw LASSO marginal curve((logmarginal_DG(x, 1, 2)),       n = 10000, col = color[3], lwd = 2, add = TRUE) # Draw triple gamma marginal curve((logmarginal_TG(x, 0.1, 0.1, 2)),       n = 10000, col = color[4], lwd = 2, add = TRUE) # Add labels to x and y axes title(ylab = (expression(log~p(beta))), line = 2, cex.lab = 1.2) title(xlab = expression(beta), line = 3, cex.lab = 1.2)  # Create legend at bottom of plot par(mar = c(0, 0, 0, 0)) plot(1, type = \"n\", axes = FALSE) legend(x = \"top\",  inset = 0,        legend = c(\"Normal Gamma Gamma\", \"Horseshoe\", \"Normal Gamma\", \"Lasso\"),        col = color[c(4, 1, 2, 3)],        lwd = 2, cex = 1, horiz = TRUE, xjust = 0.5) # Thanks again to Peter Knaus for providing the code # Some functions that return shrinkage profiles # Shrinkage profile for triple gamma dTPB <- function(x, a, c, phi) {   res <- lgamma(a + c) - lgamma(a) - lgamma(c) + c*log(phi) +       (c-1)*log(x) + (a-1)*log(1-x) - (a+c)*log(1 + (phi -1)*x)   exp(res) } # Shrinkage profile for double gamma dTPB_DG <- function(x, a, k2) {   res <- -lgamma(a) + a*log(a*k2/2) +       (a-1)*log(1-x) - (a+1)*log(x) - ((1-x)/x*a*k2/2)   exp(res) }  # Set up plotting area par(mar = c(4, 4, 0, 12.5)) # Choose color palette color <- RColorBrewer::brewer.pal(5, \"Dark2\") # Draw shrinkage profile of horsehoe curve((dTPB(x, 0.5, 0.5, 1)),       from = 0, to = 1, n = 1000, col = color[1], lwd = 2, ylab = \"\",       xlab = \"\", cex = 3, ylim = c(0, 4), main = \"\") # Draw shrinkage profile of double gamma curve((dTPB_DG(x, 0.1, 2)),       n = 1000, col = color[2], lwd = 2, add = TRUE) # Draw shrinkage profile of lasso curve((dTPB_DG(x, 1, 2)), n = 1000, col = color[3], lwd = 2, add = TRUE) # Draw shrinkage profile of triple gamma curve((dTPB(x, 0.1, 0.1, 1)),       n = 1000, col = color[4], lwd = 2, add = TRUE) # Add labels to x and y axes title(ylab = expression(p(kappa[j])), line = 2, cex.lab = 1.2) title(xlab = expression(kappa[j]), line = 2, cex.lab = 1.2) par(xpd = TRUE) legend(x = 1.05, y = 3,        legend = c(\"Normal Gamma Gamma\", \"Horseshoe\", \"Normal Gamma\", \"Lasso\"),        col = color[c(4, 1, 2, 3)], bty = \"n\",        lwd = 2, cex = 1, horiz = FALSE)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter06.html","id":"example-6-11-a-hierarchical-bayesian-lasso-prior","dir":"Articles","previous_headings":"Section 6.5: Shrinkage beyond the Horseshoe Prior","what":"Example 6.11: A hierarchical Bayesian lasso prior","title":"Chapter 6: The Bayesian Approach to Standard Regression Analysis","text":"","code":"beta2 <- beta1 <- seq(from = -2, to = 2, by = 0.01) f <- function(x1, x2) {    exp(log(a+1) + log(a) + a*log(a) - (a+2)*log(a+abs(x1)+abs(x2))) } par(mfrow = c(1, 3), mar = c(4.5, 4.5, .1, .1),      mgp = c(2.6, .6, 0)) for (a in c(0.1, 1, 10)) {    z <- outer(beta1, beta2, f)    contour(beta1, beta2, z, nlevels = 10,             xlab = expression(beta[1]), ylab = expression(beta[2]),            drawlabels = FALSE) }"},{"path":[]},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"figure-7-1-u-s--gdp-data","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Figure 7.1: U.S. GDP data","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"First, load data. extreme outliers COVID pandemic, restrict analysis time outbreak. Next, compute log returns. Now can plot data empirical autocorrelation function.  move , define function yielding posterior draws standard regression model, using tools developed Chapter 6. Now ready reproduce results book.","code":"data(\"gdp\", package = \"BayesianLearningCode\") dat <- gdp[1:which(names(gdp) == \"2019-10-01\")] logret <- log(dat[-1]) - log(dat[-length(dat)]) logret <- ts(logret, start = c(1947, 2), end = c(2019, 4),              frequency = 4) ts.plot(logret, main = \"U.S. GDP log returns\") acf(logret, lag = 8, main = \"\") title(\"Empirical autocorrelation function\") library(\"BayesianLearningCode\") library(\"mvtnorm\")  regression <- function(y, X, prior = \"improper\", b0 = 0, B0 = 1, c0 = 0.01,                        C0 = 0.01, nburn = 1000L, M = 10000L) {      N <- nrow(X)   d <- ncol(X)      if (length(b0) == 1L) b0 <- rep(b0, d)      if (!is.matrix(B0)) {     if (length(B0) == 1L) {       B0 <- diag(rep(B0, d))     } else {       B0 <- diag(B0)     }   }      if (prior == \"improper\") {      fit <- lm.fit(X, y)     betahat <- fit$coefficients     SSR <- sum(fit$residuals^2)     cN <- (N - d) / 2     CN <- SSR / 2     bN <- betahat     BN <- solve(crossprod(X))          sigma2s <- rinvgamma(M, cN, CN)     betas <- matrix(NA_real_, M, d)     for (i in seq_len(M)) betas[i, ] <- rmvnorm(1, bN, sigma2s[i] * BN)      } else if (prior == \"conjugate\") {          B0inv <- solve(B0)     BNinv <- B0inv + crossprod(X)     BN <- solve(BNinv)     bN <- BN %*% (B0inv %*% b0 + crossprod(X, y))     Seps0 <- crossprod(y) + crossprod(b0, B0inv) %*% b0 -       crossprod(bN, BNinv) %*% bN     cN <- c0 + N / 2     CN <- C0 + Seps0 / 2          sigma2s <- rinvgamma(M, cN, CN)     betas <- matrix(NA_real_, M, d)     for (i in seq_len(M)) betas[i, ] <- rmvnorm(1, bN, sigma2s[i] * BN)      } else if (prior == \"semi-conjugate\") {          # Precompute some values     B0inv <- solve(B0)     B0invb0 <- B0inv %*% b0     cN <- c0 + N / 2     XX <- crossprod(X)     Xy <- crossprod(X, y)          # Prepare memory to store the draws     betas <- matrix(NA_real_, nrow = M, ncol = d)     sigma2s <- rep(NA_real_, M)     colnames(betas) <- colnames(X)          # Set the starting value for sigma2     sigma2 <- var(y) / 2          # Run the Gibbs sampler     for (m in seq_len(nburn + M)) {       # Sample beta from its full conditional       BN <- solve(B0inv + XX / sigma2)        bN <- BN %*% (B0invb0 + Xy / sigma2)       beta <- rmvnorm(1, mean = bN, sigma = BN)              # Sample sigma^2 from its full conditional       eps <- y - tcrossprod(X, beta)       CN <- C0 + crossprod(eps) / 2       sigma2 <- rinvgamma(1, cN, CN)              # Store the results       if (m > nburn) {         betas[m - nburn, ] <- beta         sigma2s[m - nburn] <- sigma2       }     }   }   list(betas = betas, sigma2s = sigma2s) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"example-7-1-ar-modeling-of-the-u-s--gdp-data","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Example 7.1: AR modeling of the U.S. GDP data","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"begin writing function sets design matrix AR(pp) model. obtain draws four AR models improper prior.","code":"ARdesignmatrix <- function(dat, p = 1) {   d <- p + 1   N <- length(dat) - p    Xy <- matrix(NA_real_, N, d)   Xy[, 1] <- 1   for (i in seq_len(p)) {     Xy[, i + 1] <- dat[(p + 1 - i) : (length(dat) - i)]   }   Xy } set.seed(42) res <- vector(\"list\", 4) for (p in 1:4) {   y <- tail(logret, -p)   Xy <- ARdesignmatrix(logret, p)   res[[p]] <- regression(y, Xy, prior = \"improper\") }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"figure-7-2-exploratory-model-selection","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Figure 7.2: Exploratory model selection","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"Now plot draws leading coefficient, .e., coefficient corresponding highest lag models.  Next, fit AR(3) model different priors.","code":"for (p in 1:4) {   hist(res[[p]]$betas[, p + 1], freq = FALSE, main = bquote(AR(.(p))),        xlab = bquote(phi[.(p)]), ylab = \"\", breaks = seq(-.75, .75, .02))   abline(v = 0, lty = 3) } y <- tail(logret, -3) Xy <- ARdesignmatrix(logret, 3)  res_improper <- res[[3]]  res_conj_1 <- regression(y, Xy, prior = \"conjugate\",                          b0 = rep(0, 4), B0 = diag(rep(1, 4)),                          c0 = 2, C0 = 0.001)  res_semi_1 <- regression(y, Xy, prior = \"semi-conjugate\",                          b0 = rep(0, 4), B0 = diag(rep(1, 4)),                          c0 = 2, C0 = 0.001)  res_conj_2 <- regression(y, Xy, prior = \"conjugate\",                          b0 = rep(0, 4), B0 = diag(rep(100, 4)),                          c0 = 2, C0 = 0.001)  res_semi_2 <- regression(y, Xy, prior = \"semi-conjugate\",                          b0 = rep(0, 4), B0 = diag(rep(100, 4)),                          c0 = 2, C0 = 0.001)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"figure-7-3-ar3-models-with-improper-semi-conjugate-and-conjugate-priors","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Figure 7.3: AR(3) models with improper, semi-conjugate, and conjugate priors","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"now visualize posterior model parameters priors.","code":"for (i in 1:5) {   if (i <= 4) {     if (i == 1) name <- bquote(zeta) else name <- bquote(phi[.(i - 1)])     dens_improper <- density(res_improper$betas[, i], bw = \"SJ\", adj = 2)     dens_semi_1 <- density(res_semi_1$betas[, i], bw = \"SJ\", adj = 2)     dens_semi_2 <- density(res_semi_2$betas[, i], bw = \"SJ\", adj = 2)     dens_conj_1 <- density(res_conj_1$betas[, i], bw = \"SJ\", adj = 2)     dens_conj_2 <- density(res_conj_2$betas[, i], bw = \"SJ\", adj = 2)   } else {     name <- bquote(sigma[epsilon]^2)     dens_improper <- density(res_improper$sigma2, bw = \"SJ\", adj = 2)     dens_semi_1 <- density(res_semi_1$sigma2, bw = \"SJ\", adj = 2)     dens_semi_2 <- density(res_semi_2$sigma2, bw = \"SJ\", adj = 2)     dens_conj_1 <- density(res_conj_1$sigma2, bw = \"SJ\", adj = 2)     dens_conj_2 <- density(res_conj_2$sigma2, bw = \"SJ\", adj = 2)   }       plot(dens_improper, main = \"\", xlab = name, ylab = \"\",        xlim = range(dens_improper$x, dens_semi_1$x, dens_semi_2$x),        ylim = range(dens_improper$y, dens_semi_1$y, dens_semi_2$y))   if (i == 1) title(\"Semi-conjugate priors\")   lines(dens_semi_1, col = 2, lty = 2)   lines(dens_semi_2, col = 3, lty = 3)   legend(\"topright\", c(\"Improper\", \"Tight\", \"Loose\"), col = 1:3, lty = 1:3)      plot(dens_improper, main = \"\", xlab = name, ylab = \"\",        xlim = range(dens_improper$x, dens_conj_1$x, dens_conj_2$x),        ylim = range(dens_improper$y, dens_conj_1$y, dens_conj_2$y))   if (i == 1) title(\"Conjugate priors\")   lines(dens_conj_1, col = 2, lty = 2)   lines(dens_conj_2, col = 3, lty = 3)   legend(\"topright\", c(\"Improper\", \"Tight\", \"Loose\"), col = 1:3, lty = 1:3) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"section-7-2-2-exploring-stationarity-during-post-processing-1","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Section 7.2.2: Exploring stationarity during post-processing","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"reuse AR(pp) models improper prior explore stationarity p=1,2,3p = 1, 2, 3.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"figure-7-4-checking-stationarity-conditions-for-the-gdp-data","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Figure 7.4: Checking stationarity conditions for the GDP data","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"now move towards analyzing Euro area inflation data. First, plot data empirical autocorrelation function.","code":"hist(res[[1]]$betas[, 2], breaks = 20, freq = FALSE, main = \"AR(1)\",      xlab = bquote(phi), ylab = \"\")  plot(res[[2]]$betas[, 2:3], main = \"AR(2)\", xlab = bquote(phi[1]),      ylab = bquote(phi[2]), xlim = c(-2, 2), ylim = c(-1, 1),      col = rgb(0, 0, 0, .05), pch = 16) polygon(c(-2, 0, 2, -2), c(-1, 1, -1, -1), border = 2)  draws <- res[[3]]$betas[, 2:4] eigenvalues <- matrix(NA_complex_, nrow(draws), ncol(draws)) for (m in seq_len(nrow(draws))) {   Phi <- matrix(c(draws[m, ], c(1, 0, 0), c(0, 1, 0)), byrow = TRUE, nrow = 3)   eigenvalues[m, ] <- eigen(Phi, only.values = TRUE)$values } plot(eigenvalues, xlim = c(-1, 1), ylim = c(-1, 1), asp = 1,      main = \"AR(3)\", col = rgb(0, 0, 0, .05), pch = 16) symbols(0, 0, 1, add = TRUE, fg = 2, inches = FALSE) data(\"inflation\", package = \"BayesianLearningCode\") inflation <- ts(inflation, start = c(1997, 2), end = c(2025, 6),                 frequency = 12)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"figure-7-5-euro-area-inflation-data","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Figure 7.5: Euro area inflation data","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"fit AR(pp) models improper prior explore stationarity p=1,2,3p = 1, 2, 3.","code":"ts.plot(inflation, main = \"Euro area inflation\") acf(inflation, main = \"\") title(\"Empirical autocorrelation function\") res2 <- vector(\"list\", 3) for (p in 1:3) {   y <- tail(inflation, -p)   Xy <- ARdesignmatrix(inflation, p)   res2[[p]] <- regression(y, Xy, prior = \"improper\") }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"figure-7-6-checking-stationarity-conditions-for-the-euro-area-inflation-data","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Figure 7.6: Checking stationarity conditions for the Euro area inflation data","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"assess probability nonstationarity, can simply count draws outside stationarity region.","code":"ar1draws <- res2[[1]]$betas[, 2] ar2draws <- res2[[2]]$betas[, 2:3] ar3draws <- res2[[3]]$betas[, 2:4]  hist(ar1draws, breaks = 20, freq = FALSE, main = \"AR(1)\",      xlab = bquote(phi), ylab = \"\") abline(v = 1, col = 2)  plot(ar2draws, main = \"AR(2)\", xlab = bquote(phi[1]),      ylab = bquote(phi[2]), xlim = c(-2, 2), ylim = c(-1, 1),      col = rgb(0, 0, 0, .05), pch = 16) polygon(c(-2, 0, 2, -2), c(-1, 1, -1, -1), border = 2)  eigenvalues <- matrix(NA_complex_, nrow(ar3draws), ncol(ar3draws)) for (m in seq_len(nrow(ar3draws))) {   Phi <- matrix(c(ar3draws[m, ], c(1, 0, 0), c(0, 1, 0)), byrow = TRUE, nrow = 3)   eigenvalues[m, ] <- eigen(Phi, only.values = TRUE)$values } plot(eigenvalues, xlim = c(-1, 1), ylim = c(-1, 1), asp = 1,      main = \"AR(3)\", col = rgb(0, 0, 0, .05), pch = 16) symbols(0, 0, 1, add = TRUE, fg = 2, inches = FALSE) nonstationary <- matrix(NA, nrow(ar3draws), 3,                         dimnames = list(NULL, order = 1:3)) nonstationary[, 1] <- abs(ar1draws) > 1 nonstationary[, 2] <- ar2draws[, 1] + ar2draws[, 2] > 1 |     abs(ar2draws[, 2]) > 1 |     ar2draws[, 2] > 1 + ar2draws[, 1] nonstationary[, 3] <- apply(Mod(eigenvalues) > 1, 1, any) colMeans(nonstationary) #>      1      2      3  #> 0.0408 0.0107 0.0032"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"section-7-2-3-recovering-missing-time-series-data-an-introduction-to-data-augmentation","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Section 7.2.3: Recovering Missing Time Series Data ‚Äì An Introduction to Data Augmentation","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"Assume values certain time points missing. (Note sampler, require missing time points far enough apart. restriction substantial, though.) set Gibbs sampler, need starting values ùê≤miss\\mathbf y_\\text{miss}. , interpolate linearly (take average adjacent values). simplicity, employ improper prior sample iteratively. Let us briefly check trace plots ACFs draws.  Seems like mixing perfect cause concern. move visualizing observed missing values (black lines), alongside unobservable true values (red circles).  Now let us compare parameter estimates complete-data posterior posterior arising missing values ignored. start estimating model equations containing missing data simply dropped. Now can plot.","code":"missing <- seq(10, 100, by = 10) yaug <- logret yaug[missing] <- NA_real_ ymiss <- (logret[missing + 1] + logret[missing - 1]) / 2 names(ymiss) <- names(logret)[missing] ndraws <- 10000 nburn <- 1000 ind <- missing - 2  ytilde <- rep(NA_real_, 3) betas <- matrix(NA_real_, ndraws, 3) sigma2s <- rep(NA_real_, ndraws) ymisses <- matrix(NA_real_, ndraws, length(ind))  for (m in seq_len(ndraws + nburn)) {   # Augment the observed time series with the missing values   yaug[missing] <- ymiss      # Sample the parameters conditional on the augmented data   Xy <- ARdesignmatrix(yaug, 2)   y <- tail(yaug, -2)   N <- nrow(Xy)   d <- ncol(Xy)   BN <- solve(crossprod(Xy))   bN <- BN %*% crossprod(Xy, y)   cN <- (N - d) / 2   CN <- sum((y - Xy %*% bN)^2) / 2   sigma2 <- rinvgamma(1, cN, CN)   beta <- rmvnorm(1, bN, sigma2 * BN)   zeta <- beta[1]   phi <- beta[2:3]      # Sample the missing values conditional on the parameters   for (i in seq_along(ind)) {     ytilde[1] <- -zeta - phi[1] * y[ind[i] - 1] - phi[2] * y[ind[i] - 2]     ytilde[2] <- -zeta + y[ind[i] + 1] - phi[2] * y[ind[i] - 1]     ytilde[3] <- -zeta + y[ind[i] + 2] - phi[1] * y[ind[i] + 1]          X <- matrix(c(-1, phi), nrow = 3)     BN <- solve(crossprod(X))     bN <- BN %*% crossprod(X, ytilde)     ymiss[i] <- rmvnorm(1, bN, sigma2 * BN)   }      # Store the results   if (m > nburn) {     betas[m - nburn, ] <- beta     sigma2s[m - nburn] <- sigma2     ymisses[m - nburn, ] <- ymiss   } } par(mfrow = c(5, 2)) ts.plot(betas[, 1]) acf(betas[, 1]) ts.plot(betas[, 2]) acf(betas[, 2]) ts.plot(betas[, 3]) acf(betas[, 3]) ts.plot(sigma2s) acf(sigma2s) ts.plot(ymisses[, 1]) acf(ymisses[, 1]) yaug[missing] <- NA  where <- seq(max(missing) - 13, max(missing) + 3) plot(where, yaug[where], type = \"l\", ylim = range(ymisses[, i]),      xlab = \"Time\", ylab = \"\", main = \"U.S. GDP growth\") for (i in seq_along(missing)) {   for (j in seq_len(nrow(ymisses))) {     lines(c(missing[i] - 1, missing[i], missing[i] + 1),           c(yaug[missing[i] - 1], ymisses[j, i], yaug[missing[i] + 1]),           col = rgb(0, 0, 0, .02))   }   points(missing[i], logret[missing[i]], col = 2, cex = 2, pch = 16)   lines(c(missing[i] - 1, missing[i], missing[i] + 1),         c(yaug[missing[i] - 1], logret[missing[i]], yaug[missing[i] + 1]),         col = 2, cex = 2) } Xy <- ARdesignmatrix(yaug, 2) y <- tail(yaug, -2) containsNA <- apply(is.na(cbind(y, Xy)), 1, any) yred <- y[!containsNA] Xyred <- Xy[!containsNA, ] res_drop <- regression(yred, Xyred, prior = \"improper\") for (i in 1:4) {   if (i <= 3) {     if (i == 1) name <- bquote(zeta) else name <- bquote(phi[.(i - 1)])     dens_improper <- density(res[[2]]$betas[, i], bw = \"SJ\", adj = 2)     dens_drop <- density(res_drop$betas[, i], bw = \"SJ\", adj = 2)     dens_aug <- density(betas[, i], bw = \"SJ\", adj = 2)   } else {     name <- bquote(sigma[epsilon]^2)     dens_improper <- density(res[[2]]$sigma2, bw = \"SJ\", adj = 2)     dens_drop <- density(res_drop$sigma2, bw = \"SJ\", adj = 2)     dens_aug <- density(sigma2s, bw = \"SJ\", adj = 2)   }       plot(dens_improper, xlim = range(dens_improper$x, dens_drop$x, dens_aug$x),        ylim = range(dens_improper$y, dens_drop$y, dens_aug$y),        main = \"\", xlab = name, ylab = \"\", col = 3)   lines(dens_aug, col = 2, lty = 2)   lines(dens_drop, lty = 3)   legend(\"topright\", c(\"Complete-data\", \"Augmented\", \"Dropped\"),          col = 3:1, lty = 1:3) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"section-7-2-4-imposing-stationarity-via-an-independence-mh-algorithm","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models","what":"Section 7.2.4: Imposing Stationarity via an Independence MH Algorithm","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"First, fit AR(0), .e., intercept-, model, GDP data. now create Figure 7.9 via simple transformations.  continue fitting AR(0) model inflation data. can compute unconditional mean unconditional variance fitted AR models, need remove nonstationary draws. Apart , proceed .  now move imposing stationarity employing non-conjugate transformed beta prior œï\\phi, .e., (œï+1)/2‚àº‚Ñ¨(aœï,bœï), (\\phi + 1) / 2 \\sim \\mathcal{B}\\left(^\\phi, b^\\phi\\right),  employ 4-step sampler obtain posterior draws. addition, assume y0y_0 unknown priori follows stationary distribution AR(1) process. start defining density function rescaled beta. need specify hyperparameters define left hand side variable yy well design matrix. Now ready sample! repeat exercise informative beta prior. conclude, compare posteriors œï\\phi improper prior, posterior obtained post-processing draws obtain stationarity, posterior stationary-enforcing shifted beta priors.","code":"Xy <- ARdesignmatrix(logret, 0) # Fill the N x 1 design matrix with 1s res0 <- regression(logret, Xy, prior = \"improper\") mu <- sigma2 <- matrix(NA_real_, ndraws, 3, dimnames = list(NULL, order = 0:2))  mu[, \"0\"] <- res0$betas[, 1] sigma2[, \"0\"] <- res0$sigma2s for (p in 1:2) {   mu[, as.character(p)] <- res[[p]]$betas[, 1] /     (1 - rowSums(res[[p]]$betas[, 2:(p + 1), drop = FALSE])) }  # AR(1): phi <- res[[1]]$betas[, 2] sigma2[, \"1\"] <- res[[1]]$sigma2s / (1 - phi^2)  # AR(2): phi1 <- res[[2]]$betas[, 2] phi2 <- res[[2]]$betas[, 3] sigma2[, \"2\"] <- (1 - phi2) * res[[1]]$sigma2s /    ((1 + phi2) * (1 - phi1^2 + phi2^2 - 2 * phi2))    plot(density(mu[, \"0\"], bw = \"SJ\", adj = 2), xlim = range(mu), ylab = \"\",        main = \"Posterior of the marginal mean\", xlab = expression(mu)) for (p in 1:2) lines(density(mu[, as.character(p)], bw = \"SJ\", adj = 2),                      col = p + 1, lty = p + 1) legend(\"topright\", paste0(\"p = \", 0:2), col = 1:3, lty = 1:3)  plot(density(sigma2[, \"0\"], bw = \"SJ\", adj = 2), xlim = range(sigma2),      xlab = expression(sigma^2), ylab = \"\",      main = \"Posterior of the marginal variance\") for (p in 1:2) lines(density(sigma2[, as.character(p)], bw = \"SJ\", adj = 2),                      col = p + 1, lty = p + 1) legend(\"topright\", paste0(\"p = \", 0:2), col = 1:3, lty = 1:3) Xy <- ARdesignmatrix(inflation, 0) # Fill the N x 1 design matrix with 1s res20 <- regression(inflation, Xy, prior = \"improper\") mu <- sigma2 <- list()  mu[[\"0\"]] <- res20$betas[, 1] sigma2[[\"0\"]] <- res20$sigma2s  for (p in 1:2) {   mu[[as.character(p)]] <- res2[[p]]$betas[!nonstationary[, p], 1] /     (1 - rowSums(res2[[p]]$betas[!nonstationary[, p], 2:(p + 1), drop = FALSE])) }  # AR(1): phi <- res2[[1]]$betas[!nonstationary[, 1], 2] sigma2[[\"1\"]] <- res2[[1]]$sigma2s[!nonstationary[, 1]] / (1 - phi^2)  # AR(2): phi1 <- res2[[2]]$betas[!nonstationary[, 2], 2] phi2 <- res2[[2]]$betas[!nonstationary[, 2], 3] sigma2[[\"2\"]] <- (1 - phi2) * res2[[2]]$sigma2s[!nonstationary[, 2]] /    ((1 + phi2) * (1 - phi1^2 + phi2^2 - 2 * phi2))    plot(density(mu[[\"0\"]], bw = \"SJ\", adj = 2), xlim = range(mu), ylab = \"\",        main = \"Posterior of the marginal mean\", xlab = expression(mu)) for (p in 1:2) lines(density(mu[[as.character(p)]], bw = \"SJ\", adj = 2),                      col = p + 1, lty = p + 1) legend(\"topright\", paste0(\"p = \", 0:2), col = 1:3, lty = 1:3)  plot(density(sigma2[[\"0\"]], bw = \"SJ\", adj = 2), xlim = range(sigma2),      xlab = expression(sigma^2), ylab = \"\",      main = \"Posterior of the marginal variance\", ) for (p in 1:2) lines(density(sigma2[[as.character(p)]], bw = \"SJ\", adj = 2),                      col = p + 1, lty = p + 1) legend(\"topright\", paste0(\"p = \", 0:2), col = 1:3, lty = 1:3) dbetarescaled <- function(x, a, b, log = FALSE) {   tmp <- dbeta((x + 1) / 2, a, b, log = TRUE) - log(2)   if (log) tmp else exp(tmp) } # Specify the hyperparameters c0 <- 0 C0 <- 0 b0 <- 0 B0 <- Inf aphi <- 1 bphi <- 1  # Define the design matrix and y y <- inflation X <- matrix(NA_real_, nrow = length(y), 2) X[, 1] <- 1 X[, 2] <- c(NA_real_, y[-length(y)]) # Allocate some space for the posterior draws and initialize the parameters: y0s <- sigma2s <- zetas <- phis <- rep(NA_real_, ndraws) zeta <- 0 phi <- .8 sigma2 <- var(y) * (1 - phi) naccepts <- 0  for (m in seq_len(ndraws + nburn)) {   # Step (a): Draw y0   y0 <- rnorm(1, zeta + phi * y[1], sqrt(sigma2))      # Step (b): Draw the innovation variance   X[1, 2] <- y0   beta <- matrix(c(zeta, phi), nrow = 2)   tmp <- y - X %*% beta   sigma2 <- rinvgamma(1, c0 + (length(y) + 1) / 2,     C0 + .5 * (1 - phi^2) * (y0 - zeta / (1 - phi))^2 + .5 * crossprod(tmp))      # Step (c): Draw the intercept   BT <- 1 / (1 / B0 + (1 + phi) / (sigma2 * (1 - phi)) + length(y) / sigma2)   bT <- BT * (b0 / B0 + ((1 + phi) * y0 +     y[1] - phi * y0 + sum(y[-1] - phi * y[-length(y)])) / sigma2)   zeta <- rnorm(1, bT, sqrt(BT))      # Step (d): Draw the persistence   tmp <- y0^2 + sum(y[-length(y)]^2)   propmean <- (y0 * (y[1] - zeta) + sum(y[-length(y)] * (y[-1] - zeta))) / tmp   propvar <- sigma2 / tmp   phiprop <- rnorm(1, propmean, sqrt(propvar))   if (-1 < phiprop & phiprop < 1) {     logR <- dbetarescaled(phiprop, aphi, bphi, log = TRUE) -       dbetarescaled(phi, aphi, bphi, log = TRUE) +       dnorm(y0, zeta / (1 - phiprop), sqrt(sigma2 / (1 - phiprop^2)),             log = TRUE) -       dnorm(y0, zeta / (1 - phi), sqrt(sigma2 / (1 - phi^2)), log = TRUE)     if (log(runif(1)) < logR) {       phi <- phiprop       if (m > nburn) naccepts <- naccepts + 1L     }   }      # Store the draws   if (m > nburn) {     y0s[m - nburn] <- y0     sigma2s[m - nburn] <- sigma2     zetas[m - nburn] <- zeta     phis[m - nburn] <- phi   } } stationary1 <- data.frame(zeta = zetas, phi = phis, sigma2 = sigma2s, y0 = y0s) naccepts1 <- naccepts aphi <- 10 bphi <- 10  # Allocate some space for the posterior draws and initialize the parameters: y0s <- sigma2s <- zetas <- phis <- rep(NA_real_, ndraws) zeta <- 0 phi <- .8 sigma2 <- var(y) * (1 - phi) naccepts <- 0  for (m in seq_len(ndraws + nburn)) {   # Step (a): Draw y0   y0 <- rnorm(1, zeta + phi * y[1], sqrt(sigma2))      # Step (b): Draw the innovation variance   X[1, 2] <- y0   beta <- matrix(c(zeta, phi), nrow = 2)   tmp <- y - X %*% beta   sigma2 <- rinvgamma(1, c0 + (length(y) + 1) / 2,     C0 + .5 * (1 - phi^2) * (y0 - zeta / (1 - phi))^2 + .5 * crossprod(tmp))      # Step (c): Draw the intercept   BT <- 1 / (1 / B0 + (1 + phi) / (sigma2 * (1 - phi)) + length(y) / sigma2)   bT <- BT * (b0 / B0 + ((1 + phi) * y0 +     y[1] - phi * y0 + sum(y[-1] - phi * y[-length(y)])) / sigma2)   zeta <- rnorm(1, bT, sqrt(BT))      # Step (d): Draw the persistence   tmp <- y0^2 + sum(y[-length(y)]^2)   propmean <- (y0 * (y[1] - zeta) + sum(y[-length(y)] * (y[-1] - zeta))) / tmp   propvar <- sigma2 / tmp   phiprop <- rnorm(1, propmean, sqrt(propvar))   if (-1 < phiprop & phiprop < 1) {     logR <- dbetarescaled(phiprop, aphi, bphi, log = TRUE) -       dbetarescaled(phi, aphi, bphi, log = TRUE) +       dnorm(y0, zeta / (1 - phiprop), sqrt(sigma2 / (1 - phiprop^2)),             log = TRUE) -       dnorm(y0, zeta / (1 - phi), sqrt(sigma2 / (1 - phi^2)), log = TRUE)     if (log(runif(1)) < logR) {       phi <- phiprop       if (m > nburn) naccepts <- naccepts + 1L     }   }      # Store the draws   if (m > nburn) {     y0s[m - nburn] <- y0     sigma2s[m - nburn] <- sigma2     zetas[m - nburn] <- zeta     phis[m - nburn] <- phi   } } stationary2 <- data.frame(zeta = zetas, phi = phis, sigma2 = sigma2s, y0 = y0s) naccepts2 <- naccepts mybreaks <- seq(floor(100 * min(stationary1$phi, stationary2$phi)) / 100,                 ceiling(100 * max(ar1draws)) / 100,                 by = .0025) hist(stationary2$phi, breaks = mybreaks, col = rgb(1, 1, 0, .4),      main = \"Histograms of posterior draws\", xlab = expression(phi),      freq = FALSE, ylab = \"\", border = NA) hist(stationary1$phi, breaks = mybreaks, col = rgb(1, 0, 0, .2),      freq = FALSE, add = TRUE, border = NA) hist(ar1draws[!nonstationary[, 1]], breaks = mybreaks, col = rgb(0, 0, 1, .15),      freq = FALSE, add = TRUE, border = NA) hist(ar1draws, breaks = mybreaks, col = rgb(0, 1, 0, .3),      freq = FALSE, add = TRUE, border = NA) legend(\"topright\",        c(\"Unrestricted posterior\", \"Post-processed posterior\",          \"Beta prior posterior (flat)\", \"Beta prior posterior (shrunken)\"),        fill = rgb(c(0, 0, 1, 1), c(1, 0, 0, 1), c(0, 1, 0, 0),                   c(.3, .15, .2, .4)), border = NA)"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"example-7-11-improving-the-independence-mh-step","dir":"Articles","previous_headings":"Section 7.2: Bayesian Learning of (Stationary) Autoregressive Models > Section 7.2.5: Evaluating the Efficiency of an MCMC Sampler","what":"Example 7.11: Improving the independence MH step","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"Let us investigate traceplots empirical autocorrelation functions posterior draws informative stationarity-inducing prior.  now compute estimate effective sample size (ESS) inefficiency factor () use coda package. now repeat exercise, use conditional posterior resulting auxiliary moment-matched prior Step (d). Note: Currently, three methods implemented: Ignore part coming y0y_0 moment-match shifted beta prior. currently described manuscript (auxprior = analytical1) Use part coming determinant density y0y_0, resulting shifted ‚Ñ¨(aœï+0.5,bœï+0.5)\\mathcal{B}(^\\phi + 0.5, b^\\phi + 0.5) (auxprior = analytical2) Numerically approximate mode curvature distribution every iteration (auxprior = numerical) , investigate traceplots empirical autocorrelation functions draws. addition, check percentage accepted draws MH-step (d).  now compare draws two samplers; yield draws distribution, irrespective acceptance rate thus mixing Markov chain. graphically check comparing histograms quantiles draws marginal posterior œï\\phi.  can see draws appear come distribution. Note, however, first sampler gets ‚Äústuck‚Äù slightly 0.93 draws, isn‚Äôt case second sampler. conclude, compute ESSs IFs sampler utilizing optimized MH step.","code":"par(mfrow = c(4, 2), mar = c(2.5, 2.8, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5) for (i in seq_along(stationary2)) {   plot.ts(stationary2[i], xlab = \"Draws after burn-in\", ylab = labels[i])   if (i == 1) title(\"Traceplot\")   acf(stationary2[i], ylab = \"\")   if (i == 1) title(\"Empirical ACF\") } library(\"coda\") ess1 <- effectiveSize(data.frame(zeta = res2[[1]]$betas[, 1],                                  phi = res2[[1]]$betas[, 2],                                  sigma2 = res2[[1]]$sigma2s)) ess2 <- effectiveSize(data.frame(zeta = res2[[1]]$betas[!nonstationary[, 1], 1],                                  phi = res2[[1]]$betas[!nonstationary[, 1], 2],                                  sigma2 = res2[[1]]$sigma2s[!nonstationary[, 1]])) ess3 <- effectiveSize(stationary1) ess4 <- effectiveSize(stationary2) ess <- rbind(unrestricted = c(ess1, y0 = NA),              postprocessed = c(ess2, y0 = NA),              betapriorflat = ess3,              betapriorinformative = ess4) knitr::kable(round(ess)) knitr::kable(round(ndraws / ess, 2)) auxprior <- \"analytical1\"  if (auxprior == \"analytical1\") {   aphiprop <- aphi   bphiprop <- bphi } else if (auxprior == \"analytical2\") {   # Add .5 to aphi and bphi (from the determinant of the initial state prior)   aphiprop <- aphi + .5   bphiprop <- bphi + .5 }  if (auxprior != \"numerical\") {   # Compute mean and variance of the proposal using properties of the beta   auxpriormean <- 2 * aphiprop / (aphiprop + bphiprop) - 1   auxpriorvar <- 4 * (aphiprop * bphiprop) /     ((aphiprop + bphiprop)^2 * (aphiprop + bphiprop + 1)) } else {   # For the numerical method, we need the density function of the target   dprior <- function(phi, zeta, sigma2, y0, aphi, bphi, log = FALSE) {     inside <- phi <= 1 & phi >= -1     logdens <- -Inf     logdens[inside] <- dbetarescaled(phi[inside], aphi, bphi, log = TRUE) +       dnorm(y0, zeta / (1 - phi[inside]), sqrt(sigma2 / (1 - phi[inside]^2)),             log = TRUE)     if (log) logdens else exp(logdens)   } }  # Allocate some space for the posterior draws and initialize the parameters: y0s <- sigma2s <- zetas <- phis <- rep(NA_real_, ndraws) zeta <- 0 phi <- .8 sigma2 <- var(y) * (1 - phi) naccepts <- 0  for (m in seq_len(ndraws + nburn)) {   # Step (a): Draw y0   y0 <- rnorm(1, zeta + phi * y[1], sqrt(sigma2))      # Step (b): Draw the innovation variance   X[1, 2] <- y0   beta <- matrix(c(zeta, phi), nrow = 2)   tmp <- y - X %*% beta   sigma2 <- rinvgamma(1, c0 + (length(y) + 1) / 2,     C0 + .5 * (1 - phi^2) * (y0 - zeta / (1 - phi))^2 + .5 * crossprod(tmp))      # Step (c): Draw the intercept   BT <- 1 / (1 / B0 + (1 + phi) / (sigma2 * (1 - phi)) + length(y) / sigma2)   bT <- BT * (b0 / B0 + ((1 + phi) * y0 +     y[1] - phi * y0 + sum(y[-1] - phi * y[-length(y)])) / sigma2)   zeta <- rnorm(1, bT, sqrt(BT))      # Step (d): Draw the persistence   if (auxprior == \"numerical\") { # overwrites pre-defined mean and variance     mode <- optimize(dprior, c(-1, 1), zeta = zeta, sigma2 = sigma2, y0 = y0,                      aphi = aphi, bphi = bphi, log = TRUE, maximum = TRUE)$maximum     dd <- numDeriv::hessian(dprior, mode, zeta = zeta, sigma2 = sigma2, y0 = y0,                             aphi = aphi, bphi = bphi)     auxpriormean <- mode     auxpriorvar <- -1 / as.numeric(dd)   }      tmp <- y0^2 + sum(y[-length(y)]^2)   propvar <- 1 / (1 / auxpriorvar + tmp / sigma2)   propmean <- propvar * (auxpriormean / auxpriorvar + (y0 * (y[1] - zeta) +     sum(y[-length(y)] * (y[-1] - zeta))) / sigma2)    phiprop <- rnorm(1, propmean, sqrt(propvar))   if (-1 < phiprop & phiprop < 1) {     logR <- dbetarescaled(phiprop, aphi, bphi, log = TRUE) -       dbetarescaled(phi, aphi, bphi, log = TRUE) +       dnorm(y0, zeta / (1 - phiprop), sqrt(sigma2 / (1 - phiprop^2)),             log = TRUE) -       dnorm(y0, zeta / (1 - phi), sqrt(sigma2 / (1 - phi^2)), log = TRUE) +       dnorm(phi, auxpriormean, sqrt(auxpriorvar), log = TRUE) -       dnorm(phiprop, auxpriormean, sqrt(auxpriorvar), log = TRUE)     if (log(runif(1)) < logR) {       phi <- phiprop       if (m > nburn) naccepts <- naccepts + 1L     }   }      # Store the draws   if (m > nburn) {     y0s[m - nburn] <- y0     sigma2s[m - nburn] <- sigma2     zetas[m - nburn] <- zeta     phis[m - nburn] <- phi   } } stationary3 <- data.frame(zeta = zetas, phi = phis, sigma2 = sigma2s, y0 = y0s) naccepts3 <- naccepts par(mfrow = c(4, 2), mar = c(2.5, 2.8, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5) for (i in seq_along(stationary3)) {   plot.ts(stationary3[i], xlab = \"Draws after burn-in\", ylab = labels[i])   if (i == 1) title(\"Traceplot\")   acf(stationary3[i], ylab = \"\")   if (i == 1) title(\"Empirical ACF\") } mybreaks <- seq(min(stationary2$phi, stationary3$phi),                 max(stationary2$phi, stationary3$phi),                 length.out = 30) hist(stationary2$phi, breaks = mybreaks, col = rgb(0, 0, 1, .3),      main = \"Histogram\", xlab = expression(phi), freq = FALSE, ylab = \"\") hist(stationary3$phi, breaks = mybreaks, col = rgb(1, 0, 0, .3), freq = FALSE,      add = TRUE) legend(\"topleft\", c(\"Sampler 1\", \"Sampler 2\"), fill = rgb(0:1, 0, 1:0, .3)) qqplot(stationary2$phi, stationary3$phi, xlab = \"Sampler 1\", ylab = \"Sampler 2\",        main = \"QQ plot\") abline(0, 1, col = 2) ess1 <- effectiveSize(stationary2) ess2 <- effectiveSize(stationary3) ess <- rbind(`Sampler 1` = ess1, `Sampler 2` = ess2) knitr::kable(round(ess)) knitr::kable(round(ndraws / ess, 2))"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"section-7-3-1-ar-models-with-a-unit-root","dir":"Articles","previous_headings":"Section 7.3: Some Extensions","what":"Section 7.3.1: AR Models with a Unit Root","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"Let us check stationarity exchange rate data. First, load data visualize well empirical ACF. absolute returns.  clearly hints non-stationarity exchange rate series (first order) stationarity returns. check formally, fit AR(p) models . Now can graphically investigate stationarity .  explore whether nonstationarity raw series caused unit root, investigate posterior 1‚àíœï1‚àí‚Ä¶‚àíœïp1 - \\phi_1 - \\dots - \\phi_p p=1,‚Ä¶,4p = 1, \\dots, 4.  inflation data.","code":"data(\"exrates\", package = \"stochvol\") dat <- exrates$USD / exrates$CHF ret <- diff(dat) plot(dat, type = \"l\", main = \"CHF-USD exchange rate\", xlab = \"Days\",      ylab = \"CHF in USD\") plot(ret, type = \"l\", main = \"CHF-USD daily returns\", xlab = \"Days\",      ylab = \"CHF-USD\") acf(dat) acf(ret) ardat <- arret <- list() for (p in 1:4) {   y <- tail(dat, -p)   Xy <- ARdesignmatrix(dat, p)   ardat[[p]] <- regression(y, Xy, prior = \"improper\")   y <- tail(ret, -p)   Xy <- ARdesignmatrix(ret, p)   arret[[p]] <- regression(y, Xy, prior = \"improper\") } draws <- list(ardat[[2]]$betas[, 2:3], arret[[2]]$betas[, 2:3]) eigenvalues <- matrix(NA_complex_, nrow(draws[[1]]), ncol(draws[[1]])) mains <- c(\"AR(2) on the raw series\", \"AR(2) on the returns\") for (i in seq_along(draws)) {   plot(draws[[i]], main = mains[i], xlab = bquote(phi[1]),      ylab = bquote(phi[2]), xlim = c(-2, 2), ylim = c(-1, 1),      col = rgb(0, 0, 0, .05), pch = 16)   polygon(c(-2, 0, 2, -2), c(-1, 1, -1, -1), border = 2)    for (m in seq_len(nrow(draws[[i]]))) {     Phi <- matrix(c(draws[[i]][m, ], c(1, 0)), byrow = TRUE, nrow = 2)     eigenvalues[m, ] <- eigen(Phi, only.values = TRUE)$values   }   plot(eigenvalues, xlim = c(-1, 1), ylim = c(-1, 1), asp = 1,        main = mains[i], col = rgb(0, 0, 0, .05), pch = 16)   symbols(0, 0, 1, add = TRUE, fg = 2, inches = FALSE) } toplot <- matrix(NA_real_, ndraws, 4) for (p in 1:4)   toplot[, p] <- rowSums(ardat[[p]]$betas[, 2:(p + 1), drop = FALSE]) - 1 for (p in 1:4) {   hist(toplot[, p], freq = FALSE,        breaks = seq(min(toplot), max(toplot), length.out = 20),        main = paste0(\"AR(\", p, \")\"), xlab = expression(delta), ylab = \"\")   abline(v = 0, lty = 2, col = 2) } ardat <- arret <- list() for (p in 1:4) {   y <- tail(inflation, -p)   Xy <- ARdesignmatrix(inflation, p)   ardat[[p]] <- regression(y, Xy, prior = \"improper\") } for (p in 1:4)   toplot[, p] <- rowSums(ardat[[p]]$betas[, 2:(p + 1), drop = FALSE]) - 1 for (p in 1:4) {   hist(toplot[, p], freq = FALSE,        breaks = seq(min(toplot), max(toplot), length.out = 20),        main = paste0(\"AR(\", p, \")\"), xlab = expression(delta), ylab = \"\")   abline(v = 0, lty = 2, col = 2) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"section-7-3-2-bayesian-learning-of-an-ma1-model","dir":"Articles","previous_headings":"Section 7.3: Some Extensions","what":"Section 7.3.2: Bayesian Learning of an MA(1) Model","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"now implement MCMC sampler fitting MA(1) model, treat latent state œµ0‚àºùí©(0,œÉ2)\\epsilon_0 \\sim \\mathcal{N}(0, \\sigma^2) unknown. innovation variance, assume inverse gamma prior, Œ∏\\theta assumed priori uniform [‚àí1,1][-1,1]. Now plot traceplots ACFs.  repeat exercise , now use truncated Gaussian proposal random walk MH algorithm. plot traceplots ACFs.  repeat exercise , now use random walk proposal log(1+Œ∏)‚àílog(1‚àíŒ∏)\\log(1 + \\theta) - \\log(1 - \\theta). plot traceplots ACFs.  Let‚Äôs also check QQ plots equivalence.  Now, compare acceptance rates inefficiency factors 9 samplers.","code":"# Specify prior hyperparameters c0 <- C0 <- 0.01  # standard deviation for random walk MH proposal cthetas <- c(.0005, .005, .05)  # Allocate space for the draws eps0s <- sigma2s <- thetas <- matrix(NA_real_, ndraws, length(cthetas)) naccepts <- rep(0L, length(cthetas))  for (i in seq_along(cthetas)) {   # Set the starting values   theta <- 0.9   sigma2 <- var(dat) / 2    # MCMC loop   for (m in seq_len(ndraws + nburn)) {     # Sample epsilon_0     bs <- (-theta)^seq_along(dat)     as <- filter(dat, -theta, \"recursive\")     tmp <- 1 + sum(bs^2)     eps0 <- rnorm(1, -sum(as * bs) / tmp, sqrt(sigma2 / tmp))          # Sample sigma^2     eps <- filter(dat, -theta, \"recursive\", init = eps0)     cN <- c0 + (length(dat) + 1) / 2     CN <- C0 + 0.5 * (eps0^2 + sum(eps^2))     sigma2 <- rinvgamma(1, cN, CN)        # Sample theta via RW-MH     thetaprop <- rnorm(1, theta, cthetas[i])     epsprop <- filter(dat, -thetaprop, \"recursive\", init = eps0)          # Now we accept/reject. Note that because we have a uniform prior on     # (-1, 1), it suffices to check whether the proposed value is in that     # interval and we do not need to include the prior in the acceptance ratio     if (abs(thetaprop) < 1) {       logR <- -0.5 / sigma2 * (sum(epsprop^2) - sum(eps^2))       if (log(runif(1)) < logR) {         theta <- thetaprop         if (m > nburn) naccepts[i] <- naccepts[i] + 1L       }     }        # Store the results     if (m > nburn) {       eps0s[m - nburn, i] <- eps0       sigma2s[m - nburn, i] <- sigma2       thetas[m - nburn, i] <- theta     }   } } for (i in seq_along(cthetas)) {   ts.plot(thetas[, i], ylim = range(thetas), ylab = expression(theta),           xlab = \"Iterations\")   title(bquote(Traceplot ~ (c[theta] == .(cthetas[i]))))   acf(thetas[, i], ylab = \"\")   title(bquote(ACF ~ (acceptance == .(round(naccepts[i] / ndraws, 3))*\",\"~                  IF == .(round(ndraws / coda::effectiveSize(thetas[, i]), 1))))) } # standard deviation for random walk MH proposal cthetas2 <- cthetas  # Allocate space for the draws eps0s2 <- sigma2s2 <- thetas2 <- matrix(NA_real_, ndraws, length(cthetas2)) naccepts2 <- rep(0L, length(cthetas2))  for (i in seq_along(cthetas2)) {   # Set the starting values   theta <- 0.9   sigma2 <- var(dat) / 2    # MCMC loop   for (m in seq_len(ndraws + nburn)) {     # Sample epsilon_0     bs <- (-theta)^seq_along(dat)     as <- filter(dat, -theta, \"recursive\")     tmp <- 1 + sum(bs^2)     eps0 <- rnorm(1, -sum(as * bs) / tmp, sqrt(sigma2 / tmp))          # Sample sigma^2     eps <- filter(dat, -theta, \"recursive\", init = eps0)     cN <- c0 + (length(dat) + 1) / 2     CN <- C0 + 0.5 * (eps0^2 + sum(eps^2))     sigma2 <- rinvgamma(1, cN, CN)        # Sample theta via RW-MH using inverse transform sampling     # for the truncated Gaussian     pL <- pnorm(-1, theta, cthetas2[i])     pU <- pnorm(1, theta, cthetas2[i])     norm <- pU - pL     U <- runif(1)     thetaprop <- qnorm(pL + U * norm, theta, cthetas2[i])     epsprop <- filter(dat, -thetaprop, \"recursive\", init = eps0)     normprop <- diff(pnorm(c(-1, 1), thetaprop, cthetas2[i]))          # Now we accept/reject. Note that because we have a uniform prior on     # (-1, 1) and, due to the truncated proposal, we can be sure that a     # value within (-1, 1) is proposed, we do not need to include the     # prior in the acceptance ratio. However, we need to cater for the     # asymmetry of the proposal!     logR <- -0.5 / sigma2 * (sum(epsprop^2) - sum(eps^2)) +       log(norm) - log(normprop)     if (log(runif(1)) < logR) {       theta <- thetaprop       if (m > nburn) naccepts2[i] <- naccepts2[i] + 1L     }        # Store the results     if (m > nburn) {       eps0s2[m - nburn, i] <- eps0       sigma2s2[m - nburn, i] <- sigma2       thetas2[m - nburn, i] <- theta     }   } } for (i in seq_along(cthetas2)) {   ts.plot(thetas2[, i], ylim = range(thetas2), ylab = expression(theta),           xlab = \"Iterations\")   title(bquote(Traceplot ~ (c[theta] == .(cthetas2[i]))))   acf(thetas2[, i], ylab = \"\")   title(bquote(ACF ~ (acceptance == .(round(naccepts2[i] / ndraws, 3))*\",\"~                  IF == .(round(ndraws / coda::effectiveSize(thetas2[, i]), 1))))) } # Define the transformation and its inverse trans <- function(theta) log(1 + theta) - log(1 - theta) invtrans <- function(thetatrans) (exp(thetatrans) - 1) / (exp(thetatrans) + 1)  # standard deviation for random walk MH proposal cthetas3 <- 100 * cthetas2  # Allocate space for the draws eps0s3 <- sigma2s3 <- thetas3 <- matrix(NA_real_, ndraws, length(cthetas3)) naccepts3 <- rep(0L, length(cthetas3))  for (i in seq_along(cthetas3)) {   # Set the starting values   theta <- 0.9   sigma2 <- var(dat) / 2    # MCMC loop   for (m in seq_len(ndraws + nburn)) {     # Sample epsilon_0     bs <- (-theta)^seq_along(dat)     as <- filter(dat, -theta, \"recursive\")     tmp <- 1 + sum(bs^2)     eps0 <- rnorm(1, -sum(as * bs) / tmp, sqrt(sigma2 / tmp))          # Sample sigma^2     eps <- filter(dat, -theta, \"recursive\", init = eps0)     cN <- c0 + (length(dat) + 1) / 2     CN <- C0 + 0.5 * (eps0^2 + sum(eps^2))     sigma2 <- rinvgamma(1, cN, CN)        # Sample theta via RW-MH on a trans(theta)     thetatransprop <- rnorm(1, trans(theta), cthetas3[i])     thetaprop <- invtrans(thetatransprop)     epsprop <- filter(dat, -thetaprop, \"recursive\", init = eps0)          # Now we accept/reject. Note that because we have a uniform prior on     # (-1, 1) and, due to the transformation, we can be sure that a     # value within (-1, 1) is proposed, we do not need to include the     # prior in the acceptance ratio. However, we need to cater for the     # asymmetry of the proposal!     logR <- -0.5 / sigma2 * (sum(epsprop^2) - sum(eps^2)) +       log(1 - thetaprop^2) - log(1 - theta^2)     if (log(runif(1)) < logR) {       theta <- thetaprop       if (m > nburn) naccepts3[i] <- naccepts3[i] + 1L     }        # Store the results     if (m > nburn) {       eps0s3[m - nburn, i] <- eps0       sigma2s3[m - nburn, i] <- sigma2       thetas3[m - nburn, i] <- theta     }   } } for (i in seq_along(cthetas3)) {   ts.plot(thetas3[, i], ylim = range(thetas3), ylab = expression(theta),           xlab = \"Iterations\")   title(bquote(Traceplot ~ (c[theta] == .(cthetas3[i]))))   acf(thetas3[, i], ylab = \"\")   title(bquote(ACF ~ (acceptance == .(round(naccepts3[i] / ndraws, 3))*\",\"~                  IF == .(round(ndraws / coda::effectiveSize(thetas3[, i]), 1))))) } abline(c(0, 1), col = 2) qqplot(thetas[, 2], thetas3[, 2]) abline(c(0, 1), col = 2) accepts <- matrix(c(naccepts, naccepts2, naccepts3) / ndraws,                   nrow = length(naccepts), ncol = 3, byrow = TRUE) ESS <- matrix(coda::effectiveSize(cbind(thetas, thetas2, thetas3)),               nrow = ncol(thetas), ncol = 3, byrow = TRUE) colnames(ESS) <- colnames(accepts) <- c(\"tiny\", \"medium\", \"huge\") rownames(ESS) <- rownames(accepts) <- c(\"Gaussian RW\", \"truncated RW\",                                         \"transformed RW\") IF <- ndraws / ESS knitr::kable(round(accepts, 2)) knitr::kable(round(IF, 1))"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"example-7-13-wage-mobility-data","dir":"Articles","previous_headings":"Section 7.4: Markov modeling for a panel of categorical time series","what":"Example 7.13: Wage mobility data","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"load data consider workers birth cohort 1946-1960. extract columns income time: plot individual wage mobility time series three workers. persistence belonging certain wage class obvious specific person.","code":"data(\"labor\", package = \"BayesianLearningCode\") labor <- subset(labor, birthyear >= 1946 & birthyear <= 1960) nrow(labor) #> [1] 1538 income <- labor[, grepl(\"^income\", colnames(labor))] income <- sapply(income, as.integer) colnames(income) <- gsub(\"income_\", \"\", colnames(income)) set.seed(1) index <- sample(nrow(income), 3) for (i in index) {     plot(as.integer(colnames(income)), income[i, ] - 1,          main = paste(\"Person\", i), pch = 19, ylim = c(0, 5),          xlab = \"Year\", ylab = \"Income class\") }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"example-7-14-wage-mobility-data-comparing-wage-mobility-of-men-and-women","dir":"Articles","previous_headings":"Section 7.4: Markov modeling for a panel of categorical time series","what":"Example 7.14: Wage mobility data ‚Äì comparing wage mobility of men and women","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"transform data obtain worker matrix contains number transitions one class , .e., matrix values Ni,hkN_{,hk}. Based transition matrices, total transitions wage categories female male workers can obtained. obtain posterior mean estimates based uniform prior. also visualize posterior mean estimates women men.  compare posterior densities various transition probabilities Œæg,hk\\xi_{g,hk} women men.","code":"getTransitions <- function(x, classes) {     transitions <- matrix(0, length(classes), length(classes))     for (i in seq_len(length(x) - 1)) {         transitions[x[i], x[i + 1]] <- transitions[x[i], x[i + 1]] + 1     }     dimnames(transitions) <- list(from = classes, to = classes)     transitions } income_transitions <-     lapply(seq_len(nrow(income)),            function(i) getTransitions(income[i, ], classes = 0:5)) N_female <- Reduce(\"+\", income_transitions[labor$female]) N_male <- Reduce(\"+\", income_transitions[!labor$female]) knitr::kable(N_female) knitr::kable(N_male) mean_xi_female <- (1 + N_female) / rowSums(1 + N_female) mean_xi_male <- (1 + N_male) / rowSums(1 + N_male) knitr::kable(mean_xi_female, digits = 3) knitr::kable(mean_xi_male, digits = 3) corrplot::corrplot(mean_xi_female, method = \"square\", is.corr = FALSE,                    col = 1, cl.pos = \"n\") corrplot::corrplot(mean_xi_male, method = \"square\", is.corr = FALSE,                    col = 1, cl.pos = \"n\") plot(c(0.52, 0.95), c(0, 100), type = \"n\", xlab = \"\", ylab = \"\",      main = \"Posterior of persistence probabilities\") pers_female <- cbind(diag(1 + N_female),                      rowSums(1 + N_female) - diag(1 + N_female)) pers_male <- cbind(diag(1 + N_male),                      rowSums(1 + N_male) - diag(1 + N_male)) for (i in 2:6) {     curve(dbeta(x, pers_female[i, 1], pers_female[i, 2]),           col = i, add = TRUE, n = 1001, lty = 1)     curve(dbeta(x, pers_male[i, 1], pers_male[i, 2]),           col = i, add = TRUE, n = 1001, lty = 2) } legend(\"topleft\", col = 2:6, lty = 1,        legend = sapply(2:6, function(i)            substitute(xi[i], list(i = (i-1) * 11)))) legend(\"topright\", col = 1, lty = 1:2,        legend = c(\"female\", \"male\")) plot(c(0.03, 0.23), c(0, 90), type = \"n\", xlab = \"\", ylab = \"\",      main = \"Posterior of transition probabilities\") trans_female <- cbind((1 + N_female)[cbind(1:5, 2:6)],                   rowSums(1 + N_female)[1:5] - (1 + N_female)[cbind(1:5, 2:6)]) trans_male <- cbind((1 + N_male)[cbind(1:5, 2:6)],                   rowSums(1 + N_male)[1:5] - (1 + N_male)[cbind(1:5, 2:6)]) for (i in 2:5) {     curve(dbeta(x, trans_female[i, 1], trans_female[i, 2]),           col = i, add = TRUE, n = 1001, lty = 1)     curve(dbeta(x, trans_male[i, 1], trans_male[i, 2]),           col = i, add = TRUE, n = 1001, lty = 2) } legend(\"topleft\", col = 2:5, lty = 1,        legend = sapply(2:5, function(i)            substitute(xi[i], list(i = c(01, 12, 23, 34, 45)[i])))) legend(\"topright\", col = 1, lty = 1:2,        legend = c(\"female\", \"male\"))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter07.html","id":"example-7-15-wage-mobility-data-long-run","dir":"Articles","previous_headings":"Section 7.4: Markov modeling for a panel of categorical time series","what":"Example 7.15: Wage mobility data ‚Äì long run","title":"Chapter 7: Introduction to Bayesian Time Series Analysis","text":"assume men women start labor market wage distribution, 70% start wage category 1 30% wage category 2, .e., ùõà0=(0,0.7,0.3,0,0,0)\\mathbf{\\eta}_0 = (0, 0.7, 0.3, 0, 0, 0). compare evolution estimated wage distribution ùõàÃÇg,t\\hat{\\mathbf{\\eta}}_{g,t} first ten years females males.  inspect posterior distributions Œ∑t,2\\eta_{t,2} wage category 2 (left-hand side) versus Œ∑t,5\\eta_{t,5} wage category 5 (right-hand side) year t=10t = 10 females males.","code":"eta_0 <- c(0, 0.7, 0.3, 0, 0, 0) eta_hat_male_t <- eta_hat_female_t <-     matrix(NA_real_, nrow = 6, ncol = 11,            dimnames = list(0:5, 0:10)) eta_hat_male_t[, 1] <- eta_hat_female_t[, 1] <- eta_0 for (i in 2:11) {     eta_hat_female_t[, i] <- eta_hat_female_t[, i - 1] %*% mean_xi_female     eta_hat_male_t[, i] <- eta_hat_male_t[, i - 1] %*% mean_xi_male } barplot(eta_hat_female_t, main = \"Women\", xlab = \"Year\", ylab = \"Wage groups\") barplot(eta_hat_male_t, main = \"Men\", xlab = \"Year\", ylab = \"Wage groups\") M <- 1000 xi_female <- replicate(M, apply(1 + N_female, 1,                                 function(alpha) rdirichlet(1, alpha))) xi_male <- replicate(M, apply(1 + N_male, 1,                               function(alpha) rdirichlet(1, alpha))) eta_male_t <- eta_female_t <- matrix(eta_0, nrow = M, ncol = 6,                                      byrow = TRUE) for (i in 2:11) {     eta_female_t <- t(sapply(1:M, function(m)         xi_female[, , m] %*% eta_female_t[m, ]))     eta_male_t <- t(sapply(1:M, function(m)         xi_male[, , m] %*% eta_male_t[m, ])) }  breaks <- seq(0.14, 0.26, length.out = 30) hist(eta_male_t[, 3], breaks = breaks, xlim = range(breaks),      col = rgb(1, 0, 0, 0.2), xlab = \"\", main = \"Wage category 2\") hist(eta_female_t[, 3], breaks = breaks, col = rgb(0, 0, 0, 0.2), add = TRUE) legend(\"topright\", c(\"female\", \"male\"), fill = rgb(c(0, 1), 0, 0, 0.2))  breaks <- seq(0, 0.15, length.out = 30) hist(eta_female_t[, 6], breaks = breaks, xlim = range(breaks),      col = rgb(0, 0, 0, 0.2), xlab = \"\", main = \"Wage category 5\") hist(eta_male_t[, 6], breaks = breaks,      col = rgb(1, 0, 0, 0.2), add = TRUE) legend(\"topright\", c(\"female\", \"male\"), fill = rgb(c(0, 1), 0, 0, 0.2))"},{"path":[]},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-1-labor-market-data","dir":"Articles","previous_headings":"Section 8.1: Binary response variables > Section 8.1.1: Probit model","what":"Example 8.1: Labor market data","title":"Chapter 8: Beyond Standard Regression Analysis","text":"illustrate probit regression analysis labor market data. model binary variable unemployment dependent variable use covariates variables female (binary), wcollar (binary), age18 (quantitative, centered 18 years) unemployed 1997 (binary). baseline person hence 18 year old male blue collar worker employed 1997.","code":"library(\"BayesianLearningCode\") data(\"labor\", package = \"BayesianLearningCode\") y.unemp <- labor$income_1998 == \"zero\" N.unemp <- length(y.unemp)  # number of observations  X.unemp <- with(labor, cbind(intercept = rep(1, N.unemp),                              female = female,                              age18 = 1998 - birthyear - 18,                              wcollar = wcollar_1997,                              unemp97 = income_1997 == \"zero\")) # regressor matrix"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-2-","dir":"Articles","previous_headings":"Section 8.1: Binary response variables > Section 8.1.1: Probit model","what":"Example 8.2.","title":"Chapter 8: Beyond Standard Regression Analysis","text":"regression coefficients estimated using data augmentation Gibbs sampling. define function yielding posterior draws using algorithm detailed Section 8.1.1. specify prior regression effects rather flat normal independence prior estimate model. compute summary statistics posterior use following function. estimated risk unemployment baseline person low even lower white collar worker. higher females, older persons particularly unemployed 1997.  plot autocorrelation draws shows although autocorrelation, vanishes lags.  sampler easy implement, however might problems response variable contains either many successes.","code":"probit <- function(y, X, b0 = 0, B0 = 10000,                    burnin = 1000L, M = 20000L) {   N <- length(y)   d <- ncol(X) # number regression effects     B0.inv <- diag(rep(1 / B0, length.out = d), nrow = d)    b0 <- rep(b0, length.out = d)    B0inv.b0 <- B0.inv %*% b0    betas <- matrix(NA_real_, nrow = M, ncol = d)   colnames(betas) <- colnames(X)      z <- rep(NA_real_, N)    # define quantities for the Gibbs sampler   BN <- solve(B0.inv + crossprod(X))   ind0 <- (y == 0) # indicators for zeros   ind1 <- (y == 1) # indicators for ones    # starting values   beta <- c(qnorm(mean(y)), rep(0, d-1))    for (m in seq_len(burnin + M)) {     # Draw z conditional on y and beta     u <- runif(N)     eta <- X %*% beta     pi <- pnorm(eta)          z[ind0] <- eta[ind0] + qnorm(u[ind0] * (1 - pi[ind0]))     z[ind1] <- eta[ind1] + qnorm(1 - u[ind1] * pi[ind1])        # sample beta from the full conditional      bN <- BN %*% (B0inv.b0 + crossprod(X, z))     beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))          # Store the beta draws     if (m > burnin) {       betas[m - burnin, ] <- beta     }  }  return(betas) } set.seed(1234) betas <- probit(y.unemp, X.unemp, b0 = 0, B0 = 10000) res.mcmc <- function(x, lower = 0.025, upper = 0.975) {   res <- c(quantile(x, lower), mean(x), quantile(x, upper))   names(res) <- c(paste0(lower * 100, \"%\"), \"Posterior mean\",                    paste0(upper *   100, \"%\"))   res } res_beta <- t(apply(betas, 2, res.mcmc)) knitr::kable(round(res_beta, 3)) (p_unemploy_base <- pnorm(res_beta[1, 2])) #> [1] 0.0241192 for (j in seq_len(ncol(betas))) {   hist(betas[, j], freq = FALSE, main = \"\", xlab = colnames(betas)[j],         ylab = \"\") } for (j in seq_len(ncol(betas))) {   acf(betas[, j], main = \"\", xlab = colnames(betas)[j], ylab = \"\") } library(\"coda\") effectiveSize(betas) #> intercept    female     age18   wcollar   unemp97  #>  2861.744  3726.047  2758.979  3558.712  3615.351"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-3","dir":"Articles","previous_headings":"Section 8.1: Binary response variables > Section 8.1.1: Probit model","what":"Example 8.3","title":"Chapter 8: Beyond Standard Regression Analysis","text":"illustrate issue, use data N=500N = 500 trials 1 success 1 failure observed. cases autocorrelation draws decreases slowly remains still high even higher lags.  High autocorrelation MCMC draws probit models occurs successes failures rare, also covariate (linear combination covariates) perfectly allows predict successes /failures. Complete separation means successes failures can perfectly predicted covariate, whereas quasi-complete separation means either successes failures can predicted perfectly.","code":"set.seed(1234)  N <- 500 X <- matrix(1, nrow = N)  y1 <- c(0, rep(1, N-1)) betas1 <- probit(y1, X, b0 = 0, B0 = 10000)  y2 <- c(rep(0, N-1), 1) betas2 <- probit(y2, X, b0 = 0, B0 = 10000) plot(betas1, type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas1)  effectiveSize(betas1) #>     var1  #> 165.9583  plot(betas2, type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas2) effectiveSize(betas2) #>     var1  #> 150.8803"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-4","dir":"Articles","previous_headings":"Section 8.1: Binary response variables > Section 8.1.1: Probit model","what":"Example 8.4","title":"Chapter 8: Beyond Standard Regression Analysis","text":"illustrate effect complete separation estimates, generate N=500N = 500 observations half successes half failures. add binary predictor xx x=1x = 1 observe successes x=0x = 0 failures. estimate model parameters plot ACF draws. autocorrelations remain high many lags still high even lag 35.","code":"N <- 500 ns <- 250 x.sep <- rep(c(0, 1), c(ns, N - ns)) y <- rep(c(0, 1), c(ns, N - ns))  table(x.sep, y) #>      y #> x.sep   0   1 #>     0 250   0 #>     1   0 250 set.seed(1234) X.sep <- cbind(rep(1, N), x.sep) betas.sep <- probit(y, X.sep, b0 = 0, B0 = 10000)  plot(betas.sep[, 1], type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas.sep[, 1])  plot(betas.sep[, 2], type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas.sep[, 2]) effectiveSize(betas.sep) #>             x.sep  #> 8.375523 8.269881"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-5","dir":"Articles","previous_headings":"Section 8.1: Binary response variables > Section 8.1.1: Probit model","what":"Example 8.5","title":"Chapter 8: Beyond Standard Regression Analysis","text":"illustrate quasi-separation use responses Example 8.3., now set x=1x=1 successes additionally 100 failures. Hence x=0x=0 always failure observed, whereas x=1x=1 successes failures occur. autocorrelations high intercept well covariate effect.  change setting xx takes values 00 failures also successes, whereas x=1x=1 successes, observe low autocorrelations intercept still high autocorrelations covariate effect.  High autocorrelations typically indicate problems sampler. complete quasi-complete separation data, likelihood monotone maximum likelihood estimate exist. Bayesian approach using flat, improper prior regression effects result improper posterior distribution. Hence, proper prior required avoid improper posteriors case separation. now analyze data informative prior, $\\Normal(\\mathbf{0}, \\mathbf{}}$. prior assume P(y=1)P(y=1) P(y=0)P(y=0) prior probability ‚âà0.95\\approx 0.95 interval [0.023,0.977][0.023, 0.977]. case autocorrelations much lower effective sample sizes roughly 700.","code":"x.qus1 <- rep(c(0, 1), c(ns-100, N - ns+100)) table(x.qus1, y) #>       y #> x.qus1   0   1 #>      0 150   0 #>      1 100 250 par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)  set.seed(1234) X.qus1 <- cbind(rep(1, N), x.qus1) betas.qus1 <- probit(y, X.qus1, b0 = 0, B0 = 10000)  plot(betas.qus1[, 1], type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas.qus1[, 1])  plot(betas.qus1[, 2], type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas.qus1[, 2]) effectiveSize(betas.qus1) #>            x.qus1  #> 8.529486 8.683472 x.qus2 <- rep(c(0, 1), c(ns+100, N - ns-100)) table(x.qus2, y) #>       y #> x.qus2   0   1 #>      0 250 100 #>      1   0 150  set.seed(1234) X.qus2 <- cbind(rep(1, N), x.qus2) betas.qus2 <- probit(y, X.qus2, b0 = 0, B0 = 10000)  par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5) plot(betas.qus2[, 1], type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas.qus2[, 1])  plot(betas.qus2[, 2], type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas.qus2[, 2]) effectiveSize(betas.qus2) #>                  x.qus2  #> 7748.599768    6.283738 set.seed(1234) betas.sep1 <- probit(y, X.sep, b0 = 0, B0 = 1)  res_betas.sep1 <- t(apply(betas.sep1, 2, res.mcmc)) knitr::kable(round(res_betas.sep1, 3)) par(mfrow = c(2, 2), mar = c(2.5, 1.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)  plot(betas.sep1[, 1], type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas.sep1[, 1])  plot(betas.sep1[, 2], type = \"l\", main = \"\", xlab = \"\", ylab = \"\") acf(betas.sep1[, 2]) effectiveSize(betas.sep1) #>             x.sep  #> 707.9111 604.7943"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-6-labor-market-data","dir":"Articles","previous_headings":"Section 8.1: Binary response variables > Section 8.1.2: Logit model","what":"Example 8.6: Labor market data","title":"Chapter 8: Beyond Standard Regression Analysis","text":"now estimate logistic regression model labor market data using two-block Polya-Gamma sampler. use flat independence normal prior regression effects estimate model. Note logistic distribution variance œÄ2/3\\pi^2/3 hence regression effects absolutely larger probit model. However probability computed two models close, e.g., compare probability unemployed baseline person. can compare posterior estimates coefficients probit model Œ≤\\beta logit model multiplying œÄ/3\\pi/\\sqrt{3} see much difference.","code":"logit <- function(y, X, b0 = 0, B0 = 10000,                   burnin = 1000L, M = 5000L) {      N <- length(y)   d <- ncol(X) # number regression effects     B0.inv <- diag(rep(1 / B0, length.out = d), nrow = d)    b0 <- rep(b0, length.out = d)    B0inv.b0 <- B0.inv %*% b0    betas <- matrix(NA_real_, nrow = M, ncol = d)   colnames(betas) <- colnames(X)    # define quantities for the Gibbs sampler   ind0 <- (y == 0) # indicators for zeros   ind1 <- (y == 1) # indicators for ones      # starting values   beta <- rep(0, d)   z <- rep(NA_real_, N)   omega <-rep(NA_real_, N)    for (m in seq_len(burnin + M)) {     # Draw z conditional on y and beta     eta <- X %*% beta     pi <- plogis(eta)             u <- runif(N)     z[ind0] <- eta[ind0] + qlogis(u[ind0] * (1 - pi[ind0]))     z[ind1] <- eta[ind1] + qlogis (1 - u[ind1] * pi[ind1])            # Draw omega conditional on y, beta and z     omega <- pgdraw::pgdraw(b = 1, c = z - eta)        # sample beta from the full conditional      Xomega <- matrix(omega, ncol = d, nrow = N) * X     BN <- solve(B0.inv + crossprod(Xomega, X))     bN <- BN %*% (B0inv.b0 + crossprod(Xomega, z))     beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))          # Store the beta draws     if (m > burnin) {       betas[m - burnin, ] <- beta     }  }  return(betas) } betas_logit <- logit(y.unemp, X.unemp, b0 = 0, B0 = 10000) print(str(betas_logit)) #>  num [1:5000, 1:5] -3.58 -3.53 -3.49 -3.71 -3.77 ... #>  - attr(*, \"dimnames\")=List of 2 #>   ..$ : NULL #>   ..$ : chr [1:5] \"intercept\" \"female\" \"age18\" \"wcollar\" ... #> NULL  res_beta_logit <- apply(betas_logit, 2, res.mcmc) knitr::kable(round(res_beta_logit, 3)) (p_unemploy_base <- plogis(res_beta_logit[1, 2])) #> [1] 0.5364618 knitr::kable(round(res_beta * pi / sqrt(3), 3))"},{"path":[]},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-7-road-safety-data","dir":"Articles","previous_headings":"Section 8.2: Count response variables > Section 8.2.1: Poisson regression models","what":"Example 8.7: Road safety data","title":"Chapter 8: Beyond Standard Regression Analysis","text":"fit two different Poisson regression models: small model intercept, intervention effect holiday dummy (activated July/August); large model intercept, intervention effect, linear trend, seasonal pattern captured monthly dummies. sampler performance two models assessed study acceptance rate deteroriates, dd increases. load data extract observations children Linz. , define regressor matrix. compute parameters normal proposal density, use Newton-Raphson estimator described Section 8.2.1. use flat normal prior regression effects. Next set independence Metropolis-Hastings algorithm estimate model parameters. fit alternative model intercept, intervention effect, linear trend seasonal dummy variables. set prior parameters compute parameters proposal distribution fit model.","code":"data(\"accidents\", package = \"BayesianLearningCode\") y <- accidents[, \"children_accidents\"] e <- rep(1, length(y)) X <- cbind(intercept = rep(1, length(y)),            intervention = rep(c(0, 1), c(7 * 12 + 9, 8 * 12 + 3)),            holiday = rep(rep(c(0, 1, 0), c(6, 2, 4)), 16)) gen.proposal.poisson <- function(y, X, e, b0 = 0, B0 = 100, t.max = 30) {   N <- length(y)   d <- ncol(X)   betas <- matrix(NA_real_, ncol = t.max, nrow = d)   beta.old <- c(log(mean(y)), rep(0, d - 1))    b0 <- rep(b0, length.out = d)   B0.inv <- diag(rep(1 / B0, length.out = d), nrow = d)     for (t in 1:t.max) {     rate <- e * exp(X %*% beta.old)     score <- t(crossprod(y - rate, X) - (beta.old - b0) %*% B0.inv)      H <- -B0.inv     for (i in 1:N) {       H <- H - rate[i] * tcrossprod(X[i, ])     }     betas[, t] <- beta.old - solve(H, score)   }   qmean <- betas[, t.max]      # determine the variance matrix     rate <- e * exp(X %*% qmean)   H <- -B0.inv   for (i in 1:N) {      H <- H - rate[i] * tcrossprod(X[i, ])   }   qvar <- -solve(H)   return(parms.proposal = list(mean = qmean,                                var = qvar)) } parms.proposal <- gen.proposal.poisson(y, X, e, b0 = 0, B0 = 100) print(parms.proposal) #> $mean #> [1]  0.8867173 -0.3465599 -0.5944486 #>  #> $var #>              [,1]          [,2]          [,3] #> [1,]  0.005121126 -0.0048204767 -0.0031235498 #> [2,] -0.004820477  0.0111451181  0.0002039249 #> [3,] -0.003123550  0.0002039249  0.0303637113 poisson <- function(y, X, e, b0 = 0, B0 = 100, qmean, qvar,                     burnin = 1000L, M = 5000L) {   d <- ncol(X)   beta.post <- matrix(ncol = d, nrow = M)   colnames(beta.post) <- colnames(X)      acc <- numeric(length = M)   b0 <- rep(b0, length.out = d)   B0 <- diag(rep(B0, length.out = d), nrow = d)      beta <- as.vector(mvtnorm::rmvnorm(1, mean = qmean, sigma = qvar))      for (m in seq_len(burnin + M)) {     beta.old <- beta     beta.proposed <- as.vector(mvtnorm::rmvnorm(1, mean = qmean, sigma = qvar))          # compute log proposal density at proposed and old value     lq_proposed <- mvtnorm::dmvnorm(beta.proposed, mean = qmean, sigma = qvar,                                     log = TRUE)     lq_old  <- mvtnorm::dmvnorm(beta.old, mean = qmean, sigma = qvar,                                 log = TRUE)          # compute log prior of proposed and old value     lpri_proposed <- mvtnorm::dmvnorm(beta.proposed, mean = b0, sigma = B0,                                       log = TRUE)     lpri_old  <- mvtnorm::dmvnorm(beta.old, mean = b0, sigma = B0,                                   log = TRUE)          # compute log-likelihood of proposed and old value     lh_proposed <- dpois(y, e * exp(X %*% beta.proposed), log = TRUE)     lh_old  <- dpois(y, e * exp(X %*% beta.old), log = TRUE)          maxlik <- max(lh_old, lh_proposed)     ll <- sum(lh_proposed - maxlik) - sum(lh_old - maxlik)          # compute acceptance probability and accept or not     log_acc <- min(0, ll + lpri_proposed - lpri_old + lq_old - lq_proposed)          if (log(runif(1)) < log_acc) {       beta <- beta.proposed       accept <- 1     } else {       beta <- beta.old       accept <- 0     }      # Store the beta draws     if (m > burnin) {         beta.post[m-burnin, ] <- beta         acc[m-burnin] <- accept     }   }   return(res = list(beta.post = beta.post, accept = mean(acc))) } res1 <- poisson(y, X, e, b0 = 0, B0 = 100,                 qmean = parms.proposal$mean, qvar = parms.proposal$var) res.poisson1 <- t(rbind(apply(res1$beta.post, 2, res.mcmc),                         `exp(Mean)` = exp(colMeans(res1$beta.post)))) knitr::kable(round(res.poisson1, 3)) print(res1$accept) #> [1] 0.3174 seas <- rbind(diag(1, 11), rep(-1, 11))  seas.dummies <- matrix(rep(t(seas), 16), ncol = 11, byrow = TRUE) colnames(seas.dummies) <- c(\"Jan\", \"Feb\", \"Mar\", \"Apr\",\"May\", \"Jun\", \"Jul\",                             \"Aug\", \"Sep\", \"Oct\", \"Nov\") X.large <- cbind(X,                  lin.trend = 1:length(y),                  seas.dummies) parms.proposal2 <- gen.proposal.poisson(y, X.large, e, b0 = 0, B0 = 100) res2 <- poisson(y, X.large, e, b0 = 0, B0 = 100,                 qmean = parms.proposal2$mean, qvar = parms.proposal2$var) res.poisson2 <- t(rbind(apply(res2$beta.post, 2, res.mcmc),                         `exp(Mean)` = exp(colMeans(res2$beta.post)))) knitr::kable(round(res.poisson2, 3)) print(res2$accept) #> [1] 0.1132 # xtable(res.poisson1, caption= \"Count Data: estimation results for Model 1\", label=\"res.count1\", digits=3) # xtable(res.poisson2, caption= \"Count Data: estimation results for Model 2\", label=\"res.count2\", digits=3)"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-8-road-safety-data","dir":"Articles","previous_headings":"Section 8.2: Count response variables > Section 8.2.2: Negative binomial regression","what":"Example 8.8: Road safety data","title":"Chapter 8: Beyond Standard Regression Analysis","text":"Now analyze road safety data allowing unobserved heterogeneity. first set two versions three-block MH-within-Gibbs sampler specify prior Œ±\\alpha Gamma distribution shape 2 rate 0.5 use samplers estimate model parameters. expected estimation results using samplers rather similar.","code":"negbin <- function(y, X, e, b0 = 0, B0 = 100, qmean, qvar, pri.alpha,                     full.gibbs= FALSE, burnin = 1000L, M = 50000L) {      N=length(y)   d <- ncol(X)   beta.post <- matrix(ncol = d, nrow = M)   colnames(beta.post) <- colnames(X)      b0 <- rep(b0, length.out = d)   B0 <- diag(rep(B0, length.out = d), nrow = d)      acc.beta <- numeric(length = M)      alpha.post<- rep(NA,M)   acc.alpha <-rep(NA,M)   c_alpha <- 0.1      # set starting values   beta <- as.vector(mvtnorm::rmvnorm(1, mean = qmean, sigma = qvar))   alpha <- pri.alpha$shape/pri.alpha$rate   phi <- rep(1.,N)      for (m in seq_len(burnin + M)){      # Step 1: draw beta        beta.old <- beta      beta.proposed <- as.vector(mvtnorm::rmvnorm(1, mean = qmean, sigma = qvar))       # compute log proposal density at proposed and old value      lq_proposed <- mvtnorm::dmvnorm(beta.proposed, mean = qmean, sigma = qvar,                                       log = TRUE)      lq_old  <- mvtnorm::dmvnorm(beta.old, mean = qmean, sigma = qvar,                                  log = TRUE)                   # compute log prior  of proposed and old value      lpri_proposed <- mvtnorm::dmvnorm(beta.proposed, mean = b0, sigma = B0,                                         log = TRUE)      lpri_old  <- mvtnorm::dmvnorm(beta.old,  mean = b0, sigma = B0, log = TRUE)       # compute log likelihood of proposed and old value      lh_proposed <- dpois(y, exp(X %*% beta.proposed),  log = TRUE)      lh_old  <- dpois(y, exp(X %*% beta.old), log = TRUE)       maxlik <- max(lh_old,lh_proposed)      ll <- sum(lh_proposed - maxlik) - sum(lh_old - maxlik)       # compute acceptance probability and accept or not      log_acc <- min(0, ll + lpri_proposed - lpri_old + lq_old - lq_proposed)       if (log(runif(1)) < log_acc) {         beta <- beta.proposed         acc.b <- 1      }else{         beta <- beta.old         acc.b <- 0      }      linpred <- X %*% beta       # Step 2:  Sample alpha      alpha.old <- alpha      alpha.proposed <- alpha.old * exp(c_alpha * rnorm(1))            if (full.gibbs) {         llik_alpha.proposed <- sum(dgamma(phi, shape = alpha.proposed,                                           rate = alpha.proposed, log = TRUE))         llik_alpha.old      <- sum(dgamma(phi, shape = alpha.old,                                           rate = alpha.old, log = TRUE))       } else {        llik_alpha.proposed <- sum(dnbinom(y, size = alpha.proposed,                                            mu = e * exp(linpred), log = TRUE))        llik_alpha.old      <- sum(dnbinom(y, size = alpha.old,                                            mu = e * exp(linpred), log = TRUE))      }      log_acc_alpha <- llik_alpha.proposed - llik_alpha.old +                        dgamma(alpha.proposed, shape = pri.alpha$shape,                            rate = pri.alpha$rate, log = TRUE) -                        dgamma(alpha.old, shape = pri.alpha$shape,                            rate = pri.alpha$rate,log=TRUE) +                        log(alpha.proposed) - log(alpha.old)       if (log(runif(1)) < log_acc_alpha) {         alpha <- alpha.proposed         acc.a <- 1      } else {         alpha <- alpha.old         acc.a <- 0      }         # Step 3:  sample phi from its full conditional     phi <- rgamma(N, shape = alpha + y, rate = alpha + e * exp(linpred))          # Save the draws     if (m > burnin) {         beta.post[m - burnin, ] <- beta         acc.beta[m - burnin] <- acc.b                  alpha.post[m - burnin] <- alpha         acc.alpha[m - burnin] <- acc.a     }   }   return(res = list(beta.post = beta.post, acc.beta = acc.beta,                     alpha.post = alpha.post,acc.alpha = acc.alpha)) } pri.alpha <- data.frame(shape = 2, rate = 0.5)  res1 <- negbin(y, X, e, qmean = parms.proposal$mean, qvar = parms.proposal$var,                pri.alpha = pri.alpha, full.gibbs = TRUE)  res.negbin.full <- rbind(t(apply(res1$beta.post, 2, res.mcmc)),                           res.mcmc(res1$alpha.post)) rownames(res.negbin.full)[4] <- \"alpha\" knitr::kable(round(res.negbin.full, 3)) res2 <- negbin(y, X, e, qmean = parms.proposal$mean, qvar = parms.proposal$var,                pri.alpha = pri.alpha, full.gibbs = FALSE)  res.negbin.partial <- rbind(t(apply(res2$beta.post, 2, res.mcmc)),                          res.mcmc(res2$alpha.post)) rownames(res.negbin.partial)[4] <- \"alpha\" knitr::kable(round(res.negbin.partial, 3)) #require(xtable) #xtable(cbind(t(res.negbin.full), t(res.negbin.partial)), label=\"exam:negbin_est\")"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"section-8-2-3-evaluating-mcmc-samplers","dir":"Articles","previous_headings":"Section 8.2: Count response variables","what":"Section 8.2.3: Evaluating MCMC samplers","title":"Chapter 8: Beyond Standard Regression Analysis","text":"","code":"print(c(mean(res1$acc.beta), mean(res1$acc.alpha))) #> [1] 0.35490 0.70254 print(c(mean(res2$acc.beta), mean(res2$acc.alpha))) #> [1] 0.36328 0.89462 if (pdfplots) {   pdf(\"8-2_1.pdf\", width = 8, height = 5) } par(mfrow = c(1,2), mar = c(2.5, 2.5, 1.5, .1), mgp = c(1.5, .5, 0), lwd = 1.5)  qqplot(res1$beta.post[,1], res2$beta.post[,1], xlab = \"Full Gibbs\",        ylab = \"Partial Gibbs\", main = \"Intercept\") abline(a = 0, b = 1)  qqplot(res1$alpha.post, res2$alpha.post, xlab = \"Full Gibbs\",        ylab = \"Partial Gibbs\", main = \"Alpha\") abline(a = 0, b = 1)"},{"path":[]},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-12-star-cluster-data","dir":"Articles","previous_headings":"Section 8.3: Beyond i.i.d. Gaussian error distributions > Section 8.3.1: Regression analysis with heteroskedastic errors","what":"Example 8.12: Star cluster data","title":"Chapter 8: Beyond Standard Regression Analysis","text":"bivariate data set star cluster CYG OB1 available package robustbase load package visualize scatter plot:  four giant stars can also identified scatter plot following indices data set: fit standard Bayesian regression analysis improper prior p(Œ≤0,Œ≤1,œÉ2)‚àù1/œÉ2p(\\beta_0, \\beta_1, \\sigma^2) \\propto 1 / \\sigma^2 determine mean pointwise 95%-HPD regions posterior predictive distribution p(yi|xi=x,ùê≤)p(y_i |x_i = x, \\mathbf{y}) using () full data set (b) data set observations y11y_{11}, y20y_{20}, y30y_{30}, y34y_{34} corresponding giant stars omitted. posterior predictive distribution single observation ii covariate value xix_i given sample ùê≤\\mathbf{y} model matrix ùêó\\mathbf{X} available closed form using improper prior corresponds prediction intervals obtained using OLS estimation. compare expected values (full lines) pointwise 95%-HPD regions following figure model fit using data (left) subset without giant stars (right).","code":"data(\"starsCYG\", package = \"robustbase\") plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") index <- c(11, 20, 30, 34) ols_all <- lm(log.light ~ log.Te, data = starsCYG) xnew <- seq(3, 5, length.out = 100) preds_all <- predict(ols_all, newdata = data.frame(log.Te = xnew),                      interval = \"prediction\") ols_subset <- lm(log.light ~ log.Te, data = starsCYG[-index, ]) preds_subset <- predict(ols_subset, newdata = data.frame(log.Te = xnew),                         interval = \"prediction\")"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"figure-8-9-star-cluster-data","dir":"Articles","previous_headings":"Section 8.3: Beyond i.i.d. Gaussian error distributions","what":"Figure 8.9: Star cluster data","title":"Chapter 8: Beyond Standard Regression Analysis","text":"","code":"plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") lines(xnew, preds_all[, \"fit\"]) lines(xnew, preds_all[, \"lwr\"], lty = 2) lines(xnew, preds_all[, \"upr\"], lty = 2) plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") lines(xnew, preds_subset[, \"fit\"]) lines(xnew, preds_subset[, \"lwr\"], lty = 2) lines(xnew, preds_subset[, \"upr\"], lty = 2)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-13-star-cluster-data---heteroskedastic-regression-analysis-with-known-outliers","dir":"Articles","previous_headings":"Section 8.3: Beyond i.i.d. Gaussian error distributions > Figure 8.9: Star cluster data","what":"Example 8.13: Star cluster data - heteroskedastic regression analysis with known outliers","title":"Chapter 8: Beyond Standard Regression Analysis","text":"define binary indicator indicating outlying observations, .e., case observations corresponding giant stars. prepare model matrix vector response define dimensions. heteroskedastic regression, define weights depending binary indicator either 1 equal œï‚â™1\\phi \\ll 1. include weights update Gibbs sampling scheme linear regression defined Chapter 6 accordingly. Based posterior draws parameters determine draws predictive distributions new observations xnew values:","code":"S <- rep(1L, nrow(starsCYG)) S[index] <- 2L X <- cbind(1, starsCYG$log.Te) y <- starsCYG$log.light N <- length(y) d <- ncol(X) phi <- 0.001 w <- phi^(S - 1) set.seed(1)  # define prior parameters of semi-conjugate prior B0inv <- diag(rep(1 / 10000, d), nrow = d) b0 <- rep(0, d)  c0 <- 2.5 C0 <- 1.5  # define quantities for the Gibbs sampler taking the weights into # account Xtilde <- sqrt(w) * X ytilde <- sqrt(w) * y wXX <- crossprod(Xtilde) wXy <- t(Xtilde) %*% ytilde cN <- c0 + N / 2  # define burnin and M burnin <- 1000 M <- 100000  # prepare storing of results betas <- matrix(NA_real_, nrow = burnin + M, ncol = d) sigma2s <- rep(NA_real_, burnin + M) colnames(betas) <- colnames(X)  # starting value for sigma2 sigma2 <- var(y) / 2  for (m in 1:(burnin + M)) {     # sample beta from the full conditional     BN <- solve(B0inv + wXX / sigma2)     bN <- BN %*% (B0inv %*% b0 + wXy / sigma2)     beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))      # sample sigma^2 from its full conditional     eps <- ytilde - Xtilde %*% beta     CN <- C0 + crossprod(eps) / 2     sigma2 <- rinvgamma(1, cN, CN)      betas[m, ] <- beta     sigma2s[m] <- sigma2 } pred_hetero <- sapply(1:M, function(m) {     rnorm(length(xnew), cbind(1, xnew) %*% betas[m, ], sqrt(sigma2s[m])) }) plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") lines(xnew, rowMeans(pred_hetero)) lines(xnew, apply(pred_hetero, 1, quantile, 0.025), lty = 2) lines(xnew, apply(pred_hetero, 1, quantile, 0.975), lty = 2)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter08.html","id":"example-8-14-star-cluster-data---regression-analysis-with-gaussian-two-component-mixture-errors","dir":"Articles","previous_headings":"Section 8.3: Beyond i.i.d. Gaussian error distributions > Figure 8.9: Star cluster data","what":"Example 8.14: Star cluster data - regression analysis with Gaussian two-component mixture errors","title":"Chapter 8: Beyond Standard Regression Analysis","text":"now assume indices giant stars known. assume two-component mixture used weight distribution weights given wi=œïSi‚àí1w_i = \\phi^{S_i-1} indicators SiS_i unknown. assume proportion giant stars known assume correspond 10%. now modify Gibbs sampling code include sampling step mixture component indicators. visualize mean 95%-HPD region together data points show fit now robust outlying observations.  now assume indices giant stars known. assume two-component mixture used weight distribution weights given wi=œïSi‚àí1w_i = \\phi^{S_i-1} indicators SiS_i unknown. Now, also assume proportion giant stars unknown. need specify prior Œ∑\\eta. usual prior beta prior use prior mean 0.1 corresponds prior sample size 10. now modify Gibbs sampling code also include sampling step component size.  Finally, visualize mean 95%-HPD region together data points three modeling approaches: (1) heteroskedastic regression analysis known outliers, (2) Gaussian two-component mixture known component sizes (3) Gaussian two-component mixture component size unknown.  plot indicates three modeling approaches result fit robust outlying observations.","code":"eta <- 0.1 phi <- 0.001 # define prior parameters of semi-conjugate prior B0inv <- diag(rep(1, d), nrow = d) b0 <- coef(ols_subset) set.seed(1)  # starting values for beta and sigma2 beta <- coef(ols_subset) sigma2 <- var(y) / 2  for (m in seq_len(burnin + M)) {     # mixture component indicator     Xbeta <- X %*% beta     posterior <- cbind((1 - eta) * dnorm(y, Xbeta, sqrt(sigma2)),                        eta * dnorm(y, Xbeta, sqrt(sigma2 / phi)))     posterior <- posterior / rowSums(posterior)     S <- 1 + rbinom(nrow(posterior), prob = posterior[, 2],                     size = 1)      # re-weight     w <- phi^(S - 1)     Xtilde <- sqrt(w) * X     ytilde <- sqrt(w) * y     wXX <- crossprod(Xtilde)     wXy <- t(Xtilde) %*% ytilde      # sample beta from the full conditional     BN <- solve(B0inv + wXX / sigma2)     bN <- BN %*% (B0inv %*% b0 + wXy / sigma2)     beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))      # sample sigma^2 from its full conditional     eps <- ytilde - Xtilde %*% beta     CN <- C0 + crossprod(eps) / 2     sigma2 <- rinvgamma(1, cN, CN)      betas[m, ] <- beta     sigma2s[m] <- sigma2 } preds_mix_1 <- sapply(1:M, function(m) {     rnorm(length(xnew), cbind(1, xnew) %*% betas[m, ], sqrt(sigma2s[m])) }) plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") lines(xnew, rowMeans(preds_mix_1)) lines(xnew, apply(preds_mix_1, 1, quantile, 0.025), lty = 2) lines(xnew, apply(preds_mix_1, 1, quantile, 0.975), lty = 2) a0 <- 1 d0 <- 9 set.seed(1)  # prepare storing of results etas <- rep(NA_real_, burnin + M)  # starting values for eta, beta and sigma2 eta <- 0.1 beta <- coef(ols_subset) sigma2 <- var(y) / 2  for (m in seq_len(burnin + M)) {     # mixture component indicator     Xbeta <- X %*% beta     posterior <- cbind((1 - eta) * dnorm(y, Xbeta, sqrt(sigma2)),                        eta * dnorm(y, Xbeta, sqrt(sigma2 / phi)))     posterior <- posterior / rowSums(posterior)     S <- 1 + rbinom(nrow(posterior), prob = posterior[, 2],                     size = 1)      # sample eta     aN <- a0 + sum(S == 2)     dN <- d0 + sum(S == 1)     eta <- rbeta(1, aN, dN)      # re-weight     w <- phi^(S - 1)     Xtilde <- sqrt(w) * X     ytilde <- sqrt(w) * y     wXX <- crossprod(Xtilde)     wXy <- t(Xtilde) %*% ytilde      # sample beta from the full conditional     BN <- solve(B0inv + wXX / sigma2)     bN <- BN %*% (B0inv %*% b0 + wXy / sigma2)     beta <- t(mvtnorm::rmvnorm(1, mean = bN, sigma = BN))      # sample sigma^2 from its full conditional     eps <- ytilde - Xtilde %*% beta     CN <- C0 + crossprod(eps) / 2     sigma2 <- rinvgamma(1, cN, CN)      betas[m, ] <- beta     sigma2s[m] <- sigma2     etas[m] <- eta } preds_mix_2 <- sapply(1:M, function(m) {     rnorm(length(xnew), cbind(1, xnew) %*% betas[m, ], sqrt(sigma2s[m])) }) plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") lines(xnew, rowMeans(preds_mix_2)) lines(xnew, apply(preds_mix_2, 1, quantile, 0.025), lty = 2) lines(xnew, apply(preds_mix_2, 1, quantile, 0.975), lty = 2) plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") lines(xnew, rowMeans(pred_hetero)) lines(xnew, apply(pred_hetero, 1, quantile, 0.025), lty = 2) lines(xnew, apply(pred_hetero, 1, quantile, 0.975), lty = 2) plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") lines(xnew, rowMeans(preds_mix_1)) lines(xnew, apply(preds_mix_1, 1, quantile, 0.025), lty = 2) lines(xnew, apply(preds_mix_1, 1, quantile, 0.975), lty = 2) plot(starsCYG, pch = 19, xlim = c(3, 5), ylim = c(3, 7),      xlab = \"log temperature\", ylab = \"log light intensity\") lines(xnew, rowMeans(preds_mix_2)) lines(xnew, apply(preds_mix_2, 1, quantile, 0.025), lty = 2) lines(xnew, apply(preds_mix_2, 1, quantile, 0.975), lty = 2)"},{"path":[]},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter09.html","id":"example-9-1-road-safety-data","dir":"Articles","previous_headings":"Section 9.1","what":"Example 9.1: Road Safety Data","title":"Chapter 9: Bayesian Predictive Analysis","text":"load data extract observations senior people Linz. plot pdf cdf predictive distribution corresponds flat prior yf|ùê≤‚àºùí©B(Ny‚Äæ+1,N). y_f|\\mathbf{y} \\sim \\mathcal NB(N\\bar y + 1, N).","code":"data(\"accidents\", package = \"BayesianLearningCode\") y <- accidents[, \"seniors_accidents\"] (aN <- sum(y) + 1) #> [1] 1009 (bN <- length(y)) #> [1] 192 mu <- aN / bN yf <- 0:20 plot(yf, dnbinom(yf, size = aN, mu = mu),      type = \"h\", xlab = bquote(y[f]), ylab = \"\") plot(yf, pnbinom(yf, size = aN, mu = mu),      type = \"h\", xlab = bquote(y[f]), ylab = \"\") probs <- c(0.025, 0.975) abline(h = probs, lty = 3) mtext(probs, side = 2, at = probs, adj = c(0, 1), cex = .8, col = \"dimgrey\")"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter09.html","id":"example-9-2-exchange-rate-data","dir":"Articles","previous_headings":"Section 9.1","what":"Example 9.2: Exchange Rate Data","title":"Chapter 9: Bayesian Predictive Analysis","text":"load data plot pdf cdf predictive distribution corresponds improper prior yf|ùê≤‚àºùìâ2cN(y‚Äæ,Nsy2N‚àí1(1+1N)). y_f|\\mathbf{y} \\sim \\mathcal t_{2c_N}\\left(\\bar y, \\frac{N s_y^2}{N-1}\\left(1 + \\frac{1}{N}\\right)\\right).  inspect parameters Student-t distribution.","code":"library(\"BayesianLearningCode\") data(\"exrates\", package = \"stochvol\") y <- 100 * diff(log(exrates$USD / exrates$CHF)) ybar <- mean(y) N <- length(y) b0 <- 0 N0 <- 0 c0 <- -1/2 C0 <- 0 BN <- 1 / (N + N0) bN <- BN * (N * ybar + N0 * b0) cN <- c0 + N/2 CN <- C0 + .5 * sum((y - ybar)^2) + N0 * N / (2 * (N0 + N)) * (b0 - ybar)^2 x <- seq(-3, 3, length.out = 200) scale <- sqrt(CN / cN * (BN + 1)) plot(x, dstudt(x, location = bN, scale = scale, df = 2 * cN),      type = \"l\", xlab = bquote(y[f]), ylab = \"\") plot(x, pstudt(x, location = bN, scale = scale, df = 2 * cN),      type = \"l\", xlab = bquote(y[f]), ylab = \"\") probs <- c(0.025, 0.975) abline(h = probs, lty = 3) mtext(probs, side = 2, at = probs, adj = c(0, 1), cex = .8, col = \"dimgrey\") round(c(mu = bN, sigma2 = scale^2, df = 2 * cN), digits = 3) #>       mu   sigma2       df  #>    0.018    0.528 3138.000"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter09.html","id":"example-9-3-exchange-rate-data-contd","dir":"Articles","previous_headings":"Section 9.1","what":"Example 9.3: Exchange Rate Data cont‚Äôd","title":"Chapter 9: Bayesian Predictive Analysis","text":"compare method ignores parameter uncertainty, now plot posterior predictive alongside ‚Äúclassical‚Äù forecasting distribution varying NN.","code":"Ns <- c(5, 10, 30, N)  for (i in seq_along(Ns)) {   N <- Ns[i]   yshort <- y[1:N]   ybar <- mean(yshort)   BN <- 1 / (N + N0)   bN <- BN * (N * ybar + N0 * b0)   cN <- c0 + N/2   CN <- C0 + .5 * sum((yshort - ybar)^2) + N0 * N / (2 * (N0 + N)) * (b0 - ybar)^2   scale <- sqrt(CN / cN * (BN + 1))   plot(x, dnorm(x, ybar, sd(yshort)), lty = 2, type = \"l\",        xlab = bquote(y[f]), ylab = \"\", main = paste(\"N =\", N))   lines(x, dstudt(x, location = bN, scale = scale, df = 2 * cN)) }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter09.html","id":"example-9-4-road-safety-data-contd","dir":"Articles","previous_headings":"Section 9.1","what":"Example 9.4: Road Safety Data cont‚Äôd","title":"Chapter 9: Bayesian Predictive Analysis","text":"verify 95% predictive interval given [1, 9] using cdf compute effective coverage.","code":"pnbinom(9, size = aN, mu = mu) - pnbinom(0, size = aN, mu = mu) #> [1] 0.9522248"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter09.html","id":"example-9-5-exchange-rate-data-contd","dir":"Articles","previous_headings":"Section 9.1","what":"Example 9.5: Exchange Rate Data cont‚Äôd","title":"Chapter 9: Bayesian Predictive Analysis","text":"determine 95% predictive interval using quantile function. now work effective coverage naive interval ignores parameter uncertainty.","code":"round(qstudt(probs, location = bN, scale = scale, df = 2 * cN), digits = 3) #> [1] -1.408  1.443 coverage <- rep(NA_real_, length(Ns)) names(coverage) <- Ns for (i in seq_along(Ns)) {   N <- Ns[i]   yshort <- y[1:N]   quants <- qnorm(probs, mean(yshort), sd(yshort))   coverage[i] <- diff(pstudt(quants, mean(yshort), sd(yshort) * sqrt(1 + 1/N),                              2 * (c0 + N/2))) } knitr::kable(t(round(coverage, 4)))"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/articles/Chapter09.html","id":"example-9-8-sampling-based-prediction-for-the-chfusd-exchange-rates","dir":"Articles","previous_headings":"Section 9.1","what":"Example 9.8: Sampling-based prediction for the CHF/USD exchange rates","title":"Chapter 9: Bayesian Predictive Analysis","text":"proceed exactly Chapter 4. Gaussian distribution, posterior œÉ2\\sigma^2 inverse gamma, can easily generate iid draws. Student-tt model, can use inverse transform sampling. First, draw uniformly interval spanned 0 maximum non-normalized cumulative posterior. , draw, find interval pointwise cdf approximation posterior, interpolate linearly interval boundaries. Next, simulate draws posterior predictive. Now can compute empirical quantiles, data. conclude visualizing data predictive distributions.","code":"set.seed(2) ndraws <- 10000 sigma2draws_normal <- rinvgamma(ndraws, N / 2, sum(y^2) / 2) # We need the nonnormalized cumulative posterior distribution post_nonnormalized_nonvec <- function(sigma2, y, nu, log = FALSE) {   logdens <- -length(y) / 2 * log(sigma2) -     (nu + 1) / 2 * sum(log(1 + y^2 / (nu * sigma2))) - log(sigma2)   if (log) logdens else exp(logdens) } post_nonnormalized <- Vectorize(post_nonnormalized_nonvec, \"sigma2\")  # Now, we compute the pdf and cdf on a reasonably chosen grid nu <- 7 sigma2 <- seq(0.25, 0.45, length.out = 3000) pdf_u <- post_nonnormalized(sigma2, y = y, nu = nu) pdf_u <- pdf_u / max(pdf_u) cdf_u <- cumsum(pdf_u) / sum(pdf_u)  # Now we can perform inverse transpose sampling unifdraws <- runif(ndraws, 0, cdf_u[length(cdf_u)]) leftind <- findInterval(unifdraws, cdf_u) rightind <- leftind + 1L distprop <- (unifdraws - cdf_u[leftind]) / (cdf_u[rightind] - cdf_u[leftind]) sigma2draws_t <- sigma2[leftind] + distprop *   (sigma2[rightind] - sigma2[leftind]) yf_normal <- rnorm(ndraws, 0, sqrt(sigma2draws_normal)) yf_t <- rstudt(ndraws, 0, sqrt(sigma2draws_t), 7) quants <- c(0.01, 0.05, 0.25, 0.4, 0.5, 0.6, 0.75, 0.95, 0.99) q_normal <- quantile(yf_normal, quants) q_t <- quantile(yf_t, quants) q_y <- quantile(y, quants)  knitr::kable(round(t(cbind(q_y, q_t, q_normal)), 3)) grid <- seq(-ceiling(max(abs(y))), ceiling(max(abs(y))), length.out = 50) hist(y, freq = FALSE, breaks = grid, main = \"Histogram and predictive densitites\") lines(density(yf_normal, adjust = 2), col = 4, lty = 1, lwd = 2) lines(density(yf_t, adjust = 2), col = 2, lty = 2, lwd = 2) legend(\"topleft\", c(\"Normal\", \"Student t\"), lty = 1:2, col = c(4,2), lwd = 2) ts.plot(y, main = \"Time series plot and some predictive intervals\") abline(h = q_normal, col = 4, lty = 1, lwd = 2) abline(h = q_t, col = 2, lty = 2, lwd = 2) legend(\"topleft\", c(\"Normal\", \"Student t\"), lty = 1:2, col = c(4,2), lwd = 2)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Sylvia Fr√ºhwirth-Schnatter. Author. Bettina Gr√ºn. Author. Gregor Kastner. Author, maintainer. Helga Wagner. Author.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Fr√ºhwirth-Schnatter S, Gr√ºn B, Kastner G, Wagner H (2026). BayesianLearningCode: Accompanying Code Future Book. R package version 0.0.1, https://gregorkastner.github.io/BayesianLearningCode/.","code":"@Manual{,   title = {BayesianLearningCode: Accompanying Code to a Future Book},   author = {Sylvia Fr√ºhwirth-Schnatter and Bettina Gr√ºn and Gregor Kastner and Helga Wagner},   year = {2026},   note = {R package version 0.0.1},   url = {https://gregorkastner.github.io/BayesianLearningCode/}, }"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/index.html","id":"bayesianlearningcode","dir":"","previous_headings":"","what":"Accompanying Code to a Future Book","title":"Accompanying Code to a Future Book","text":"BayesianLearningCode provides R code reproduce () examples future book.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/index.html","id":"typical-use-case","dir":"","previous_headings":"","what":"Typical use case","title":"Accompanying Code to a Future Book","text":"Click Articles top page view R code.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Accompanying Code to a Future Book","text":"reason ‚Äôd like entire package computer, can installing development version BayesianLearningCode GitHub : Note typically needed advanced use cases.","code":"# install.packages(\"pak\") pak::pak(\"gregorkastner/BayesianLearningCode\")"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/Dirichlet.html","id":null,"dir":"Reference","previous_headings":"","what":"The Dirichlet Distribution ‚Äî Dirichlet","title":"The Dirichlet Distribution ‚Äî Dirichlet","text":"Random generation Dirichlet distribution parameter vector alpha.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/Dirichlet.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Dirichlet Distribution ‚Äî Dirichlet","text":"","code":"rdirichlet(ndraws, alpha)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/Dirichlet.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Dirichlet Distribution ‚Äî Dirichlet","text":"ndraws number observations. alpha positive parameter vector.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/Dirichlet.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The Dirichlet Distribution ‚Äî Dirichlet","text":"rdirichlet generates random deviates.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/InvGammaDist.html","id":null,"dir":"Reference","previous_headings":"","what":"The Inverse Gamma Distribution ‚Äî InvGammaDist","title":"The Inverse Gamma Distribution ‚Äî InvGammaDist","text":"Density, distribution function, quantile function random generation inverse gamma distribution parameters b.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/InvGammaDist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Inverse Gamma Distribution ‚Äî InvGammaDist","text":"","code":"dinvgamma(x, a, b, log = FALSE)  pinvgamma(q, a, b)  qinvgamma(p, a, b)  rinvgamma(ndraws, a, b)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/InvGammaDist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Inverse Gamma Distribution ‚Äî InvGammaDist","text":"x vector quantiles. positive parameter. b positive parameter. log logical; TRUE, densities returned logarithms. q vector quantiles. p vector probabilities. ndraws number observations.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/InvGammaDist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The Inverse Gamma Distribution ‚Äî InvGammaDist","text":"dinvgamma gives density, pinvgamma gives distribution function, qinvgamma gives quantile function, rinvgamma generates random deviates.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/InvGammaDist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"The Inverse Gamma Distribution ‚Äî InvGammaDist","text":"details, please see GammaDist.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/StudTDist.html","id":null,"dir":"Reference","previous_headings":"","what":"The Non-standardized Student t Distribution ‚Äî StudTDist","title":"The Non-standardized Student t Distribution ‚Äî StudTDist","text":"Density, distribution function, quantile function random generation location-scale-transformed Student t distribution.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/StudTDist.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The Non-standardized Student t Distribution ‚Äî StudTDist","text":"","code":"dstudt(x, location = 0, scale = 1, df, log = FALSE)  pstudt(q, location = 0, scale = 1, df)  qstudt(p, location = 0, scale = 1, df)  rstudt(n, location = 0, scale = 1, df)"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/StudTDist.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The Non-standardized Student t Distribution ‚Äî StudTDist","text":"x vector quantiles. location parameter. scale positive parameter. df positive parameter. log logical; TRUE, densities returned logarithms. q vector quantiles. p vector probabilities. n number observations.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/StudTDist.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The Non-standardized Student t Distribution ‚Äî StudTDist","text":"dstudt gives density, pstudt gives distribution function, qstudt gives quantile function, rstudt generates random deviates.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/StudTDist.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"The Non-standardized Student t Distribution ‚Äî StudTDist","text":"non-standardized Student t distribution location \\(\\mu\\), scale \\(\\tau\\), df \\(\\nu > 2\\) mean \\(\\mu\\) variance \\(\\tau^2 \\frac{\\nu}{\\nu-2}\\).","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/accidents.html","id":null,"dir":"Reference","previous_headings":"","what":"Accidents Data ‚Äî accidents","title":"Accidents Data ‚Äî accidents","text":"Seriously injured killed pedestrians per month Linz, Austria, January 1987 December 2002.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/accidents.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Accidents Data ‚Äî accidents","text":"","code":"accidents"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/accidents.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Accidents Data ‚Äî accidents","text":"monthly time series object 192 rows 4 columns children_accidents number children aged 6 10 years seriously injured killed children_exposure estimated number children exposed seniors_accidents senior persons 65 seriously injured killed seniors_exposure estimated number seniors exposed","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/accidents.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Accidents Data ‚Äî accidents","text":"TBD","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/eyetracking.html","id":null,"dir":"Reference","previous_headings":"","what":"Eye Tracking Data ‚Äî eyetracking","title":"Eye Tracking Data ‚Äî eyetracking","text":"Count data eye tracking anomalies 101 schizophrenic patients, sorted size","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/eyetracking.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Eye Tracking Data ‚Äî eyetracking","text":"","code":"eyetracking"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/eyetracking.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Eye Tracking Data ‚Äî eyetracking","text":"data frame 101 observations 1 variable anomalies number eye tracking anomalies","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/eyetracking.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Eye Tracking Data ‚Äî eyetracking","text":"Escobar & West (1998)","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/gdp.html","id":null,"dir":"Reference","previous_headings":"","what":"GDP data ‚Äî gdp","title":"GDP data ‚Äî gdp","text":"Gross domestic product USA. Quarterly data 1947Q1 2025Q1.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/gdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"GDP data ‚Äî gdp","text":"","code":"gdp"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/gdp.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"GDP data ‚Äî gdp","text":"sequence length 313","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/gdp.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"GDP data ‚Äî gdp","text":"U.S. Bureau Economic Analysis, Gross Domestic Product (GDP), retrieved FRED, Federal Reserve Bank St. Louis; https://fred.stlouisfed.org/series/GDP, July 24, 2025.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/gdp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"GDP data ‚Äî gdp","text":"Gross domestic product (GDP), featured measure U.S. output, market value goods services produced labor property located United States. units Billions Dollars, Seasonally Adjusted Annual Rate.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/inflation.html","id":null,"dir":"Reference","previous_headings":"","what":"Inflation Data ‚Äî inflation","title":"Inflation Data ‚Äî inflation","text":"Harmonized Index Consumer Prices (HICP) Euro area (changing composition). Monthly data January 1997 June 2025.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/inflation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Inflation Data ‚Äî inflation","text":"","code":"inflation"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/inflation.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Inflation Data ‚Äî inflation","text":"sequence length 342.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/inflation.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Inflation Data ‚Äî inflation","text":"European Central Bank Data Portal, retrieved https://data.ecb.europa.eu/data/datasets/ICP/ICP.M.U2.N.000000.4.ANR, July 25, 2025.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/labor.html","id":null,"dir":"Reference","previous_headings":"","what":"Labor Market Data ‚Äî labor","title":"Labor Market Data ‚Äî labor","text":"Austrian social security authority collects detailed data workers. data set random sample 4376 persons birth cohort 1921-1980 work status recorded years 1986-1998.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/labor.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Labor Market Data ‚Äî labor","text":"","code":"labor"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/labor.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Labor Market Data ‚Äî labor","text":"data frame 4376 observations (persons) 41 variables: female logical, TRUE person female birthyear integer, year birth income_1986, income_1987, income_1988, income_1989, income_1990, income_1991, income_1992, income_1993, income_1994, income_1995, income_1996, income_1997, income_1998 ordered factor, income status based gross monthly wage May 31st specific year levels zero (income) <= 1st quintile, <= 2nd quintile, <= 3rd quintile, <= 4th quintile <= 5th quintile according yearly wage distribution employerchanges_1986, employerchanges_1987, employerchanges_1988, employerchanges_1989, employerchanges_1990, employerchanges_1991, employerchanges_1992, employerchanges_1993, employerchanges_1994, employerchanges_1995, employerchanges_1996, employerchanges_1997, employerchanges_1998 integer, number times person changed employer respective year wcollar_1986, wcollar_1987, wcollar_1988, wcollar_1989, wcollar_1990, wcollar_1991, wcollar_1992, wcollar_1993, wcollar_1994, wcollar_1995, wcollar_1996, wcollar_1997, wcollar_1998 logical, TRUE person classified white collar employee blue collar employee respective year","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/labor.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Labor Market Data ‚Äî labor","text":"Zweim√ºller, J., R. Winter-Ebmer, R. Lalive, . Kuhn, J.-P. Wuellrich, O. Ruf, S. B√ºchi (2009): Austrian Social Security Database (ASSD). Working Paper 0903, NRN: Austrian Center Labor Economics Analysis Welfare State, Linz, Austria.","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/movies.html","id":null,"dir":"Reference","previous_headings":"","what":"Movie Open Box Office Data ‚Äî movies","title":"Movie Open Box Office Data ‚Äî movies","text":"TBD","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/movies.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Movie Open Box Office Data ‚Äî movies","text":"","code":"movies"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/movies.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Movie Open Box Office Data ‚Äî movies","text":"data frame 94 observations 29 variables OpenBoxOffice numeric vector Action binary vector Adventure binary vector Animation binary vector Comedy binary vector Crime binary vector Drama binary vector Family binary vector Fantasy binary vector Mystery binary vector Romance binary vector Sci-Fi binary vector Thriller binary vector PG binary vector PG13 binary vector R binary vector Budget numeric vector Weeks numeric vector Screens numeric vector S-21-27 numeric vector S-14-20 numeric vector S-7-13 numeric vector S-4-6 numeric vector S-1-3 numeric vector Vol-21-27 numeric vector Vol-14-20 numeric vector Vol-7-13 numeric vector Vol-4-6 numeric vector Vol-1-3 numeric vector","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/movies.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Movie Open Box Office Data ‚Äî movies","text":"TBD: doi:10.7910/DVN/JS3AVM","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/words.html","id":null,"dir":"Reference","previous_headings":"","what":"Words Data ‚Äî words","title":"Words Data ‚Äî words","text":"List 1000 common English headwords (.e., dictionary encyclopedia entries set related words appear) according British National Corpus Corpus Contemporary American English (BNC/COCA).","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/words.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Words Data ‚Äî words","text":"","code":"words"},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/words.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Words Data ‚Äî words","text":"data frame 1000 observations 2 variables word English (head)word frequency number occurrences","code":""},{"path":"https://gregorkastner.github.io/BayesianLearningCode/reference/words.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Words Data ‚Äî words","text":"Sourced https://www.eapfoundation.com/vocab/general/bnccoca/ March 21, 2025","code":""}]
